"Abstract:  The non-linear binary Kerdock codes are known to be Gray images of certain
extended cyclic codes of length $N = 2^m$ over $\mathbb{Z}_4$. We show that
exponentiating these $\mathbb{Z}_4$-valued codewords by $\imath \triangleq
\sqrt{-1}$ produces stabilizer states that are quantum states obtained using
only Clifford unitaries. These states are also the common eigenvectors of
commuting Hermitian matrices forming maximal commutative subgroups (MCS) of the
Pauli group. We use this quantum description to simplify the derivation of the
classical weight distribution of Kerdock codes. Next we organize the
stabilizer states to form $N+1$ mutually unbiased bases and prove that
automorphisms of the Kerdock code permute their corresponding MCS thereby
forming a subgroup of the Clifford group. When represented as symplectic
matrices this subgroup is isomorphic to the projective special linear group
PSL($2N$). We show that this automorphism group acts transitively on the Pauli
matrices which implies that the ensemble is Pauli mixing and hence forms a
unitary $2$-design. The Kerdock design described here was originally discovered
by Cleve et al. (arXiv:1501.04592) but the connection to classical codes is
new which simplifies its description and translation to circuits significantly.
Sampling from the design is straightforward the translation to circuits uses
only Clifford gates and the process does not require ancillary qubits.
Finally we also develop algorithms for optimizing the synthesis of unitary
$2$-designs on encoded qubits i.e. to construct logical unitary $2$-designs.
Software implementations are available at
this https URL which we use to provide
empirical gate complexities for up to $16$ qubits.
# DGC-Net: Dense Geometric Correspondence Network
This is a PyTorch implementation of our work [""DGC-Net: Dense Geometric Correspondence Network""](https://arxiv.org/abs/1810.08393)

**TL;DR** A CNN-based approach to obtain dense pixel correspondences between two views.

## Installation
- create and activate conda environment with Python 3.x
```
conda create -n my_fancy_env python=3.7
source activate my_fancy_env
```
- install Pytorch v1.0.0 and torchvision library
```
pip install torch torchvision
```
- install all dependencies by running the following command:
```
pip install -r requirements.txt
```

## Getting started
* ```eval.py``` demonstrates the results on the HPatches dataset
To be able to run ```eval.py``` script:
    * Download an archive with pre-trained models [click](https://drive.google.com/file/d/1p1FarlU5byWez_mQC68DZ_eRQKfF9IIf/view?usp=sharing) and extract it
to the project folder
    * Download HPatches dataset (Full image sequences). The dataset is available [here](https://github.com/hpatches/hpatches-dataset) at the end of the page
    * Run the following command:
    ```
    python eval.py --image-data-path /path/to/hpatches-geometry
    ```

* ```train.py``` is a script to train DGC-Net/DGCM-Net model from scratch. To run this script please follow the next procedure:
    * Download the [TokyoTimeMachine dataset](https://www.di.ens.fr/willow/research/netvlad/)
    * Run the command:
    ```
    python train.py --image-data-path /path/to/TokyoTimeMachine
    ```

## Performance on [HPatches](https://github.com/hpatches/hpatches-dataset) dataset
Method / HPatches ID|Viewpoint 1|Viewpoint 2|Viewpoint 3|Viewpoint 4|Viewpoint 5
:---|:---:|:---:|:---:|:---:|:---:
[PWC-Net](https://arxiv.org/abs/1709.02371)| 4.43 | 11.44 | 15.47 | 20.17 | 28.30
[GM](https://arxiv.org/abs/1703.05593) best model | 9.59 | 18.55 | 21.15 | 27.83 | 35.19
DGC-Net (paper) | **1.55** | **5.53** | **8.98** | 11.66 | 16.70
DGCM-Net (paper) | 2.97 | 6.85 | 9.95 | 12.87 | 19.13
DGC-Net (repo) | 1.74 | 5.88 | 9.07 | 12.14 | 16.50
DGCM-Net (repo) | 2.33 | 5.62 | 9.55 | **11.59** | **16.48**

Note: There is a difference in numbers presented in the original paper and obtained by the models of this repo. It might be related to the fact that both models (DGC-Net and DGCM-Net) have been trained using ```Pytorch v0.3```.

More qualitative results are presented on the [project page](https://aaltovision.github.io/dgc-net-site/)

## How to cite
If you use this software in your own research please cite our publication:

```
@inproceedings{Melekhov+Tiulpin+Sattler+Pollefeys+Rahtu+Kannala:2018
      title = {{DGC-Net}: Dense geometric correspondence network}
      author = {Melekhov Iaroslav and Tiulpin Aleksei and 
               Sattler Torsten and 
               Pollefeys Marc and 
               Rahtu Esa and Kannala Juho}
       year = {2019}
       booktitle = {Proceedings of the IEEE Winter Conference on 
                    Applications of Computer Vision (WACV)}
}
```"
"Abstract:  This paper addresses the challenge of dense pixel correspondence estimation
between two images. This problem is closely related to optical flow estimation
task where ConvNets (CNNs) have recently achieved significant progress. While
optical flow methods produce very accurate results for the small pixel
translation and limited appearance variation scenarios they hardly deal with
the strong geometric transformations that we consider in this work. In this
paper we propose a coarse-to-fine CNN-based framework that can leverage the
advantages of optical flow approaches and extend them to the case of large
transformations providing dense and subpixel accurate estimates. It is trained
on synthetic transformations and demonstrates very good performance to unseen
realistic data. Further we apply our method to the problem of relative camera
pose estimation and demonstrate that the model outperforms existing dense
approaches.
# DGC-Net: Dense Geometric Correspondence Network
This is a PyTorch implementation of our work [""DGC-Net: Dense Geometric Correspondence Network""](https://arxiv.org/abs/1810.08393)

**TL;DR** A CNN-based approach to obtain dense pixel correspondences between two views.

## Installation
- create and activate conda environment with Python 3.x
```
conda create -n my_fancy_env python=3.7
source activate my_fancy_env
```
- install Pytorch v1.0.0 and torchvision library
```
pip install torch torchvision
```
- install all dependencies by running the following command:
```
pip install -r requirements.txt
```

## Getting started
* ```eval.py``` demonstrates the results on the HPatches dataset
To be able to run ```eval.py``` script:
    * Download an archive with pre-trained models [click](https://drive.google.com/file/d/1p1FarlU5byWez_mQC68DZ_eRQKfF9IIf/view?usp=sharing) and extract it
to the project folder
    * Download HPatches dataset (Full image sequences). The dataset is available [here](https://github.com/hpatches/hpatches-dataset) at the end of the page
    * Run the following command:
    ```
    python eval.py --image-data-path /path/to/hpatches-geometry
    ```

* ```train.py``` is a script to train DGC-Net/DGCM-Net model from scratch. To run this script please follow the next procedure:
    * Download the [TokyoTimeMachine dataset](https://www.di.ens.fr/willow/research/netvlad/)
    * Run the command:
    ```
    python train.py --image-data-path /path/to/TokyoTimeMachine
    ```

## Performance on [HPatches](https://github.com/hpatches/hpatches-dataset) dataset
Method / HPatches ID|Viewpoint 1|Viewpoint 2|Viewpoint 3|Viewpoint 4|Viewpoint 5
:---|:---:|:---:|:---:|:---:|:---:
[PWC-Net](https://arxiv.org/abs/1709.02371)| 4.43 | 11.44 | 15.47 | 20.17 | 28.30
[GM](https://arxiv.org/abs/1703.05593) best model | 9.59 | 18.55 | 21.15 | 27.83 | 35.19
DGC-Net (paper) | **1.55** | **5.53** | **8.98** | 11.66 | 16.70
DGCM-Net (paper) | 2.97 | 6.85 | 9.95 | 12.87 | 19.13
DGC-Net (repo) | 1.74 | 5.88 | 9.07 | 12.14 | 16.50
DGCM-Net (repo) | 2.33 | 5.62 | 9.55 | **11.59** | **16.48**

Note: There is a difference in numbers presented in the original paper and obtained by the models of this repo. It might be related to the fact that both models (DGC-Net and DGCM-Net) have been trained using ```Pytorch v0.3```.

More qualitative results are presented on the [project page](https://aaltovision.github.io/dgc-net-site/)

## How to cite
If you use this software in your own research please cite our publication:

```
@inproceedings{Melekhov+Tiulpin+Sattler+Pollefeys+Rahtu+Kannala:2018
      title = {{DGC-Net}: Dense geometric correspondence network}
      author = {Melekhov Iaroslav and Tiulpin Aleksei and 
               Sattler Torsten and 
               Pollefeys Marc and 
               Rahtu Esa and Kannala Juho}
       year = {2019}
       booktitle = {Proceedings of the IEEE Winter Conference on 
                    Applications of Computer Vision (WACV)}
}
```"
"Abstract:  This paper describes an Open Source Software (OSS) project: PythonRobotics.
This is a collection of robotics algorithms implemented in the Python
programming language. The focus of the project is on autonomous navigation and
the goal is for beginners in robotics to understand the basic ideas behind each
algorithm. In this project the algorithms which are practical and widely used
in both academia and industry are selected. Each sample code is written in
Python3 and only depends on some standard modules for readability and ease of
use. It includes intuitive animations to understand the behavior of the
simulation.
<img align=""right"" src=""https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true"" width=""300""/>

# PythonRobotics
[![Build Status](https://travis-ci.org/AtsushiSakai/PythonRobotics.svg?branch=master)](https://travis-ci.org/AtsushiSakai/PythonRobotics)
[![Documentation Status](https://readthedocs.org/projects/pythonrobotics/badge/?version=latest)](https://pythonrobotics.readthedocs.io/en/latest/?badge=latest)
[![Build status](https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true)](https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics)
[![Coverage Status](https://coveralls.io/repos/github/AtsushiSakai/PythonRobotics/badge.svg?branch=master)](https://coveralls.io/github/AtsushiSakai/PythonRobotics?branch=master)
[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/AtsushiSakai/PythonRobotics.svg?logo=lgtm&amp;logoWidth=18)](https://lgtm.com/projects/g/AtsushiSakai/PythonRobotics/context:python)
[![CodeFactor](https://www.codefactor.io/repository/github/atsushisakai/pythonrobotics/badge/master)](https://www.codefactor.io/repository/github/atsushisakai/pythonrobotics/overview/master)
[![tokei](https://tokei.rs/b1/github/AtsushiSakai/PythonRobotics)](https://github.com/AtsushiSakai/PythonRobotics)
[![Say Thanks!](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/AtsushiSakai)

Python codes for robotics algorithm.



# Table of Contents
   * [What is this?](#what-is-this)
   * [Requirements](#requirements)
   * [Documentation](#documentation)
   * [How to use](#how-to-use)
   * [Localization](#localization)
      * [Extended Kalman Filter localization](#extended-kalman-filter-localization)
      * [Particle filter localization](#particle-filter-localization)
      * [Histogram filter localization](#histogram-filter-localization)
   * [Mapping](#mapping)
      * [Gaussian grid map](#gaussian-grid-map)
      * [Ray casting grid map](#ray-casting-grid-map)
      * [k-means object clustering](#k-means-object-clustering)
      * [Rectangle fitting](#rectangle-fitting)
   * [SLAM](#slam)
      * [Iterative Closest Point (ICP) Matching](#iterative-closest-point-icp-matching)
      * [FastSLAM 1.0](#fastslam-10)
      * [Graph based SLAM](#graph-based-slam)
   * [Path Planning](#path-planning)
      * [Dynamic Window Approach](#dynamic-window-approach)
      * [Grid based search](#grid-based-search)
         * [Dijkstra algorithm](#dijkstra-algorithm)
         * [A* algorithm](#a-algorithm)
         * [Potential Field algorithm](#potential-field-algorithm)
      * [State Lattice Planning](#state-lattice-planning)
         * [Biased polar sampling](#biased-polar-sampling)
         * [Lane sampling](#lane-sampling)
      * [Probabilistic Road-Map (PRM) planning](#probabilistic-road-map-prm-planning)
      * [Rapidly-Exploring Random Trees (RRT)](#rapidly-exploring-random-trees-rrt)
         * [RRT*](#rrt)
         * [RRT* with reeds-sheep path](#rrt-with-reeds-sheep-path)
         * [LQR-RRT*](#lqr-rrt)
      * [Quintic polynomials planning](#quintic-polynomials-planning)
      * [Reeds Shepp planning](#reeds-shepp-planning)
      * [LQR based path planning](#lqr-based-path-planning)
      * [Optimal Trajectory in a Frenet Frame](#optimal-trajectory-in-a-frenet-frame)
   * [Path Tracking](#path-tracking)
      * [move to a pose control](#move-to-a-pose-control)
      * [Stanley control](#stanley-control)
      * [Rear wheel feedback control](#rear-wheel-feedback-control)
      * [Linear–quadratic regulator (LQR) speed and steering control](#linearquadratic-regulator-lqr-speed-and-steering-control)
      * [Model predictive speed and steering control](#model-predictive-speed-and-steering-control)
      * [Nonlinear Model predictive control with C-GMRES](#nonlinear-model-predictive-control-with-c-gmres)
   * [Arm Navigation](#arm-navigation)
      * [N joint arm to point control](#n-joint-arm-to-point-control)
      * [Arm navigation with obstacle avoidance](#arm-navigation-with-obstacle-avoidance)
   * [Aerial Navigation](#aerial-navigation)
      * [drone 3d trajectory following](#drone-3d-trajectory-following)
      * [rocket powered landing](#rocket-powered-landing)
   * [Bipedal](#bipedal)
      * [bipedal planner with inverted pendulum](#bipedal-planner-with-inverted-pendulum)
   * [License](#license)
   * [Use-case](#use-case)
   * [Contribution](#contribution)
   * [Support](#support)
   * [Authors](#authors)

# What is this?

This is a Python code collection of robotics algorithms especially for autonomous navigation.

Features:

1. Easy to read for understanding each algorithm's basic idea.

2. Widely used and practical algorithms are selected.

3. Minimum dependency.

See this paper for more details:

- [\[1808\.10703\] PythonRobotics: a Python code collection of robotics algorithms](https://arxiv.org/abs/1808.10703) ([BibTeX](https://github.com/AtsushiSakai/PythonRoboticsPaper/blob/master/python_robotics.bib))


# Requirements

- Python 3.6.x (2.7 is not supported)

- numpy

- scipy

- matplotlib

- pandas

- [cvxpy](http://www.cvxpy.org/en/latest/) 

# Documentation

This README only shows some examples of this project. 

If you are interested in other examples or mathematical backgrounds of each algorithm 

You can check the full documentation online: [https://pythonrobotics.readthedocs.io/](https://pythonrobotics.readthedocs.io/)

All animation gifs are stored here: [AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs)

# How to use

1. Clone this repo.

&gt; git clone https://github.com/AtsushiSakai/PythonRobotics.git

&gt; cd PythonRobotics/


2. Install the required libraries. You can use environment.yml with conda command.

&gt; conda env create -f environment.yml


3. Execute python script in each directory.

4. Add star to this repo if you like it :smiley:. 

# Localization

## Extended Kalman Filter localization

<img src=""https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif"" width=""640""/>

Documentation: [Notebook](https://github.com/AtsushiSakai/PythonRobotics/blob/master/Localization/extended_kalman_filter/extended_kalman_filter_localization.ipynb)

## Particle filter localization

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif)

This is a sensor fusion localization with Particle Filter(PF).

The blue line is true trajectory the black line is dead reckoning trajectory

and the red line is estimated trajectory with PF.

It is assumed that the robot can measure a distance from landmarks (RFID).

This measurements are used for PF localization.

Ref:

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)


## Histogram filter localization

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif)

This is a 2D localization example with Histogram filter.

The red cross is true position black points are RFID positions.

The blue grid shows a position probability of histogram filter.  

In this simulation xy are unknown yaw is known.

The filter integrates speed input and range observations from RFID for localization.

Initial position is not needed.

Ref:

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)

# Mapping

## Gaussian grid map

This is a 2D Gaussian grid mapping example.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif)

## Ray casting grid map

This is a 2D ray casting grid mapping example.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif)

## k-means object clustering

This is a 2D object clustering with k-means algorithm.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif)

## Rectangle fitting

This is a 2D rectangle fitting for vehicle detection.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif)


# SLAM

Simultaneous Localization and Mapping(SLAM) examples

## Iterative Closest Point (ICP) Matching

This is a 2D ICP matching example with singular value decomposition.

It can calculate a rotation matrix and a translation vector between points to points.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif)

Ref:

- [Introduction to Mobile Robotics: Iterative Closest Point Algorithm](https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf)


## FastSLAM 1.0

This is a feature based SLAM example using FastSLAM 1.0.

The blue line is ground truth the black line is dead reckoning the red line is the estimated trajectory with FastSLAM.

The red points are particles of FastSLAM.

Black points are landmarks blue crosses are estimated landmark positions by FastSLAM.


![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif)


Ref:

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)

- [SLAM simulations by Tim Bailey](http://www-personal.acfr.usyd.edu.au/tbailey/software/slam_simulations.htm)


## Graph based SLAM

This is a graph based SLAM example.

The blue line is ground truth.

The black line is dead reckoning.

The red line is the estimated trajectory with Graph based SLAM.

The black stars are landmarks for graph edge generation.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/GraphBasedSLAM/animation.gif)

Ref:

- [A Tutorial on Graph-Based SLAM](http://www2.informatik.uni-freiburg.de/~stachnis/pdf/grisetti10titsmag.pdf)


# Path Planning

## Dynamic Window Approach

This is a 2D navigation sample code with Dynamic Window Approach.

- [The Dynamic Window Approach to Collision Avoidance](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf)

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif)


## Grid based search

### Dijkstra algorithm

This is a 2D grid based shortest path planning with Dijkstra's algorithm.

![PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif)

In the animation cyan points are searched nodes.

### A\* algorithm

This is a 2D grid based shortest path planning with A star algorithm.

![PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif)

In the animation cyan points are searched nodes.

Its heuristic is 2D Euclid distance.

### Potential Field algorithm

This is a 2D grid based path planning with Potential Field algorithm.

![PotentialField](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif)

In the animation the blue heat map shows potential value on each grid.

Ref:

- [Robotic Motion Planning:Potential Functions](https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf)

## State Lattice Planning

This script is a path planning code with state lattice planning.

This code uses the model predictive trajectory generator to solve boundary problem.

Ref: 

- [Optimal rough terrain trajectory generation for wheeled mobile robots](http://journals.sagepub.com/doi/pdf/10.1177/0278364906075328)

- [State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments](http://www.frc.ri.cmu.edu/~alonzo/pubs/papers/JFR_08_SS_Sampling.pdf)


### Biased polar sampling

![PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif)


### Lane sampling

![PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif)

## Probabilistic Road-Map (PRM) planning 

![PRM](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif)

This PRM planner uses Dijkstra method for graph search.

In the animation blue points are sampled points

Cyan crosses means searched points with Dijkstra method

The red line is the final path of PRM.

Ref:

- [Probabilistic roadmap \- Wikipedia](https://en.wikipedia.org/wiki/Probabilistic_roadmap)

　　

## Rapidly-Exploring Random Trees (RRT)

### RRT\*

![PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif)

This is a path planning code with RRT\*

Black circles are obstacles green line is a searched tree red crosses are start and goal positions.

Ref:

- [Incremental Sampling-based Algorithms for Optimal Motion Planning](https://arxiv.org/abs/1005.0416)

- [Sampling-based Algorithms for Optimal Motion Planning](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.419.5503&amp;rep=rep1&amp;type=pdf)

### RRT\* with reeds-sheep path

![Robotics/animation.gif at master · AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif))

Path planning for a car robot with RRT\* and reeds sheep path planner.

### LQR-RRT\*

This is a path planning simulation with LQR-RRT\*.

A double integrator motion model is used for LQR local planner.

![LQRRRT](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif)

Ref:

- [LQR\-RRT\*: Optimal Sampling\-Based Motion Planning with Automatically Derived Extension Heuristics](http://lis.csail.mit.edu/pubs/perez-icra12.pdf)

- [MahanFathi/LQR\-RRTstar: LQR\-RRT\* method is used for random motion planning of a simple pendulum in its phase plot](https://github.com/MahanFathi/LQR-RRTstar)


## Quintic polynomials planning

Motion planning with quintic polynomials.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif)

It can calculate 2D path velocity and acceleration profile based on quintic polynomials.

Ref:

- [Local Path Planning And Motion Control For Agv In Positioning](http://ieeexplore.ieee.org/document/637936/)

## Reeds Shepp planning

A sample code with Reeds Shepp path planning.

![RSPlanning](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true)

Ref:

- [15.3.2 Reeds\-Shepp Curves](http://planning.cs.uiuc.edu/node822.html) 

- [optimal paths for a car that goes both forwards and backwards](https://pdfs.semanticscholar.org/932e/c495b1d0018fd59dee12a0bf74434fac7af4.pdf)

- [ghliu/pyReedsShepp: Implementation of Reeds Shepp curve\.](https://github.com/ghliu/pyReedsShepp)


## LQR based path planning

A sample code using LQR based path planning for double integrator model.

![RSPlanning](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true)


## Optimal Trajectory in a Frenet Frame 

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif)

This is optimal trajectory generation in a Frenet Frame.

The cyan line is the target course and black crosses are obstacles.

The red line is predicted path.

Ref:

- [Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame](https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf)

- [Optimal trajectory generation for dynamic street scenarios in a Frenet Frame](https://www.youtube.com/watch?v=Cj6tAQe7UCY)


# Path Tracking

## move to a pose control

This is a simulation of moving to a pose control

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/move_to_pose/animation.gif)

Ref:

- [P. I. Corke ""Robotics Vision and Control"" \| SpringerLink p102](https://link.springer.com/book/10.1007/978-3-642-20144-8)


## Stanley control

Path tracking simulation with Stanley steering control and PID speed control.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif)

Ref:

- [Stanley: The robot that won the DARPA grand challenge](http://robots.stanford.edu/papers/thrun.stanley05.pdf)

- [Automatic Steering Methods for Autonomous Automobile Path Tracking](https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf)



## Rear wheel feedback control

Path tracking simulation with rear wheel feedback steering control and PID speed control.

![PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif)

Ref:

- [A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles](https://arxiv.org/abs/1604.07446)


## Linear–quadratic regulator (LQR) speed and steering control

Path tracking simulation with LQR speed and steering control.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif)

Ref:

- [Towards fully autonomous driving: Systems and algorithms \- IEEE Conference Publication](http://ieeexplore.ieee.org/document/5940562/)


## Model predictive speed and steering control

Path tracking simulation with iterative linear model predictive speed and steering control.

<img src=""https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif"" width=""640""/>

Ref:

- [notebook](https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathTracking/model_predictive_speed_and_steer_control/Model_predictive_speed_and_steering_control.ipynb)

## Nonlinear Model predictive control with C-GMRES

A motion planning and path tracking simulation with NMPC of C-GMRES 

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif)

Ref:

- [notebook](https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathTracking/cgmres_nmpc/cgmres_nmpc.ipynb)


# Arm Navigation

## N joint arm to point control

N joint arm to a point control simulation.

This is a interactive simulation.

You can set the goal position of the end effector with left-click on the ploting area. 

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif)

In this simulation N = 10 however you can change it.

## Arm navigation with obstacle avoidance 

Arm navigation with obstacle avoidance simulation.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif)


# Aerial Navigation

## drone 3d trajectory following 

This is a 3d trajectory following simulation for a quadrotor.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif)

## rocket powered landing

This is a 3d trajectory generation simulation for a rocket powered landing.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif)

Ref:

- [notebook](https://github.com/AtsushiSakai/PythonRobotics/blob/master/AerialNavigation/rocket_powered_landing/rocket_powered_landing.ipynb)

# Bipedal

## bipedal planner with inverted pendulum

This is a bipedal planner for modifying footsteps with inverted pendulum.

You can set the footsteps and the planner will modify those automatically.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif)

# License 

MIT

# Use-case

See: [users\_comments](https://github.com/AtsushiSakai/PythonRobotics/blob/master/users_comments.md)

# Contribution

A small PR like bug fix is welcome.

If your PR is merged multiple times I will add your account to the author list.

# Support

If you or your company would like to support this project please consider:

- [Become a backer or sponsor on Patreon](https://www.patreon.com/myenigma)

- [One-time donation via PayPal](https://www.paypal.me/myenigmapay/)

You can add your name or your company logo in README if you are a patron.

E-mail consultant is also available.

 　

Your comment using [![Say Thanks!](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/AtsushiSakai) is also welcome. 

This is a list: [Users comments](https://github.com/AtsushiSakai/PythonRobotics/blob/master/users_comments.md)

# Authors

- [Atsushi Sakai](https://github.com/AtsushiSakai/) ([@Atsushi_twi](https://twitter.com/Atsushi_twi))

- [Daniel Ingram](https://github.com/daniel-s-ingram)

- [Joe Dinius](https://github.com/jwdinius)

- [Karan Chawla](https://github.com/karanchawla)

- [Antonin RAFFIN](https://github.com/araffin)

- [Alexis Paques](https://github.com/AlexisTM)"
"Abstract:  From the early days of computing games have been important testbeds for
studying how well machines can do sophisticated decision making. In recent
years machine learning has made dramatic advances with artificial agents
reaching superhuman performance in challenge domains like Go Atari and some
variants of poker. As with their predecessors of chess checkers and
backgammon these game domains have driven research by providing sophisticated
yet well-defined challenges for artificial intelligence practitioners. We
continue this tradition by proposing the game of Hanabi as a new challenge
domain with novel problems that arise from its combination of purely
cooperative gameplay and imperfect information in a two to five player setting.
In particular we argue that Hanabi elevates reasoning about the beliefs and
intentions of other agents to the foreground. We believe developing novel
techniques capable of imbuing artificial agents with such theory of mind will
not only be crucial for their success in Hanabi but also in broader
collaborative efforts and especially those with human partners. To facilitate
future research we introduce the open-source Hanabi Learning Environment
propose an experimental framework for the research community to evaluate
algorithmic advances and assess the performance of current state-of-the-art
techniques.
This is not an officially supported Google product.

hanabi\_learning\_environment is a research platform for Hanabi experiments. The file rl\_env.py provides an RL environment using an API similar to OpenAI Gym. A lower level game interface is provided in pyhanabi.py for non-RL methods like Monte Carlo tree search.

### Getting started
```
sudo apt-get install g++         # if you don't already have a CXX compiler
sudo apt-get install cmake       # if you don't already have CMake
sudo apt-get install python-pip  # if you don't already have pip
pip install cffi                 # if you don't already have cffi
cmake .
make
python rl_env_example.py         # Runs RL episodes
python game_example.py           # Plays a game using the lower level interface
```"
"Abstract:  We introduce a new family of deep neural network models. Instead of
specifying a discrete sequence of hidden layers we parameterize the derivative
of the hidden state using a neural network. The output of the network is
computed using a black-box differential equation solver. These continuous-depth
models have constant memory cost adapt their evaluation strategy to each
input and can explicitly trade numerical precision for speed. We demonstrate
these properties in continuous-depth residual networks and continuous-time
latent variable models. We also construct continuous normalizing flows a
generative model that can train by maximum likelihood without partitioning or
ordering the data dimensions. For training we show how to scalably
backpropagate through any ODE solver without access to its internal
operations. This allows end-to-end training of ODEs within larger models.
# Tensorflow Experiments on Neural Ordinary Differential Equations

&gt; You can contact me on twitter as [@mandubian](http://twitter.com/mandubian)

The notebook is a sandbox to test concepts exposed in this amazing paper:

This notebook is a sandbox to test concepts exposed in this amazing paper:

&gt; Neural Ordinary Differential Equations
&gt; http://arxiv.org/abs/1806.07366
&gt;
&gt; Authors: 
&gt;    Chen Ricky T. Q.
&gt;    Rubanova Yulia
&gt;    Bettencourt Jesse
&gt;    Duvenaud David

My idea is to reproduce the concepts exposed in the paper fully in Tensorflow using Eager-Mode and GradientTape. 

I didn't want to depend on Autograd and to be able to use classic Keras models.

For ODE Solver I wanted to compare implementations in TF with Scipy very robust ones (the only one I found is Runge-Kutta Dopri5).

### v1.1 [tf-neural-ode-v1.1.ipynb](tf-neural-ode-v1.1.ipynb)
- Added batched TF augmented gradient
- Added mini-batch optimization inspired by cool Pytorch implementation https://github.com/rtqichen/torchdiffeq allowing to have much faster converging &amp; deterministic training

### v1.0 [tf-neural-ode-v1.0.ipynb](tf-neural-ode-v1.0.ipynb)
- First Implementation of TF augmented gradient
- Samples with basic optimization on whole dataset

**Licensed under MIT License**"
"Abstract:  Git and GitHub are common tools for keeping track of multiple versions of
data analytic content which allow for more than one person to simultaneously
work on a project. GitHub Classroom aims to provide a way for students to work
on and submit their assignments via Git and GitHub giving teachers an
opportunity to teach these version control tools as part of their course. In
the Fall 2017 semester we implemented GitHub Classroom in two educational
settings--an introductory computational statistics lab and a more advanced
computational statistics course. We found many educational benefits of
implementing GitHub Classroom such as easily providing coding feedback during
assignments and making students more confident in their ability to collaborate
and use version control tools for future data science work. To encourage and
ease the transition into using GitHub Classroom we provide free and publicly
available resources--both for students to begin using Git/GitHub and for
teachers to use GitHub Classroom for their own courses.
# Tensorflow Experiments on Neural Ordinary Differential Equations

&gt; You can contact me on twitter as [@mandubian](http://twitter.com/mandubian)

The notebook is a sandbox to test concepts exposed in this amazing paper:

This notebook is a sandbox to test concepts exposed in this amazing paper:

&gt; Neural Ordinary Differential Equations
&gt; http://arxiv.org/abs/1806.07366
&gt;
&gt; Authors: 
&gt;    Chen Ricky T. Q.
&gt;    Rubanova Yulia
&gt;    Bettencourt Jesse
&gt;    Duvenaud David

My idea is to reproduce the concepts exposed in the paper fully in Tensorflow using Eager-Mode and GradientTape. 

I didn't want to depend on Autograd and to be able to use classic Keras models.

For ODE Solver I wanted to compare implementations in TF with Scipy very robust ones (the only one I found is Runge-Kutta Dopri5).

### v1.1 [tf-neural-ode-v1.1.ipynb](tf-neural-ode-v1.1.ipynb)
- Added batched TF augmented gradient
- Added mini-batch optimization inspired by cool Pytorch implementation https://github.com/rtqichen/torchdiffeq allowing to have much faster converging &amp; deterministic training

### v1.0 [tf-neural-ode-v1.0.ipynb](tf-neural-ode-v1.0.ipynb)
- First Implementation of TF augmented gradient
- Samples with basic optimization on whole dataset

**Licensed under MIT License**"
"Abstract:  Unsupervised image-to-image translation has gained considerable attention due
to the recent impressive progress based on generative adversarial networks
(GANs). However previous methods often fail in challenging cases in
particular when an image has multiple target instances and a translation task
involves significant changes in shape e.g. translating pants to skirts in
fashion images. To tackle the issues we propose a novel method coined
instance-aware GAN (InstaGAN) that incorporates the instance information
(e.g. object segmentation masks) and improves multi-instance transfiguration.
The proposed method translates both an image and the corresponding set of
instance attributes while maintaining the permutation invariance property of
the instances. To this end we introduce a context preserving loss that
encourages the network to learn the identity function outside of target
instances. We also propose a sequential mini-batch inference/training technique
that handles multiple instances with a limited GPU memory and enhances the
network to generalize better for multiple instances. Our comparative evaluation
demonstrates the effectiveness of the proposed method on different image
datasets in particular in the aforementioned challenging cases. Code and
results are available in this https URL
# InstaGAN: Instance-aware Image-to-Image Translation

PyTorch implementation of [""InstaGAN: Instance-aware Image-to-Image Translation""](https://openreview.net/forum?id=ryxwJhC9YX) (ICLR 2019).
The implementation is based on the [official CycleGAN](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) code.
Our major contributions are in `./models/insta_gan_model.py` and `./models/networks.py`.

<img src=""imgs/intro.png""/>
<img src=""imgs/model.png""/>


## Getting Started
### Installation

- Clone this repository
```
git clone https://github.com/sangwoomo/instagan
```

- Install PyTorch 0.4+ and torchvision from http://pytorch.org and other dependencies (e.g. [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate)).
You can install all the dependencies by
```
pip install -r requirements.txt
```

- For Conda users you can use a script `./scripts/conda_deps.sh` to install PyTorch and other libraries.

- **Acknowledgment:** Installation scripts are from the [official CycleGAN](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) code.


### Download base datasets

- Download [clothing-co-parsing (CCP)](https://github.com/bearpaw/clothing-co-parsing) dataset:
```
git clone https://github.com/bearpaw/clothing-co-parsing ./datasets/clothing-co-parsing
```

- Download [multi-human parsing (MHP)](https://lv-mhp.github.io/) dataset:
```
# Download ""LV-MHP-v1"" from the link and locate in ./datasets
```

- Download [MS COCO](http://cocodataset.org/) dataset:
```
./datasets/download_coco.sh
```

### Generate two-domain datasets

- Generate two-domain dataset for experiments:
```
python ./datasets/generate_ccp_dataset.py --save_root ./datasets/jeans2skirt_ccp --cat1 jeans --cat2 skirt
python ./datasets/generate_mhp_dataset.py --save_root ./datasets/pants2skirt_mhp --cat1 pants --cat2 skirt
python ./datasets/generate_coco_dataset.py --save_root ./datasets/shp2gir_coco --cat1 sheep --cat2 giraffe
```
- **Note:** Generated dataset contains images and corresponding masks which are located in image folders (e.g. 'trainA') and mask folders (e.g. 'trainA_seg') respectively.
For each image (e.g. '0001.png') corresponding masks for each instance (e.g. '0001_0.png' '0001_1.png' ...) are provided.

### Run experiments

- Train a model:
```
python train.py --dataroot ./datasets/jeans2skirt_ccp --model insta_gan --name jeans2skirt_ccp_instagan --loadSizeH 330 --loadSizeW 220 --fineSizeH 300 --fineSizeW 200 --niter 400 --niter_decay 200
python train.py --dataroot ./datasets/pants2skirt_mhp --model insta_gan --name pants2skirt_mhp_instagan --loadSizeH 270 --loadSizeW 180 --fineSizeH 240 --fineSizeW 160
python train.py --dataroot ./datasets/shp2gir_coco --model insta_gan --name shp2gir_coco_instagan --loadSizeH 220 --loadSizeW 220 --fineSizeH 200 --fineSizeW 200
```

- To view training results and loss plots run `python -m visdom.server` and click the URL http://localhost:8097.
To see more intermediate results check out `./checkpoints/experiment_name/web/index.html`.

- For faster experiment increase batch size and use more gpus:
```
python train.py --dataroot ./datasets/shp2gir_coco --model insta_gan --name shp2gir_coco_instagan --loadSizeH 220 --loadSizeW 220 --fineSizeH 200 --fineSizeW 200 --batch_size 4 --gpu_ids 0123
```

- Test the model:
```
python test.py --dataroot ./datasets/jeans2skirt_ccp --model insta_gan --name jeans2skirt_ccp_instagan --loadSizeH 300 --loadSizeW 200 --fineSizeH 300 --fineSizeW 200
python test.py --dataroot ./datasets/pants2skirt_mhp --model insta_gan --name pants2skirt_mhp_instagan --loadSizeH 240 --loadSizeW 160 --fineSizeH 240 --fineSizeW 160 --ins_per 2 --ins_max 20
python test.py --dataroot ./datasets/shp2gir_coco --model insta_gan --name shp2gir_coco_instagan --loadSizeH 200 --loadSizeW 200 --fineSizeH 200 --fineSizeW 200 --ins_per 2 --ins_max 20
```
- The test results will be saved to a html file here: `./results/experiment_name/latest_test/index.html`.


### Apply a pre-trained model

- You can download a pre-trained model (pants-&gt;skirt and/or sheep-&gt;giraffe) from the following [Google drive link](https://drive.google.com/drive/folders/10TfnuqZ4tIVAQP23cgHxJQKuVeJusu85?usp=sharing).
Save the pretrained model in `./checkpoints/` directory.

- We provide samples of two datasets (pants-&gt;skirt and sheep-&gt;giraffe) in this repository.
To test the model:
```
python test.py --dataroot ./datasets/pants2skirt_mhp --model insta_gan --name pants2skirt_mhp_instagan --loadSizeH 240 --loadSizeW 160 --fineSizeH 240 --fineSizeW 160 --ins_per 2 --ins_max 20 --phase sample --epoch 200
python test.py --dataroot ./datasets/shp2gir_coco --model insta_gan --name shp2gir_coco_instagan --loadSizeH 200 --loadSizeW 200 --fineSizeH 200 --fineSizeW 200 --ins_per 2 --ins_max 20 --phase sample --epoch 200
```


## Results

We provide some translation results of our model.
See the [**link**](/docs/more_results.md) for more translation results.

### 1. Fashion dataset (pants-&gt;skirt)

<img src=""imgs/results-1.png""/>

### 2. COCO dataset (sheep-&gt;giraffe)

<img src=""imgs/results-2.png""/>

### 3. Results on Google-searched images (pants-&gt;skirt)

<img src=""imgs/results-3.png""/>

### 4. Results on YouTube-searched videos (pants-&gt;skirt)

<img src=""imgs/results-4.png""/>


## Citation
If you use this code for your research please cite our papers.
```
@inproceedings{
    mo2018instagan
    title={InstaGAN: Instance-aware Image-to-Image Translation}
    author={Sangwoo Mo and Minsu Cho and Jinwoo Shin}
    booktitle={International Conference on Learning Representations}
    year={2019}
    url={https://openreview.net/forum?id=ryxwJhC9YX}
}
```"
"Abstract:  Neural architecture search (NAS) has been proposed to automatically tune deep
neural networks but existing search algorithms e.g. NASNet PNAS usually
suffer from expensive computational cost. Network morphism which keeps the
functionality of a neural network while changing its neural architecture could
be helpful for NAS by enabling more efficient training during the search. In
this paper we propose a novel framework enabling Bayesian optimization to
guide the network morphism for efficient neural architecture search. The
framework develops a neural network kernel and a tree-structured acquisition
function optimization algorithm to efficiently explores the search space.
Intensive experiments on real-world benchmark datasets have been done to
demonstrate the superior performance of the developed framework over the
state-of-the-art methods. Moreover we build an open-source AutoML system based
on our method namely Auto-Keras. The system runs in parallel on CPU and GPU
with an adaptive search strategy for different GPU memory limits.
<img alt=""drawing"" src=""https://github.com/keras-team/autokeras/blob/master/logo.png?raw=true"" style=""display: block; margin-left: auto; margin-right: auto"" width=""400px""/>

[![Build Status](https://travis-ci.org/keras-team/autokeras.svg?branch=master)](https://travis-ci.org/keras-team/autokeras)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/620bd322918c476aa33230ec911a4301)](https://www.codacy.com/app/jhfjhfj1/autokeras?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=keras-team/autokeras&amp;utm_campaign=Badge_Grade)
[![Codacy Badge](https://api.codacy.com/project/badge/Coverage/620bd322918c476aa33230ec911a4301)](https://www.codacy.com/app/jhfjhfj1/autokeras?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=keras-team/autokeras&amp;utm_campaign=Badge_Coverage)
<a href=""https://badge.fury.io/py/autokeras""><img alt=""PyPI version"" src=""https://badge.fury.io/py/autokeras.svg"" style=""width: 125px""/></a>

Official Website: [autokeras.com](https://autokeras.com)

Auto-Keras is an open source software library for automated machine learning (AutoML).
It is developed by <a href=""http://faculty.cs.tamu.edu/xiahu/index.html"" rel=""nofollow"" target=""_blank"">DATA Lab</a> at Texas A&amp;M University and community contributors.
The ultimate goal of AutoML is to provide easily accessible deep learning tools to domain experts with limited data science or machine learning background. 
Auto-Keras provides functions to automatically search for architecture and hyperparameters of deep learning models.

## Example

Here is a short example of using the package.

```python
import autokeras as ak

clf = ak.ImageClassifier()
clf.fit(x_train y_train)
results = clf.predict(x_test)
```

## Cite this work

Auto-Keras: An Efficient Neural Architecture Search System.
Haifeng Jin Qingquan Song and Xia Hu.
[arXiv:1806.10282](https://arxiv.org/abs/1806.10282).

Biblatex entry:

    @online{jin2018efficient
      author       = {Haifeng Jin and Qingquan Song and Xia Hu}
      title        = {Auto-Keras: An Efficient Neural Architecture Search System}
      date         = {2018-06-27}
      year         = {2018}
      eprintclass  = {cs.LG}
      eprinttype   = {arXiv}
      eprint       = {cs.LG/1806.10282}
    }

## Community

You can use Gitter to communicate with people who also interested in Auto-Keras.
<a href=""https://gitter.im/autokeras/Lobby?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge""><img alt=""Join the chat at https://gitter.im/autokeras/Lobby"" src=""https://badges.gitter.im/autokeras/Lobby.svg"" style=""width: 92px""/></a>

You can also follow us on Twitter [@autokeras](https://twitter.com/autokeras) for the latest news.

## Contributing Code

You can follow the [Contributing Guide](https://autokeras.com/temp/contribute/) for details.
The easist way to contribute is to resolve the issues with the ""[call for contributors](https://github.com/keras-team/autokeras/labels/call%20for%20contributors)"" tag.
They are friendly to beginners.
 
## Support Auto-Keras

We accept donations on [Open Collective](https://opencollective.com/autokeras).
Thank every backer for supporting us!

<a href=""https://opencollective.com/autokeras/donate"" target=""_blank"">
<img src=""https://opencollective.com/autokeras/donate/button@2x.png?color=blue"" width=""200"">
</img></a>


## DISCLAIMER

Please note that this is a **pre-release** version of the Auto-Keras which is still undergoing final testing before its official release. The website its software and all content found on it are provided on an
“as is” and “as available” basis. Auto-Keras does **not** give any warranties whether express or implied as to the suitability or usability of the website its software or any of its content. Auto-Keras will **not** be liable for any loss whether such loss is direct indirect special or consequential suffered by any party as a result of their use of the libraries or content. Any usage of the libraries is done at the user’s own risk and the user will be solely responsible for any damage to any computer system or loss of data that results from such activities. Should you encounter any bugs glitches lack of functionality or
other problems on the website please let us know immediately so we
can rectify these accordingly. Your help in this regard is greatly
appreciated.

## Acknowledgements

The authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&amp;M College of Engineering and Texas A&amp;M. "
"Abstract:  Energy efficiency has become an increasingly important concern in computer
architecture due to the end of Dennard scaling. Heterogeneity has been explored
as a way to achieve better energy efficiency and heterogeneous
microarchitecture chips have become common in the mobile setting.
Recent research has explored using heterogeneous-ISA heterogeneous
microarchitecture general-purpose cores to achieve further energy efficiency
gains. However there is no open-source hardware implementation of a
heterogeneous-ISA processor available for research and effective research on
heterogeneous-ISA processors necessitates the emulation speed provided by FPGA
prototyping. This work describes our experiences creating JuxtaPiton by
integrating a small RISC-V core into the OpenPiton framework which uses a
modified OpenSPARC T1 core. This is the first time a new core has been
integrated with the OpenPiton framework and JuxtaPiton is the first
open-source general-purpose heterogeneous-ISA processor. JuxtaPiton inherits
all the capabilities of OpenPiton including vital FPGA emulation
infrastructure which can boot full-stack Debian Linux. Using this
infrastructure we investigate area and timing effects of using the new RISC-V
core on FPGA and the performance of the new core running microbenchmarks.
<img alt=""drawing"" src=""https://github.com/keras-team/autokeras/blob/master/logo.png?raw=true"" style=""display: block; margin-left: auto; margin-right: auto"" width=""400px""/>

[![Build Status](https://travis-ci.org/keras-team/autokeras.svg?branch=master)](https://travis-ci.org/keras-team/autokeras)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/620bd322918c476aa33230ec911a4301)](https://www.codacy.com/app/jhfjhfj1/autokeras?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=keras-team/autokeras&amp;utm_campaign=Badge_Grade)
[![Codacy Badge](https://api.codacy.com/project/badge/Coverage/620bd322918c476aa33230ec911a4301)](https://www.codacy.com/app/jhfjhfj1/autokeras?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=keras-team/autokeras&amp;utm_campaign=Badge_Coverage)
<a href=""https://badge.fury.io/py/autokeras""><img alt=""PyPI version"" src=""https://badge.fury.io/py/autokeras.svg"" style=""width: 125px""/></a>

Official Website: [autokeras.com](https://autokeras.com)

Auto-Keras is an open source software library for automated machine learning (AutoML).
It is developed by <a href=""http://faculty.cs.tamu.edu/xiahu/index.html"" rel=""nofollow"" target=""_blank"">DATA Lab</a> at Texas A&amp;M University and community contributors.
The ultimate goal of AutoML is to provide easily accessible deep learning tools to domain experts with limited data science or machine learning background. 
Auto-Keras provides functions to automatically search for architecture and hyperparameters of deep learning models.

## Example

Here is a short example of using the package.

```python
import autokeras as ak

clf = ak.ImageClassifier()
clf.fit(x_train y_train)
results = clf.predict(x_test)
```

## Cite this work

Auto-Keras: An Efficient Neural Architecture Search System.
Haifeng Jin Qingquan Song and Xia Hu.
[arXiv:1806.10282](https://arxiv.org/abs/1806.10282).

Biblatex entry:

    @online{jin2018efficient
      author       = {Haifeng Jin and Qingquan Song and Xia Hu}
      title        = {Auto-Keras: An Efficient Neural Architecture Search System}
      date         = {2018-06-27}
      year         = {2018}
      eprintclass  = {cs.LG}
      eprinttype   = {arXiv}
      eprint       = {cs.LG/1806.10282}
    }

## Community

You can use Gitter to communicate with people who also interested in Auto-Keras.
<a href=""https://gitter.im/autokeras/Lobby?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge""><img alt=""Join the chat at https://gitter.im/autokeras/Lobby"" src=""https://badges.gitter.im/autokeras/Lobby.svg"" style=""width: 92px""/></a>

You can also follow us on Twitter [@autokeras](https://twitter.com/autokeras) for the latest news.

## Contributing Code

You can follow the [Contributing Guide](https://autokeras.com/temp/contribute/) for details.
The easist way to contribute is to resolve the issues with the ""[call for contributors](https://github.com/keras-team/autokeras/labels/call%20for%20contributors)"" tag.
They are friendly to beginners.
 
## Support Auto-Keras

We accept donations on [Open Collective](https://opencollective.com/autokeras).
Thank every backer for supporting us!

<a href=""https://opencollective.com/autokeras/donate"" target=""_blank"">
<img src=""https://opencollective.com/autokeras/donate/button@2x.png?color=blue"" width=""200"">
</img></a>


## DISCLAIMER

Please note that this is a **pre-release** version of the Auto-Keras which is still undergoing final testing before its official release. The website its software and all content found on it are provided on an
“as is” and “as available” basis. Auto-Keras does **not** give any warranties whether express or implied as to the suitability or usability of the website its software or any of its content. Auto-Keras will **not** be liable for any loss whether such loss is direct indirect special or consequential suffered by any party as a result of their use of the libraries or content. Any usage of the libraries is done at the user’s own risk and the user will be solely responsible for any damage to any computer system or loss of data that results from such activities. Should you encounter any bugs glitches lack of functionality or
other problems on the website please let us know immediately so we
can rectify these accordingly. Your help in this regard is greatly
appreciated.

## Acknowledgements

The authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&amp;M College of Engineering and Texas A&amp;M. "
"Abstract:  In lifelong learning the learner is presented with a sequence of tasks
incrementally building a data-driven prior which may be leveraged to speed up
learning of a new task. In this work we investigate the efficiency of current
lifelong approaches in terms of sample complexity computational and memory
cost. Towards this end we first introduce a new and a more realistic
evaluation protocol whereby learners observe each example only once and
hyper-parameter selection is done on a small and disjoint set of tasks which
is not used for the actual learning experience and evaluation. Second we
introduce a new metric measuring how quickly a learner acquires a new skill.
Third we propose an improved version of GEM (Lopez-Paz & Ranzato 2017)
dubbed Averaged GEM (A-GEM) which enjoys the same or even better performance
as GEM while being almost as computationally and memory efficient as EWC
(Kirkpatrick et al. 2016) and other regularization-based methods. Finally we
show that all algorithms including A-GEM can learn even more quickly if they
are provided with task descriptors specifying the classification tasks under
consideration. Our experiments on several standard lifelong learning benchmarks
demonstrate that A-GEM has the best trade-off between accuracy and efficiency.
# Efficient Lifelong Learning with A-GEM

This is the official implementation of the [Averaged Gradient Episodic Memory (A-GEM)](https://arxiv.org/abs/1812.00420) and [Experience Replay with Tiny Memories](http://arxiv.org/abs/1902.10486) in Tensorflow.

## Requirements

TensorFlow &gt;= v1.9.0.

## Training

To replicate the results of the paper on a particular dataset execute (see the Note below for downloading the CUB and AWA datasets):
```bash
$ ./replicate_results_iclr19.sh <dataset> <thread-id> <je>
```
Example runs are:
```bash
$ ./replicate_results_iclr19.sh MNIST 3      /* Train PNN and A-GEM on MNIST */
$ ./replicate_results_iclr19.sh CUB 1 1      /* Train JE models of RWALK and A-GEM on CUB */
```

### Note
For CUB and AWA experiments download the dataset prior to running the above script. Run following for downloading the datasets:

```bash
$ ./download_cub_awa.sh
```
The plotting code is provided under the folder `plotting_code/`. Update the paths in the plotting code accordingly.
 
## Experience Replay
 The code provides an implementation of experience replay (ER) with reservoir sampling on MNIST and CIFAR datasets. To run the ER experiments execute the following script:
```bash
$ ./replicate_results_er.sh
```

When using this code please cite our papers:

```
@inproceedings{AGEM
  title={Efficient Lifelong Learning with A-GEM}
  author={Chaudhry Arslan and Ranzato Marc’Aurelio and Rohrbach Marcus and Elhoseiny Mohamed}
  booktitle={ICLR}
  year={2019}
}

@article{chaudhryER_2019
  title={Continual Learning with Tiny Episodic Memories}
  author={Chaudhry Arslan and Rohrbach Marcus and Elhoseiny Mohamed and Ajanthan Thalaiyasingam and Dokania Puneet K and Torr Philip HS and Ranzato Marc’Aurelio}
  journal={arXiv preprint arXiv:1902.10486 2019}
  year={2019}
}

@inproceedings{chaudhry2018riemannian
  title={Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence}
  author={Chaudhry Arslan and Dokania Puneet K and Ajanthan Thalaiyasingam and Torr Philip HS}
  booktitle={ECCV}
  year={2018}
}
```

## Questions/ Bugs
* For questions contact the author Arslan Chaudhry (arslan.chaudhry@eng.ox.ac.uk).
* Feel free to open the bugs if anything is broken.

## License
This source code is released under The MIT License found in the LICENSE file in the root directory of this source tree. 
</je></thread-id></dataset>"
"Abstract:  Multi-level hierarchies have the potential to accelerate learning in sparse
reward tasks because they can divide a problem into a set of short horizon
subproblems. In order to realize this potential Hierarchical Reinforcement
Learning (HRL) algorithms need to be able to learn the multiple levels within a
hierarchy in parallel so these simpler subproblems can be solved
simultaneously. Yet most existing HRL methods that can learn hierarchies are
not able to efficiently learn multiple levels of policies at the same time
particularly in continuous domains. To address this problem we introduce a
framework that can learn multiple levels of policies in parallel. Our approach
consists of two main components: (i) a particular hierarchical architecture and
(ii) a method for jointly learning multiple levels of policies. The hierarchies
produced by our framework are comprised of a set of nested goal-conditioned
policies that use the state space to decompose a task into short subtasks. All
policies in the hierarchy are learned simultaneously using two types of
hindsight transitions. We demonstrate experimentally in both grid world and
simulated robotics domains that our approach can significantly accelerate
learning relative to other non-hierarchical and hierarchical methods. Indeed
our framework is the first to successfully learn 3-level hierarchies in
parallel in tasks with continuous state and action spaces.
# Hierarchical Actor-Critc (HAC)
This repository contains the code to implement the *Hierarchical Actor-Critic (HAC)* algorithm.  HAC helps agents learn tasks more quickly by enabling them to break problems down into short sequences of actions.  The paper describing the algorithm is available [here](https://arxiv.org/abs/1712.00948).

To run HAC execute the command *""python3 initialize_HAC.py --retrain""*.  By default this will train a UR5 agent with a 3-level hierarchy to learn to achieve certain poses.  This UR5 agent should achieve a 90+% success rate in around 350 episodes.  The following [video](https://www.youtube.com/watch?v=R86Vs9Vb6Bc) shows how a 3-layered agent performed after 450 episodes of training.  In order to watch your trained agent execute the command *""python3 initialize_HAC.py --test --show""*.  Please note that in order to run this repository you must have (i) a MuJoCo [license](https://www.roboti.us/license.html) (ii) the required MuJoCo software [libraries](https://www.roboti.us/index.html) and (iii) the MuJoCo Python [wrapper](https://github.com/openai/mujoco-py) from OpenAI.  

To run HAC with your own agents and MuJoCo environments you need to complete the template in the *""design_agent_and_env.py""* file.  The *""example_designs""* folder contains other examples of design templates that build different agents in the UR5 reacher and inverted pendulum environments.

Happy to answer any questions you have.  Please email me at andrew_levy2@brown.edu.

## UPDATE LOG

### 10/12/2018 - Key Changes
1.  Bounded Q-Values

The Q-values output by the critic network at each level are now bounded between *[-T0]* in which *T* is the max sequence length in which each policy specializes as well as the negative of the subgoal penalty.  We use an upper bound of 0 because our code uses a nonpositive reward function.  Consequently Q-values should never be positive.  However we noticed that somtimes the critic function approximator would make small mistakes and assign positive Q-values which occassionally proved harmful to results.  In addition we observed improved results when we used a tighter lower bound of *-T* (i.e. the subgoal penalty).  The improved results may result from the increased flexibility the bounded Q-values provides the critic.  The critic can assign a value of *-T* to any (stateactiongoal) tuple in which the action does not bring the agent close to the goal instead of having to learn the exact value.

2.  Removed Target Networks

We also noticed improved results when we used the regular Q-networks to determine the Bellman target updates (i.e. *reward + Q(next statepi(next state)goal)*) instead of the separate target networks that are used in DDPG.  The default setting of our code base thus no longer uses target networks.  However the target networks can be easily activated by making the changes specified in (i) the *""learn""* method in the *""layer.py""* file and (ii) the *""update""* method in the *""critic.py""* file.  

3.  Centralized Design Template

Users can now configure the agent and environment in the single file *""design_agent_and_env.py""*.  This template file contains most of the significant hyperparameters in HAC.  We have removed the command-line options that can change the architecture of the agent's hierarchy.

4.  Added UR5 Reacher Environment

We have added a new UR5 reacher environment in which a UR5 agent can learn to achieve various poses.  The *""ur5.xml""* MuJoCo file also contains commented code for a Robotiq gripper if you would like to augment the agent.  Additional environments will hopefully be added shortly.  "
"Abstract:  In this paper we introduce the concept of network semantic segmentation for
social network analysis. We consider the GitHub social coding network which has
been a center of attention for both researchers and software developers.
Network semantic segmentation describes the process of associating each user
with a class label such as a topic of interest. We augment node attributes with
network significant connections and then employ machine learning approaches to
cluster the users. We compare the results with a network segmentation performed
using community detection algorithms and one executed by clustering with node
attributes. Results are compared in terms of community diversity within the
semantic segments along with topic
# endpoints 

[![Gitter](https://badges.gitter.im/.svg)](https://gitter.im/julienrf/endpoints)
[![codecov.io](http://codecov.io/github/julienrf/endpoints/coverage.svg?branch=master)](http://codecov.io/github/julienrf/endpoints?branch=master)
[![Build Status](https://travis-ci.org/julienrf/endpoints.svg?branch=master)](https://travis-ci.org/julienrf/endpoints)


*endpoints* is a Scala library for defining communication protocols over HTTP between
applications.

See the [documentation](http://julienrf.github.io/endpoints) to learn more.

## Running the Examples

~~~
$ sbt
&gt; wow 2.12.4
&gt; <example>/reStart
~~~

Where `<example>` can be either
[`example-cqrs`](documentation/examples/cqrs)
[`example-documented`](documentation/examples/documented)
or [`example-quickstart-server`](documentation/examples/quickstart).

And then browse http://localhost:9000.

## Contributing

See the [open issues](https://github.com/julienrf/endpoints/issues).

## License

This content is released under the [MIT License](http://opensource.org/licenses/mit-license.php).
</example></example>"
"Abstract:  In the era of big data reducing data dimensionality is critical in many
areas of science. Widely used Principal Component Analysis (PCA) addresses this
problem by computing a low dimensional data embedding that maximally explain
variance of the data. However PCA has two major weaknesses. Firstly it only
considers linear correlations among variables (features) and secondly it is
not suitable for categorical data. We resolve these issues by proposing
Maximally Correlated Principal Component Analysis (MCPCA). MCPCA computes
transformations of variables whose covariance matrix has the largest Ky Fan
norm. Variable transformations are unknown can be nonlinear and are computed
in an optimization. MCPCA can also be viewed as a multivariate extension of
Maximal Correlation. For jointly Gaussian variables we show that the covariance
matrix corresponding to the identity (or the negative of the identity)
transformations majorizes covariance matrices of non-identity functions. Using
this result we characterize global MCPCA optimizers for nonlinear functions of
jointly Gaussian variables for every rank constraint. For categorical variables
we characterize global MCPCA optimizers for the rank one constraint based on
the leading eigenvector of a matrix computed using pairwise joint
distributions. For a general rank constraint we propose a block coordinate
descend algorithm and show its convergence to stationary points of the MCPCA
optimization. We compare MCPCA with PCA and other state-of-the-art
dimensionality reduction methods including Isomap LLE multilayer autoencoders
(neural networks) kernel PCA probabilistic PCA and diffusion maps on several
synthetic and real datasets. We show that MCPCA consistently provides improved
performance compared to other methods.
# MCPCA
Reference:
Soheil Feizi and David Tse Maximally Correlated Principal Component Analysis arXiv:1702.05471

To apply MCPCA on datasets with discrete (categorical) features use:
MCPCA_sample_disc_wrapper.m 

To apply MCPCA on datasets with continuous features use: MCPCA_sample_polynomial_wrapper.m

To apply MCPCA on datasets with mixed discrete and continuous features use:
MCPCA_sample_mixed_wrapper.m 

For an example run of MCPCA on discrete data see: 
example_MCPCA_discrete.m

For an example run of MCPCA on continuous data see:
example_MCPCA_cont.m"
"Abstract:  From the early days of computing games have been important testbeds for
studying how well machines can do sophisticated decision making. In recent
years machine learning has made dramatic advances with artificial agents
reaching superhuman performance in challenge domains like Go Atari and some
variants of poker. As with their predecessors of chess checkers and
backgammon these game domains have driven research by providing sophisticated
yet well-defined challenges for artificial intelligence practitioners. We
continue this tradition by proposing the game of Hanabi as a new challenge
domain with novel problems that arise from its combination of purely
cooperative gameplay and imperfect information in a two to five player setting.
In particular we argue that Hanabi elevates reasoning about the beliefs and
intentions of other agents to the foreground. We believe developing novel
techniques capable of imbuing artificial agents with such theory of mind will
not only be crucial for their success in Hanabi but also in broader
collaborative efforts and especially those with human partners. To facilitate
future research we introduce the open-source Hanabi Learning Environment
propose an experimental framework for the research community to evaluate
algorithmic advances and assess the performance of current state-of-the-art
techniques.
This is not an officially supported Google product.

hanabi\_learning\_environment is a research platform for Hanabi experiments. The file rl\_env.py provides an RL environment using an API similar to OpenAI Gym. A lower level game interface is provided in pyhanabi.py for non-RL methods like Monte Carlo tree search.

### Getting started
```
sudo apt-get install g++         # if you don't already have a CXX compiler
sudo apt-get install cmake       # if you don't already have CMake
sudo apt-get install python-pip  # if you don't already have pip
pip install cffi                 # if you don't already have cffi
cmake .
make
python rl_env_example.py         # Runs RL episodes
python game_example.py           # Plays a game using the lower level interface
```"
"Abstract:  Social coding platforms such as GitHub can serve as natural laboratories
for studying the diffusion of innovation through tracking the pattern of code
adoption by programmers. This paper focuses on the problem of predicting the
popularity of software repositories over time; our aim is to forecast the time
series of popularity-related events (code forks and watches). In particular we
are interested in cross-repository patterns-how do events on one repository
affect other repositories? Our proposed LSTM (Long Short-Term Memory) recurrent
neural network integrates events across multiple active repositories
outperforming a standard ARIMA (Auto-Regressive Integrated Moving Average) time
series prediction based on the single repository. The ability of the LSTM to
leverage cross-repository information gives it a significant edge over standard
time series forecasting.
This is not an officially supported Google product.

hanabi\_learning\_environment is a research platform for Hanabi experiments. The file rl\_env.py provides an RL environment using an API similar to OpenAI Gym. A lower level game interface is provided in pyhanabi.py for non-RL methods like Monte Carlo tree search.

### Getting started
```
sudo apt-get install g++         # if you don't already have a CXX compiler
sudo apt-get install cmake       # if you don't already have CMake
sudo apt-get install python-pip  # if you don't already have pip
pip install cffi                 # if you don't already have cffi
cmake .
make
python rl_env_example.py         # Runs RL episodes
python game_example.py           # Plays a game using the lower level interface
```"
"Abstract:  Much of machine learning research focuses on producing models which perform
well on benchmark tasks in turn improving our understanding of the challenges
associated with those tasks. From the perspective of ML researchers the
content of the task itself is largely irrelevant and thus there have
increasingly been calls for benchmark tasks to more heavily focus on problems
which are of social or cultural relevance. In this work we introduce
Kuzushiji-MNIST a dataset which focuses on Kuzushiji (cursive Japanese) as
well as two larger more challenging datasets Kuzushiji-49 and
Kuzushiji-Kanji. Through these datasets we wish to engage the machine learning
community into the world of classical Japanese literature. Dataset available at
this https URL
# Kuzushiji-MNIST

[![License: CC BY-SA 4.0](https://img.shields.io/badge/License-CC%20BY--SA%204.0-blue.svg)](https://creativecommons.org/licenses/by-sa/4.0/)  
ðŸ“š [Read the paper](https://arxiv.org/abs/1812.01718) to learn more about Kuzushiji the datasets and our motivations for making them!

## Citing Kuzushiji-MNIST

If you use any of the Kuzushiji datasets in your work we would appreciate a reference to our paper:

**Deep Learning for Classical Japanese Literature. Tarin Clanuwat et al. [arXiv:1812.01718](https://arxiv.org/abs/1812.01718)**

```latex
@online{clanuwat2018deep
  author       = {Tarin Clanuwat and Mikel Bober-Irizar and Asanobu Kitamoto and Alex Lamb and Kazuaki Yamamoto and David Ha}
  title        = {Deep Learning for Classical Japanese Literature}
  date         = {2018-12-03}
  year         = {2018}
  eprintclass  = {cs.CV}
  eprinttype   = {arXiv}
  eprint       = {cs.CV/1812.01718}
}
```
## News and Updates
**IMPORTANT** If you downloaded the KMNIST and K49 datasets before **5 February 2019** please kindly download the dataset and run your code again. We fixed minor bugs due to image processing and it might slightly change the result number. Thank you for your coorporation.

## The Dataset

**Kuzushiji-MNIST** is a drop-in replacement for the MNIST dataset (28x28 grayscale 70000 images) provided in the original MNIST format as well as a NumPy format. Since MNIST restricts us to 10 classes we chose one character to represent each of the 10 rows of Hiragana when creating Kuzushiji-MNIST.

**Kuzushiji-49** as the name suggests has 49 classes (28x28 grayscale 270912 images) is a much larger but imbalanced dataset containing 48 Hiragana characters and one Hiragana iteration mark.

**Kuzushiji-Kanji** is an imbalanced dataset of total 3832 Kanji characters (64x64 grayscale 140426 images) ranging from 1766 examples to only a single example per class.

<p align=""center"">
<img src=""images/kmnist_examples.png""/>
  The 10 classes of Kuzushiji-MNIST with the first column showing each character's modern hiragana counterpart.
</p>

## Get the data ðŸ’¾

ðŸŒŸ You can run [`python download_data.py`](download_data.py) to interactively select and download any of these datasets!

### Kuzushiji-MNIST

Kuzushiji-MNIST contains 70000 28x28 grayscale images spanning 10 classes (one from each column of [hiragana](https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Table_hiragana.svg/768px-Table_hiragana.svg.png)) and is perfectly balanced like the original MNIST dataset (6k/1k train/test for each class).

| File            | Examples | Download (MNIST format)    | Download (NumPy format)      |
|-----------------|--------------------|----------------------------|------------------------------|
| Training images | 60000             | [train-images-idx3-ubyte.gz](http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz) (18MB) | [kmnist-train-imgs.npz](http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz) (18MB)   |
| Training labels | 60000             | [train-labels-idx1-ubyte.gz](http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz) (30KB) | [kmnist-train-labels.npz](http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz) (30KB)  |
| Testing images  | 10000             | [t10k-images-idx3-ubyte.gz](http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz) (3MB) | [kmnist-test-imgs.npz](http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz) (3MB)   |
| Testing labels  | 10000             | [t10k-labels-idx1-ubyte.gz](http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz) (5KB)  | [kmnist-test-labels.npz](http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz) (5KB) |

Mapping from class indices to characters: [kmnist_classmap.csv](http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist_classmap.csv) (1KB)

We recommend using standard top-1 accuracy on the test set for evaluating on Kuzushiji-MNIST.

##### Which format do I download?
If you're looking for a drop-in replacement for the MNIST or Fashion-MNIST dataset (for tools that currently work with these datasets) download the data in MNIST format.

Otherwise it's recommended to download in NumPy format which can be loaded into an array as easy as:  
`arr = np.load(filename)['arr_0']`.

### Kuzushiji-49

Kuzushiji-49 contains 270912 images spanning 49 classes and is an extension of the Kuzushiji-MNIST dataset.

| File            | Examples |  Download (NumPy format)      |
|-----------------|--------------------|----------------------------|
| Training images | 232365            | [k49-train-imgs.npz](http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-imgs.npz) (63MB)   |
| Training labels | 232365            | [k49-train-labels.npz](http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-labels.npz) (200KB)  |
| Testing images  | 38547             | [k49-test-imgs.npz](http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-imgs.npz) (11MB)   |
| Testing labels  | 38547             | [k49-test-labels.npz](http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-labels.npz) (50KB) |

Mapping from class indices to characters: [k49_classmap.csv](http://codh.rois.ac.jp/kmnist/dataset/k49/k49_classmap.csv) (1KB)

We recommend using balanced accuracy on the test set for evaluating on Kuzushiji-49.

### Kuzushiji-Kanji

Kuzushiji-Kanji is a large and highly imbalanced 64x64 dataset of 3832 Kanji characters containing 140426 images of both common and rare characters.  

The full dataset is available for download [here](http://codh.rois.ac.jp/kmnist/dataset/kkanji/kkanji.tar) (310MB).  
We plan to release a train/test split version as a low-shot learning dataset very soon.

![Examples of Kuzushiji-Kanji classes](images/kkanji_examples.png)

## Benchmarks &amp; Results ðŸ“ˆ

Have more results to add to the table? Feel free to submit an [issue](https://github.com/rois-codh/kmnist/issues/new) or [pull request](https://github.com/rois-codh/kmnist/compare)!

|Model                            | MNIST | Kuzushiji-MNIST | Kuzushiji-49 | Credit
|---------------------------------|-------|--------|-----|---|
|[4-Nearest Neighbour Baseline](benchmarks/kuzushiji_mnist_knn.py)     |97.14% | 91.56% | 86.01%|
|[Tuned SVM (RBF kernel)](https://github.com/rois-codh/kmnist/issues/3) | 98.57% | 92.82% | | [TomZephire](https://github.com/TomZephire)
|[Keras Simple CNN Benchmark](benchmarks/kuzushiji_mnist_cnn.py)       |99.06% | 95.12% |89.25%|
|PreActResNet-18                  |99.56% | 97.82% |96.64%|
|PreActResNet-18 + Input Mixup    |99.54% | 98.41% |97.04%|
|PreActResNet-18 + Manifold Mixup |99.54% | 98.83% | **97.33%** |
|[ResNet18 + VGG Ensemble](https://github.com/ranihorev/Kuzushiji_MNIST) | **99.60%** | **98.90%** | | [Rani Horev](https://twitter.com/HorevRani)

For MNIST and Kuzushiji-MNIST we use a standard accuracy metric while Kuzushiji-49 is evaluated using balanced accuracy (so that all classes have equal weight).

## License

Both the dataset itself and the contents of this repo are licensed under a permissive  [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license except where specified within some benchmark scripts. CC BY-SA 4.0 license requires attribution and we would suggest to use the following attribution to the KMNIST dataset.

""KMNIST Dataset"" (created by CODH) adapted from ""Kuzushiji Dataset"" 
(created by NIJL and others) doi:10.20676/00000341





## Related datasets

Kuzushiji Dataset http://codh.rois.ac.jp/char-shape/ offers 4645 character types and 684165 character images with CSV files containing the bounding box of characters on the original page images. At this moment the description of the dataset is available only in Japanese but the English version will be available soon. "
"Abstract:  We introduce a class of convolutional neural networks (CNNs) that utilize
recurrent neural networks (RNNs) as convolution filters. A convolution filter
is typically implemented as a linear affine transformation followed by a
non-linear function which fails to account for language compositionality. As a
result it limits the use of high-order filters that are often warranted for
natural language processing tasks. In this work we model convolution filters
with RNNs that naturally capture compositionality and long-term dependencies in
language. We show that simple CNN architectures equipped with recurrent neural
filters (RNFs) achieve results that are on par with the best published ones on
the Stanford Sentiment Treebank and two answer sentence selection datasets.
# Convolutional Neural Networks with Recurrent Neural Filters


Author: Yi Yang

Contact: yyang464@bloomberg.net


## Basic description

This is the Python implementation of the recurrent neural filters for convolutional neural networks 
described in

    Yi Yang
    ""Convolutional Neural Networks with Recurrent Neural Filters""
    EMNLP 2018

[[pdf]](https://arxiv.org/abs/1808.09315)

BibTeX

    @inproceedings{yang2018convolutional
      title={Convolutional Neural Networks with Recurrent Neural Filters}
      author={Yang Yi}
      booktitle={Proceedings of Empirical Methods in Natural Language Processing}
      year={2018}
    }

## Dependencies

1. [TensorFlow](https://www.tensorflow.org/)
2. [Keras](https://keras.io/)
3. Optional: [CUDA Toolkit](http://docs.nvidia.com/cuda/) for GPU programming.


## Data

We use the Stanford Sentiment Treebank (SST) datasets processed by Lei et al. (2015). 
Please put all the files of [this directory](https://github.com/taolei87/text_convnet/tree/master/data) into the [data/sst_text_convnet](data/sst_text_convnet) folder.

Please download the pre-trained [GloVe vectors](http://nlp.stanford.edu/data/glove.840B.300d.zip) and unzip it into the [data](data) folder.


## Results

Running the code requires two steps:

1. Prepare the data and generate the required data files
    ```
    # binary sentiment classification
    python proc_data.py data/stsa.binary.pkl

    # fine-grained sentiment classification
    python proc_data.py --train-path data/sst_text_convnet/stsa.fine.phrases.train \
                        --dev-path   data/sst_text_convnet/stsa.fine.dev \
                        --test-path  data/sst_text_convnet/stsa.fine.test \
                        data/stsa.fine.pkl
    ```

2. CNNs for sentiment classification with linear filters and recurrent neural filters (RNFs)
    ```
    # binary sentiment classification
    python cnn_keras.py --filter-type linear data/stsa.binary.pkl
    python cnn_keras.py --filter-type rnf data/stsa.binary.pkl

    # fine-grained sentiment classification
    python cnn_keras.py --filter-type linear data/stsa.fine.pkl
    python cnn_keras.py --filter-type rnf data/stsa.fine.pkl
    ```

Hyperparameter tunning may be needed to achive the best results reported in the paper. 

Unfortunately I failed to find out how to entirely eliminate randomness for training Keras-based models. 
However you should be easily able to achieve 89\%+ and 52\%+ accuracies with RNFs after a few runs.

Recurrent neural filters consistently outperform linear filters across different filter widths
by 3-4\% accuracy."
"Abstract:  Rapid progress in deep learning has spurred its application to bioinformatics
problems including protein structure prediction and design. In classic machine
learning problems like computer vision progress has been driven by
standardized data sets that facilitate fair assessment of new methods and lower
the barrier to entry for non-domain experts. While data sets of protein
sequence and structure exist they lack certain components critical for machine
learning including high-quality multiple sequence alignments and insulated
training / validation splits that account for deep but only weakly detectable
homology across protein space. We have created the ProteinNet series of data
sets to provide a standardized mechanism for training and assessing data-driven
models of protein sequence-structure relationships. ProteinNet integrates
sequence structure and evolutionary information in programmatically
accessible file formats tailored for machine learning frameworks. Multiple
sequence alignments of all structurally characterized proteins were created
using substantial high-performance computing resources. Standardized data
splits were also generated to emulate the difficulty of past CASP (Critical
Assessment of protein Structure Prediction) experiments by resetting protein
sequence and structure space to the historical states that preceded six prior
CASPs. Utilizing sensitive evolution-based distance metrics to segregate
distantly related proteins we have additionally created validation sets
distinct from the official CASP sets that faithfully mimic their difficulty.
ProteinNet thus represents a comprehensive and accessible resource for training
and assessing machine-learned models of protein structure.
# ProteinNet
ProteinNet is a standardized data set for machine learning of protein structure. It provides protein sequences structures ([secondary](https://en.wikipedia.org/wiki/Protein_secondary_structure) and [tertiary](https://en.wikipedia.org/wiki/Protein_tertiary_structure)) multiple sequence alignments ([MSAs](https://en.wikipedia.org/wiki/Multiple_sequence_alignment)) position-specific scoring matrices ([PSSMs](https://en.wikipedia.org/wiki/Position_weight_matrix)) and standardized [training / validation / test](https://en.wikipedia.org/wiki/Training_test_and_validation_sets) splits. ProteinNet builds on the biennial [CASP](http://predictioncenter.org/) assessments which carry out blind predictions of recently solved but publicly unavailable protein structures to provide test sets that push the frontiers of computational methodology. It is organized as a series of data sets spanning CASP 7 through 12 (covering a ten-year period) to provide a range of data set sizes that enable assessment of new methods in relatively data poor and data rich regimes.

**Note that this is a preliminary release.** The raw data used for construction of the data sets as well as the MSAs are not yet generally available. However the raw MSA data (4TB) for ProteinNet 12 is available upon request. Transfer requires downloading of a Globus client.

### Motivation
Protein structure prediction is one of the central problems of biochemistry. While the problem is well-studied within the biological and chemical sciences it is less well represented within the machine learning community. We suspect this is due to two reasons: 1) a high barrier to entry for non-domain experts and 2) lack of standardization in terms of training / validation / test splits that make fair and consistent comparisons across methods possible. If these two issues are addressed protein structure prediction can become a major source of innovation in ML research alongside the canonical tasks of computer vision NLP and speech recognition. Much like [ImageNet](http://www.image-net.org) helped [spur the development](https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/) of new computer vision techniques ProteinNet aims to facilitate ML research on protein structure by providing a standardized data set and standardized training / validation / test splits that any group can use with minimal effort to get started.

### Approach
Once every two years the [CASP](http://predictioncenter.org/) assessment is held. During this competition structure predictors from across the globe are presented with protein sequences whose structures have been recently solved but which have not yet been made publicly available. The predictors make blind predictions of these structures which are then assessed for their accuracy. The CASP structures thus provide a standardized benchmark for how well prediction methods perform at a _given moment in time_. The basic idea behind ProteinNet is to piggyback on CASP by using CASP structures as test sets. ProteinNet augments these test sets with training / validation sets that _reset the historical record_ to the conditions preceding each CASP experiment. In particular ProteinNet restricts the set of sequences (used for building PSSMs and MSAs) and structures to those available prior to the commencement of each CASP. This is critical as standard databases such as [BLAST](https://blast.ncbi.nlm.nih.gov/Blast.cgi) do not maintain historical versions. We use time-reset versions of the [UniParc](http://www.uniprot.org/uniparc/) dataset as well as metagenomic sequences from the [JGI](https://img.jgi.doe.gov/) to build sequence databases for deriving MSAs. ProteinNet further provides carefully split validation sets that range in difficulty from easy (&gt;90% seq. id.) useful for assessing a model's ability to predict minor changes in protein structure such as mutations to extremely difficult (&lt;10 seq. id.) useful for assessing a model's abiliy to predict entirely new protein folds as in the CASP Free Modeling (FM) category. In a sense our validation sets provide a series of transferability challenges to test how well a model can withstand distributional shifts in the data set. We have found that our most difficult validation subsets exceed the difficulty of CASP FM targets.

### Download
| CASP7 | CASP8 | CASP9 | CASP10 | CASP11 | CASP12* |
| --- | --- | --- | --- | --- | --- |
| [Text-based](https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet/human_readable/casp7.tar.gz) | [Text-based](https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet/human_readable/casp8.tar.gz) | [Text-based](https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet/human_readable/casp9.tar.gz) | [Text-based](https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet/human_readable/casp10.tar.gz) | [Text-based](https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet/human_readable/casp11.tar.gz) | [Text-based](https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet/human_readable/casp12.tar.gz) |
| [TF Records](https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet/tfrecords/casp7.tar.gz) | [TF Records](https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet/tfrecords/casp8.tar.gz) | [TF Records](https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet/tfrecords/casp9.tar.gz) | [TF Records](https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet/tfrecords/casp10.tar.gz) | [TF Records](https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet/tfrecords/casp11.tar.gz) | [TF Records](https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet/tfrecords/casp12.tar.gz) |

\* CASP12 test set is incomplete due to embargoed structures. Once the embargo is lifted we will release all structures.

### Documentation
* [ProteinNet Records](docs/proteinnet_records.md)
* [Splitting Methodology](docs/splitting_methodology.md)
* [Raw Data](docs/raw_data.md)
* [FAQ](docs/FAQ.md)

### PyTorch Parser
ProteinNet includes an official TensorFlow-based parser. [Jeppe Hallgren](https://github.com/JeppeHallgren) has kindly created a PyTorch-based parser that is available [here](https://github.com/OpenProtein/openprotein/blob/master/preprocessing.py).

### Citation
Please cite the [ProteinNet preprint](https://arxiv.org/abs/1902.00249) on arXiv.

### Acknowledgements
Construction of this data set consumed millions of compute hours and was possible thanks to the generous support of the [HMS Laboratory of Systems Pharmacology](http://hits.harvard.edu/the-program/laboratory-of-systems-pharmacology/about/) the [Harvard Program in Therapeutic Science](http://hits.harvard.edu/the-program/program-in-regulatory-science/about/) and the [Research Computing](https://rc.hms.harvard.edu) group at [Harvard Medical School](https://hms.harvard.edu). We also thank [Martin Steinegger](https://github.com/martin-steinegger) and [Milot Mirdita](https://github.com/milot-mirdita) for their extensive help with the MMseqs2 and HHblits software packages [Sergey Ovchinnikov](http://site.solab.org/) for providing metagenomic sequences [Andriy Kryshtafovych](http://predictioncenter.org/people/kryshtafovych/index.cgi) for his assistance with CASP data and [Sean Eddy](https://github.com/cryptogenomicon) for his help with the HMMer software package. This data set is hosted by the [HMS Research Information Technology Solutions](https://rits.hms.harvard.edu) group at Harvard University."
"Abstract:  Correctly evaluating defenses against adversarial examples has proven to be
extremely difficult. Despite the significant amount of recent work attempting
to design defenses that withstand adaptive attacks few have succeeded; most
papers that propose defenses are quickly shown to be incorrect.
We believe a large contributing factor is the difficulty of performing
security evaluations. In this paper we discuss the methodological foundations
review commonly accepted best practices and suggest new methods for evaluating
defenses to adversarial examples. We hope that both researchers developing
defenses as well as readers and reviewers who wish to understand the
completeness of an evaluation consider our advice in order to avoid common
pitfalls.
# On Evaluating Adversarial Robustness

This repository contains the LaTeX source for the paper [On Evaluating Adversarial Robustness](https://github.com/evaluating-adversarial-robustness/adv-eval-paper/releases/latest/). It is a paper written with the intention of helping everyone---from those designing their own neural networks to those reviewing defense papers to those just wondering what goes into a defense evaluation---learn more about methods for evaluating adversarial robustness.

## This is a Living Document

We do not intend for this to be a traditional paper where it is written once and never updated. While the fundamentals for how to evaluate adversarial robustness will not change most of the specific advice we give today on evaluating adversarial robustness may quickly become out of date. We therefore expect to update this document from time to time in order to match the currently accepted best practices in the research community.

### Abstract

Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks few have succeeded; most papers that propose defenses are quickly shown to be incorrect.

We believe a large contributing factor is the difficulty of performing security evaluations. In this paper we discuss the methodological foundations review commonly accepted best practices and suggest new methods for evaluating defenses to adversarial examples. We hope that both researchers developing defenses as well as readers and reviewers who wish to understand the completeness of an evaluation consider our advice in order to avoid common pitfalls.


## Contributing

We welcome any contributions to the paper through both issues and pull requests. Please prefer issues for topics which warrant initial discussion (such as suggesting a new item to be added to the checklist) and pull requests for changes that will require less discussion (fixing typos or writing content for a topic discussed previously in an issue).


### Contributors
- Nicholas Carlini (Google Brain)
- Anish Athalye (MIT)
- Nicolas Papernot (Google Brain)
- Wieland Brendel (University of Tubingen)
- Jonas Rauber (University of Tubingen)
- Dimitris Tsipras (MIT)
- Ian Goodfellow (Google Brain)
- Aleksander Madry (MIT)
- Alexey Kurakin (Google Brain)

NOTE: contributors are ordered according to the amount of their contribution
to the text of the paper similar to the
[Cleverhans tech report](https://github.com/tensorflow/cleverhans#authors).
List of contributors may be expanded and order
may change with the new revisions of the paper.

## Changelog

2018-02-20: Explain author order (#5)

2018-02-18: Initial Revision

## Citation

If you use this paper in academic research you may cite the following:

```
@article{carlini2019evaluating
  title={On Evaluating Adversarial Robustness}
  author={Carlini Nicholas and Athalye Anish and Papernot Nicolas and Brendel Wieland and Rauber Jonas and Tsipras Dimitris and Goodfellow Ian and Madry Aleksander and Kurakin Alexey}
  journal={arXiv preprint arXiv:1902.06705}
  year={2019}
}
```"
"Abstract:  In this paper we address the problem of artist style transfer where the
painting style of a given artist is applied on a real world photograph. We
train our neural networks in adversarial setting via recently introduced
quadratic potential divergence for stable learning process. To further improve
the quality of generated artist stylized images we also integrate some of the
recently introduced deep learning techniques in our method. To our best
knowledge this is the first attempt towards artist style transfer via quadratic
potential divergence. We provide some stylized image samples in the
supplementary material. The source code for experimentation was written in
PyTorch and is available online in my GitHub repository.
# Artist Style Transfer Via Quadratic Potential

[**Rahul Bhalley**](https://github.com/rahulbhalley) and [Jianlin Su](https://github.com/bojone)

[arXiv paper](https://arxiv.org/abs/1902.11108)

### Abstract

In this paper we address the problem of artist style transfer where the painting style of a given artist is applied on a real world photograph. We train our neural networks in adversarial setting via recently introduced quadratic potential divergence for stable learning process. To further improve the quality of generated artist stylized images we also integrate some of the recently introduced deep learning techniques in our method. To our best knowledge this is the first attempt towards artist style transfer via quadratic potential divergence. We provide some stylized image samples in the supplementary material. The source code for experimentation was written in [PyTorch](https://pytorch.org) and is available online in my [GitHub repository](https://github.com/rahulbhalley/cyclegan-qp).

If you find our work or this repository helpful please consider citing our work with the following BibTex:
```
@article{bhalley2019artist
  title={Artist Style Transfer Via Quadratic Potential}
  author={Bhalley Rahul and Su Jianlin}
  journal={arXiv preprint arXiv:1902.11108}
  year={2019}
}
```

**NOTE: Pre-trained models are available in [`Google Drive`](https://drive.google.com/drive/folders/1IJ0OswLFD_-P2wgDg6RJhf29AQxYUc0_?usp=sharing). Please download it in the root directory of this repository.**

### Prerequisites

This code was tested in following environment setting:

- Python (version &gt;= 3.6.0)
- [PyTorch](https://github.com/pytorch/pytorch) (version &gt;= 1.0.0)
- [Torchvision](https://github.com/pytorch/vision) (version = 0.2.1)

### Usage

First clone this repository:
```
git clone https://github.com/rahulbhalley/cyclegan-qp.git
```

#### Getting Datasets

Enter into the `cyclegan-qp` directory via terminal.
```
cd cyclegan-qp
```

To download the datasets (for instance `ukiyoe2photo`) run:
```
bash download_dataset.sh ukiyoe2photo
```

Now `ukiyoe2photo` dataset will be downloaded and unzipped in `cyclegan-qp/datasets/ukiyoe2photo` directory.

#### Training &amp; Inference

To train the network set `TRAIN = True` in [config.py](https://github.com/rahulbhalley/cyclegan-qp/blob/master/config.py) and for inference set it to `False`. Then one may only need to execute the following command in terminal.
```
python main.py
```

#### Configurations

Following is a list of configurable variables (in [config.py](https://github.com/rahulbhalley/cyclegan-qp/blob/master/config.py)) to perform experiments with different settings.

##### Data

- `DATASET_DIR` - name of directory containing dataset. Default: `""datasets""`.
- `DATASET_NAME` - name of dataset to use. Default: `""vangogh2photo""`.
- `LOAD_DIM` - sets the size of images to load. Default: `286`.
- `CROP_DIM` - square crops the images from center. Default: `256`.
- `CKPT_DIR` - name of directory to save checkpoints in. Default: `""checkpoints""`.
- `SAMPLE_DIR` - directory name where inferred samples will be saved. Default: `""samples""`.

##### Quadratic Potential

- `LAMBDA` - see equation (1) in [paper](https://arxiv.org/abs/1902.11108). Default: `10.0`.
- `NORM` - see equation (2) in [paper](https://arxiv.org/abs/1902.11108). Possible values: `""l1""` `""l2""`. Default: `""l1""`.

##### CycleGAN-QP

- `CYC_WEIGHT` - cycle consistency weight. Default: `10.0`.
- `ID_WEIGHT` - identity weight. Default: `0.5`.

##### Network

- `N_CHANNELS` - number of channels of images in dataset. Set to `3` for RGB and `1` for grayscale. Default: `3`.
- `UPSAMPLE` - set `True` to use ([Odena et al. 2016](https://distill.pub/2016/deconv-checkerboard/)) technique but `False` to use vanilla transpose convolution layers in generator networks. Default: `True`.

##### Training

- `RANDOM_SEED` - random seed to reproduce the experiments. Default: `12345`.
- `BATCH_SIZE` - batch size for training. Default: `4`.
- `LR` - learning rate. Default: `2e-4`.
- `BETA1` - hyper-parameter of Adam optimizer. Default: `0.5`.
- `BETA2` - hyper-parameter of Adam optimizer. Default: `0.999`.
- `BEGIN_ITER` - if `0` the train begins from start but when set to `&gt; 0` then training continues from `BEGIN_ITER`th checkpoint. Default: `0`.
- `END_ITER` - number of iteration for training. Default: `15000`.
- `TRAIN` - set `True` for training CycleGAN-QP but `False` to perform inference (for more inference configurations see next subsection). Default: `True`.

##### Inference

- `INFER_ITER` - performs inference by loading parameters from this checkpoint. Default: `15000`.
- `INFER_STYLE` - style to be transferred on images. Possible values: `""ce""` `""mo""` `""uk""` `""vg""`. Default: `""vg""`.
- `IMG_NAME` - name of image to be performed inference on. Default: `""image.jpg""`.
- `IN_IMG_DIR` - name of directory containing `IMG_NAME`. Default: `""images""`.
- `OUT_STY_DIR` - name of directory to save inferred `IMG_NAME`. Default: `""sty""`.
- `OUT_REC_DIR` - name of directory to save recovered (original) `IMG_NAME`. Default: `""rec""`.
- `IMG_SIZE` - set `None` to infer with the original sized `IMG_NAME` or set some integral value to infer with `IMG_SIZE`. Default: `None`.

##### Logs

- `ITERS_PER_LOG` - iterations duration at which screen logs should be made. Default: `100`
- `ITERS_PER_CKPT` - iterations duration at which checkpoints should be saved. Default: `1000`

### Results

The images in each column (from left to right) corresponds to:
- Original image
- Paul Cézanne
- Claude Monet
- Ukiyo-e
- Vincent Van Gogh. 

And each row contains a different image.

#### Real Image to Stylized Image
![](https://github.com/rahulbhalley/cyclegan-qp/raw/master/assets/grid_sty.jpg)

#### Stylized Image to Real Image
![](https://github.com/rahulbhalley/cyclegan-qp/raw/master/assets/grid_rec.jpg)

### Contact

For queries contact me at `rahulbhalley@protonmail.com`."
"Abstract:  Git and GitHub are common tools for keeping track of multiple versions of
data analytic content which allow for more than one person to simultaneously
work on a project. GitHub Classroom aims to provide a way for students to work
on and submit their assignments via Git and GitHub giving teachers an
opportunity to teach these version control tools as part of their course. In
the Fall 2017 semester we implemented GitHub Classroom in two educational
settings--an introductory computational statistics lab and a more advanced
computational statistics course. We found many educational benefits of
implementing GitHub Classroom such as easily providing coding feedback during
assignments and making students more confident in their ability to collaborate
and use version control tools for future data science work. To encourage and
ease the transition into using GitHub Classroom we provide free and publicly
available resources--both for students to begin using Git/GitHub and for
teachers to use GitHub Classroom for their own courses.
# Artist Style Transfer Via Quadratic Potential

[**Rahul Bhalley**](https://github.com/rahulbhalley) and [Jianlin Su](https://github.com/bojone)

[arXiv paper](https://arxiv.org/abs/1902.11108)

### Abstract

In this paper we address the problem of artist style transfer where the painting style of a given artist is applied on a real world photograph. We train our neural networks in adversarial setting via recently introduced quadratic potential divergence for stable learning process. To further improve the quality of generated artist stylized images we also integrate some of the recently introduced deep learning techniques in our method. To our best knowledge this is the first attempt towards artist style transfer via quadratic potential divergence. We provide some stylized image samples in the supplementary material. The source code for experimentation was written in [PyTorch](https://pytorch.org) and is available online in my [GitHub repository](https://github.com/rahulbhalley/cyclegan-qp).

If you find our work or this repository helpful please consider citing our work with the following BibTex:
```
@article{bhalley2019artist
  title={Artist Style Transfer Via Quadratic Potential}
  author={Bhalley Rahul and Su Jianlin}
  journal={arXiv preprint arXiv:1902.11108}
  year={2019}
}
```

**NOTE: Pre-trained models are available in [`Google Drive`](https://drive.google.com/drive/folders/1IJ0OswLFD_-P2wgDg6RJhf29AQxYUc0_?usp=sharing). Please download it in the root directory of this repository.**

### Prerequisites

This code was tested in following environment setting:

- Python (version &gt;= 3.6.0)
- [PyTorch](https://github.com/pytorch/pytorch) (version &gt;= 1.0.0)
- [Torchvision](https://github.com/pytorch/vision) (version = 0.2.1)

### Usage

First clone this repository:
```
git clone https://github.com/rahulbhalley/cyclegan-qp.git
```

#### Getting Datasets

Enter into the `cyclegan-qp` directory via terminal.
```
cd cyclegan-qp
```

To download the datasets (for instance `ukiyoe2photo`) run:
```
bash download_dataset.sh ukiyoe2photo
```

Now `ukiyoe2photo` dataset will be downloaded and unzipped in `cyclegan-qp/datasets/ukiyoe2photo` directory.

#### Training &amp; Inference

To train the network set `TRAIN = True` in [config.py](https://github.com/rahulbhalley/cyclegan-qp/blob/master/config.py) and for inference set it to `False`. Then one may only need to execute the following command in terminal.
```
python main.py
```

#### Configurations

Following is a list of configurable variables (in [config.py](https://github.com/rahulbhalley/cyclegan-qp/blob/master/config.py)) to perform experiments with different settings.

##### Data

- `DATASET_DIR` - name of directory containing dataset. Default: `""datasets""`.
- `DATASET_NAME` - name of dataset to use. Default: `""vangogh2photo""`.
- `LOAD_DIM` - sets the size of images to load. Default: `286`.
- `CROP_DIM` - square crops the images from center. Default: `256`.
- `CKPT_DIR` - name of directory to save checkpoints in. Default: `""checkpoints""`.
- `SAMPLE_DIR` - directory name where inferred samples will be saved. Default: `""samples""`.

##### Quadratic Potential

- `LAMBDA` - see equation (1) in [paper](https://arxiv.org/abs/1902.11108). Default: `10.0`.
- `NORM` - see equation (2) in [paper](https://arxiv.org/abs/1902.11108). Possible values: `""l1""` `""l2""`. Default: `""l1""`.

##### CycleGAN-QP

- `CYC_WEIGHT` - cycle consistency weight. Default: `10.0`.
- `ID_WEIGHT` - identity weight. Default: `0.5`.

##### Network

- `N_CHANNELS` - number of channels of images in dataset. Set to `3` for RGB and `1` for grayscale. Default: `3`.
- `UPSAMPLE` - set `True` to use ([Odena et al. 2016](https://distill.pub/2016/deconv-checkerboard/)) technique but `False` to use vanilla transpose convolution layers in generator networks. Default: `True`.

##### Training

- `RANDOM_SEED` - random seed to reproduce the experiments. Default: `12345`.
- `BATCH_SIZE` - batch size for training. Default: `4`.
- `LR` - learning rate. Default: `2e-4`.
- `BETA1` - hyper-parameter of Adam optimizer. Default: `0.5`.
- `BETA2` - hyper-parameter of Adam optimizer. Default: `0.999`.
- `BEGIN_ITER` - if `0` the train begins from start but when set to `&gt; 0` then training continues from `BEGIN_ITER`th checkpoint. Default: `0`.
- `END_ITER` - number of iteration for training. Default: `15000`.
- `TRAIN` - set `True` for training CycleGAN-QP but `False` to perform inference (for more inference configurations see next subsection). Default: `True`.

##### Inference

- `INFER_ITER` - performs inference by loading parameters from this checkpoint. Default: `15000`.
- `INFER_STYLE` - style to be transferred on images. Possible values: `""ce""` `""mo""` `""uk""` `""vg""`. Default: `""vg""`.
- `IMG_NAME` - name of image to be performed inference on. Default: `""image.jpg""`.
- `IN_IMG_DIR` - name of directory containing `IMG_NAME`. Default: `""images""`.
- `OUT_STY_DIR` - name of directory to save inferred `IMG_NAME`. Default: `""sty""`.
- `OUT_REC_DIR` - name of directory to save recovered (original) `IMG_NAME`. Default: `""rec""`.
- `IMG_SIZE` - set `None` to infer with the original sized `IMG_NAME` or set some integral value to infer with `IMG_SIZE`. Default: `None`.

##### Logs

- `ITERS_PER_LOG` - iterations duration at which screen logs should be made. Default: `100`
- `ITERS_PER_CKPT` - iterations duration at which checkpoints should be saved. Default: `1000`

### Results

The images in each column (from left to right) corresponds to:
- Original image
- Paul Cézanne
- Claude Monet
- Ukiyo-e
- Vincent Van Gogh. 

And each row contains a different image.

#### Real Image to Stylized Image
![](https://github.com/rahulbhalley/cyclegan-qp/raw/master/assets/grid_sty.jpg)

#### Stylized Image to Real Image
![](https://github.com/rahulbhalley/cyclegan-qp/raw/master/assets/grid_rec.jpg)

### Contact

For queries contact me at `rahulbhalley@protonmail.com`."
"Abstract:  We derive analytic closed form numerically stable solutions for the total
flux received from a spherical planet moon or star during an occultation if
the specific intensity map of the body is expressed as a sum of spherical
harmonics. Our expressions are valid to arbitrary degree and may be computed
recursively for speed. The formalism we develop here applies to the computation
of stellar transit light curves planetary secondary eclipse light curves and
planet-planet/planet-moon occultation light curves as well as thermal
(rotational) phase curves. In this paper we also introduce STARRY an
open-source package written in C++ and wrapped in Python that computes these
light curves. The algorithm in STARRY is six orders of magnitude faster than
direct numerical integration and several orders of magnitude more precise.
STARRY also computes analytic derivatives of the light curves with respect to
all input parameters for use in gradient-based optimization and inference such
as Hamiltonian Monte Carlo (HMC) allowing users to quickly and efficiently fit
observed light curves to infer properties of a celestial body's surface map.
# Artist Style Transfer Via Quadratic Potential

[**Rahul Bhalley**](https://github.com/rahulbhalley) and [Jianlin Su](https://github.com/bojone)

[arXiv paper](https://arxiv.org/abs/1902.11108)

### Abstract

In this paper we address the problem of artist style transfer where the painting style of a given artist is applied on a real world photograph. We train our neural networks in adversarial setting via recently introduced quadratic potential divergence for stable learning process. To further improve the quality of generated artist stylized images we also integrate some of the recently introduced deep learning techniques in our method. To our best knowledge this is the first attempt towards artist style transfer via quadratic potential divergence. We provide some stylized image samples in the supplementary material. The source code for experimentation was written in [PyTorch](https://pytorch.org) and is available online in my [GitHub repository](https://github.com/rahulbhalley/cyclegan-qp).

If you find our work or this repository helpful please consider citing our work with the following BibTex:
```
@article{bhalley2019artist
  title={Artist Style Transfer Via Quadratic Potential}
  author={Bhalley Rahul and Su Jianlin}
  journal={arXiv preprint arXiv:1902.11108}
  year={2019}
}
```

**NOTE: Pre-trained models are available in [`Google Drive`](https://drive.google.com/drive/folders/1IJ0OswLFD_-P2wgDg6RJhf29AQxYUc0_?usp=sharing). Please download it in the root directory of this repository.**

### Prerequisites

This code was tested in following environment setting:

- Python (version &gt;= 3.6.0)
- [PyTorch](https://github.com/pytorch/pytorch) (version &gt;= 1.0.0)
- [Torchvision](https://github.com/pytorch/vision) (version = 0.2.1)

### Usage

First clone this repository:
```
git clone https://github.com/rahulbhalley/cyclegan-qp.git
```

#### Getting Datasets

Enter into the `cyclegan-qp` directory via terminal.
```
cd cyclegan-qp
```

To download the datasets (for instance `ukiyoe2photo`) run:
```
bash download_dataset.sh ukiyoe2photo
```

Now `ukiyoe2photo` dataset will be downloaded and unzipped in `cyclegan-qp/datasets/ukiyoe2photo` directory.

#### Training &amp; Inference

To train the network set `TRAIN = True` in [config.py](https://github.com/rahulbhalley/cyclegan-qp/blob/master/config.py) and for inference set it to `False`. Then one may only need to execute the following command in terminal.
```
python main.py
```

#### Configurations

Following is a list of configurable variables (in [config.py](https://github.com/rahulbhalley/cyclegan-qp/blob/master/config.py)) to perform experiments with different settings.

##### Data

- `DATASET_DIR` - name of directory containing dataset. Default: `""datasets""`.
- `DATASET_NAME` - name of dataset to use. Default: `""vangogh2photo""`.
- `LOAD_DIM` - sets the size of images to load. Default: `286`.
- `CROP_DIM` - square crops the images from center. Default: `256`.
- `CKPT_DIR` - name of directory to save checkpoints in. Default: `""checkpoints""`.
- `SAMPLE_DIR` - directory name where inferred samples will be saved. Default: `""samples""`.

##### Quadratic Potential

- `LAMBDA` - see equation (1) in [paper](https://arxiv.org/abs/1902.11108). Default: `10.0`.
- `NORM` - see equation (2) in [paper](https://arxiv.org/abs/1902.11108). Possible values: `""l1""` `""l2""`. Default: `""l1""`.

##### CycleGAN-QP

- `CYC_WEIGHT` - cycle consistency weight. Default: `10.0`.
- `ID_WEIGHT` - identity weight. Default: `0.5`.

##### Network

- `N_CHANNELS` - number of channels of images in dataset. Set to `3` for RGB and `1` for grayscale. Default: `3`.
- `UPSAMPLE` - set `True` to use ([Odena et al. 2016](https://distill.pub/2016/deconv-checkerboard/)) technique but `False` to use vanilla transpose convolution layers in generator networks. Default: `True`.

##### Training

- `RANDOM_SEED` - random seed to reproduce the experiments. Default: `12345`.
- `BATCH_SIZE` - batch size for training. Default: `4`.
- `LR` - learning rate. Default: `2e-4`.
- `BETA1` - hyper-parameter of Adam optimizer. Default: `0.5`.
- `BETA2` - hyper-parameter of Adam optimizer. Default: `0.999`.
- `BEGIN_ITER` - if `0` the train begins from start but when set to `&gt; 0` then training continues from `BEGIN_ITER`th checkpoint. Default: `0`.
- `END_ITER` - number of iteration for training. Default: `15000`.
- `TRAIN` - set `True` for training CycleGAN-QP but `False` to perform inference (for more inference configurations see next subsection). Default: `True`.

##### Inference

- `INFER_ITER` - performs inference by loading parameters from this checkpoint. Default: `15000`.
- `INFER_STYLE` - style to be transferred on images. Possible values: `""ce""` `""mo""` `""uk""` `""vg""`. Default: `""vg""`.
- `IMG_NAME` - name of image to be performed inference on. Default: `""image.jpg""`.
- `IN_IMG_DIR` - name of directory containing `IMG_NAME`. Default: `""images""`.
- `OUT_STY_DIR` - name of directory to save inferred `IMG_NAME`. Default: `""sty""`.
- `OUT_REC_DIR` - name of directory to save recovered (original) `IMG_NAME`. Default: `""rec""`.
- `IMG_SIZE` - set `None` to infer with the original sized `IMG_NAME` or set some integral value to infer with `IMG_SIZE`. Default: `None`.

##### Logs

- `ITERS_PER_LOG` - iterations duration at which screen logs should be made. Default: `100`
- `ITERS_PER_CKPT` - iterations duration at which checkpoints should be saved. Default: `1000`

### Results

The images in each column (from left to right) corresponds to:
- Original image
- Paul Cézanne
- Claude Monet
- Ukiyo-e
- Vincent Van Gogh. 

And each row contains a different image.

#### Real Image to Stylized Image
![](https://github.com/rahulbhalley/cyclegan-qp/raw/master/assets/grid_sty.jpg)

#### Stylized Image to Real Image
![](https://github.com/rahulbhalley/cyclegan-qp/raw/master/assets/grid_rec.jpg)

### Contact

For queries contact me at `rahulbhalley@protonmail.com`."
"Abstract:  Understanding the role of the Dzyaloshinskii-Moriya interaction (DMI) for the
formation of helimagnetic order as well as the emergence of skyrmions in
magnetic systems that lack inversion symmetry has found increasing interest
due to the significant potential for novel spin based technologies. Candidate
materials to host skyrmions include those belonging to the B20 group such as
FeGe known for stabilising Bloch-like skyrmions interfacial systems such as
cobalt multilayers or Pd/Fe bilayers on top of Ir(111) known for stabilising
Néel-like skyrmions and recently alloys with a crystallographic symmetry
where anti-skyrmions are stabilised. Micromagnetic simulations have become a
standard approach to aid the design and optimisation of spintronic and magnetic
nanodevices and are also applied to the modelling of device applications which
make use of skyrmions. Several public domain micromagnetic simulation packages
such as OOMMF MuMax3 and Fidimag already offer implementations of different
DMI terms. It is therefore highly desirable to propose a so-called
micromagnetic standard problem that would allow one to benchmark and test the
different software packages in a similar way as is done for ferromagnetic
materials without DMI. Here we provide a sequence of well-defined and
increasingly complex computational problems for magnetic materials with DMI.
Our test problems include 1D 2D and 3D domains spin wave dynamics in the
presence of DMI and validation of the analytical and numerical solutions
including uniform magnetisation edge tilting spin waves and skyrmion
formation. This set of problems can be used by developers and users of new
micromagnetic simulation codes for testing and validation and hence
establishing scientific credibility.
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1174311.svg)](https://doi.org/10.5281/zenodo.1174311)
[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/fangohr/paper-supplement-standard-problem-dmi/master)
[![Build Status](https://travis-ci.org/fangohr/paper-supplement-standard-problem-dmi.svg?branch=master)](https://travis-ci.org/fangohr/paper-supplement-standard-problem-dmi)

# paper-supplement-standard-problem-dmi

Electronic and additional information for the manuscript: *Proposal for a 
micromagnetic standard problem for materials with Dzyaloshinskii-Moriya 
interaction*

Authors: [D. Cortés-Ortuño](https://github.com/davidcortesortuno)
M. Beg V. Nehruji L. Breth R. Pepper T. Kluyver G. Downing 
T. Hesjedal P. Hatton T. Lancaster R. Hertel O. Hovorka and 
H. Fangohr

This repository contains scripts and notebooks to reproduce the four standard
problems specified in the paper. The majority of the problems are specified
with an artificial ferromagnetic material:

    D     :: 3     mJ m^-2  
    A     :: 13    pJ m^-1  
    Ms    :: 0.86  MA m^-1  
    Ku    :: 0.4   MJ m^-3
    
Simulations are based on the micromagnetic packages [Fidimag](http://computationalmodelling.github.io/fidimag/) [OOMMF](https://math.nist.gov/oommf/) and [MuMax3](http://mumax.github.io/).  

## One-dimensional problem

A 100 nm  x 1 nm x 1 nm wire made of a Py-like material

![](notebooks/mayavi/one-dim.png)

## Two-dimensional problem

A 100 nm wide and 2 nm thick disk made of a Py-like material

![](notebooks/mayavi/system_2d.png)

## Three-dimensional problem

A 183 nm wide and 21 nm thick FeGe cylinder under an applied field `B`

![](notebooks/mayavi/system_3d_cylinder.png)

Material parameters are

    D     :: 1.58   mJ m^-2  
    A     :: 8.78   pJ m^-1  
    Ms    :: 0.384  MA m^-1
    B     :: 0.4    T

## Dynamics problem

Calculation of the spin wave-spectrum of a permalloy-like thin film with
Dzyaloshinkii-Moriya interactions.

![](notebooks/mayavi/sws/sws.png)

# Simulations

In the `notebooks` folder there is a notebook for every standard problem.
[Fidimag](http://computationalmodelling.github.io/fidimag/) and [OOMMF](https://math.nist.gov/oommf/) simulations can be run directly from the notebooks.  

[MuMax3](http://mumax.github.io/) simulations can be run from the `Makefile` in the [`sims/MUMAX3/`](https://github.com/fangohr/paper-supplement-standard-problem-dmi/tree/master/sims/MUMAX3) folder
which automatically converts `OVF` files into readable formats to analyse the
data in the notebooks. These simulations are in a separate folder since
MuMax3 requires a proper installation with a working NVidia graphics card.

Simulations for the calculation of spin wave spectra are computationally
intensive thus they can be run separately from the scripts and `Makefile`s in
the [`sims/spin_waves_sims/`](https://github.com/fangohr/paper-supplement-standard-problem-dmi/tree/master/sims/spin_waves_sims/) folder. Some of these scripts are called from the
corresponding notebook with the analysis of the data of the dynamics problem.
Tools to process the data from spin wave simulations can be found in the
[`sims/spin_waves_sims/data_libs/`](https://github.com/fangohr/paper-supplement-standard-problem-dmi/tree/master/sims/spin_waves_sims/data_libs) folder.

Finite element calculation results can be found in the notebooks whose names
finish with `finite-element`. These results were obtained with non-publicly
available finite element codes thus they are mostly for reference but
simulation scripts for one of the finite element packages are hosted in the
[`sims/finmag/`](https://github.com/fangohr/paper-supplement-standard-problem-dmi/tree/master/sims/finmag) directory.

# Extensions

For the OOMMF code we simulate different DMI classes using these modules which can be installed in [OOMMF](https://math.nist.gov/oommf/)'s source code separately:

1. C<sub>nv</sub> - interfacial DMI ([repository](https://github.com/joommf/oommf-extension-dmi-cnv))
2. T(O) - bulk DMI ([repository](https://github.com/joommf/oommf-extension-dmi-t)) and
3. D<sub>2d</sub> - antiskyrmion DMI ([repository](https://github.com/joommf/oommf-extension-dmi-d2d))

For the [MuMax3](http://mumax.github.io/) code we simulate D<sub>2d</sub> DMI from a new module hosted in this repository in the [`sims/MUMAX3/src_files`](https://github.com/fangohr/paper-supplement-standard-problem-dmi/tree/master/sims/MUMAX3/src_files) folder. These can be installed by copying the files in the corresponding directories of the MuMax3 source code and re-compiling it.

# Data

Data to reproduce the plots in the paper without running the notebooks are
provided as text files in the [`notebooks/data`](https://github.com/fangohr/paper-supplement-standard-problem-dmi/tree/master/notebooks/data) directory. 

# Binder

It is possible to interactively run the simulations and analyse the data from a
Jupyter notebook server provided by `Binder`. For this it is only necessary to
click on the badge at the top of this documentation. The server only allows
to run Fidimag and OOMMF simulations at the moment.

# Citation

To cite this repository refer to:

```
@Misc{Cortes2018
  author       = {David Cort{\'e}s-Ortu{\~n}o and Marijan Beg and Vanessa Nehruji and Leoni Breth and Ryan Pepper and Thomas Kluyver and Gary Downing and Thorsten Hesjedal and Peter Hatton and Tom Lancaster and Riccardo Hertel and Ondrej Hovorka and Hans Fangohr}
  title        = {Data set for: Proposal for a micromagnetic standard problem for materials with {Dzyaloshinskii-Moriya} interaction}
  howpublished = {Zenodo doi:10.5281/zenodo.1174311. Github: https://github.com/fangohr/paper-supplement-standard-problem-dmi}
  month        = apr
  year         = {2018}
  doi          = {10.5281/zenodo.1174311}
  url          = {https://doi.org/10.5281/zenodo.1174311}
}
```"
"Abstract:  Modern CPU architectures offer strong isolation guarantees towards user
applications in the form of enclaves. For instance Intel's threat model for
SGX assumes fully trusted enclaves yet there is an ongoing debate on whether
this threat model is realistic. In particular it is unclear to what extent
enclave malware could harm a system. In this work we practically demonstrate
the first enclave malware which fully and stealthily impersonates its host
application. Together with poorly-deployed application isolation on personal
computers such malware can not only steal or encrypt documents for extortion
but also act on the user's behalf e.g. sending phishing emails or mounting
denial-of-service attacks. Our SGX-ROP attack uses new TSX-based
memory-disclosure primitive and a write-anything-anywhere primitive to
construct a code-reuse attack from within an enclave which is then
inadvertently executed by the host application. With SGX-ROP we bypass ASLR
stack canaries and address sanitizer. We demonstrate that instead of
protecting users from harm SGX currently poses a security threat facilitating
so-called super-malware with ready-to-hit exploits. With our results we seek
to demystify the enclave malware threat and lay solid ground for future
research on and defense against enclave malware.
# SGX-ROP: Practical Enclave Malware with Intel SGX

This repository contains the implementations of the paper ""Practical Enclave Malware with Intel SGX"". 
The repository consists of three parts: `tap_claw` `demo` and `egghunter`. 

## TAP + CLAW

Contains the Intel TSX-based primitives to check whether a page is mapped and writable without using syscalls. 

## Demo

Uses TAP + CLAW inside a (malicious) SGX enclave to break ASLR of the host application create a ROP payload and mount a simple PoC attack (i.e. create a file in the current directory). 

## Egg Hunter

Shows how to use TAP as egg hunter for classical exploits. 


## License

All code is licensed under the MIT license."
"Abstract:  A grand challenge in reinforcement learning is intelligent exploration
especially when rewards are sparse or deceptive. Two Atari games serve as
benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall.
On both games current RL algorithms perform poorly even those with intrinsic
motivation which is the dominant method to improve performance on
hard-exploration domains. To address this shortfall we introduce a new
algorithm called Go-Explore. It exploits the following principles: (1) remember
previously visited states (2) first return to a promising state (without
exploration) then explore from it and (3) solve simulated environments
through any available means (including by introducing determinism) then
robustify via imitation learning. The combined effect of these principles is a
dramatic performance improvement on hard-exploration problems. On Montezuma's
Revenge Go-Explore scores a mean of over 43k points almost 4 times the
previous state of the art. Go-Explore can also harness human-provided domain
knowledge and when augmented with it scores a mean of over 650k points on
Montezuma's Revenge. Its max performance of nearly 18 million surpasses the
human world record meeting even the strictest definition of ""superhuman""
performance. On Pitfall Go-Explore with domain knowledge is the first
algorithm to score above zero. Its mean score of almost 60k points exceeds
expert human performance. Because Go-Explore produces high-performing
demonstrations automatically and cheaply it also outperforms imitation
learning work where humans provide solution demonstrations. Go-Explore opens up
many new research directions into improving it and weaving its insights into
current RL algorithms. It may also enable progress on previously unsolvable
hard-exploration problems in many domains especially those that harness a
simulator during training (e.g. robotics).
# Go-Explore

Paper located at: [arxiv.org/abs/1901.10995](https://arxiv.org/abs/1901.10995)

## Requirements

Tested with Python 3.6. `requirements.txt` gives the exact libraries and versions used on a test machine
able to run all phases. Unless otherwise specified libraries can be installed using `pip install <library_name>`.

**Required libraries for Phase 1:**
- matplotlib
- loky==2.3.1
- dataclasses
- tqdm
- gym
- opencv-python

These libraries are sufficient to run Go-Explore Phase 1 with custom environments which you may model after `goexplore_py/pitfall_env.py` and `goexplore_py/montezuma_env.py`.

The ALE/atari-py is not part of Go-Explore. If you are interested in running Go-Explore on Atari environments (for example to reproduce our experiments) you may install `gym[atari]` instead of just `gym`. Doing so will install atari-py. atari-py is licensed under GPLv2.

**Additional libraries for demo generation:**
- ffmpeg (non-Python library install using package manager)
- imageio
- fire

Additionally to run `gen_demo` you will need to clone [openai/atari-demo](https://github.com/openai/atari-demo) and
put a copy or link of the subfolder `atari_demo` at `gen_demo/atari_demo` in this codebase.

E.g. you could run:

`git clone https://github.com/openai/atari-demo`

`cp -r atari-demo/atari_demo gen_demo`

**Additional libraries for Phase 2:**
- [openmpi](https://www.open-mpi.org/software/ompi/v4.0/) (non-Python library install for source or using package manager)
- tensorflow-gpu
- pandas
- horovod (install using `HOROVOD_WITH_TENSORFLOW=1 pip install horovod`
- baselines (ignore mujoco-related errors)

Additionally to run Phase 2 you will need to clone [uber-research/atari-reset](https://github.com/uber-research/atari-reset) (note: this is an improved fork of the original project which you can find at [openai/atari-reset](https://github.com/openai/atari-reset)) and
put it copy it or link to it as `atari_reset` in the root folder for this project.
E.g. you could run:

`git clone https://github.com/uber-research/atari-reset atari_reset`

## Usage

Running Phase 1 of Go-Explore can be done using the `phase1.sh` script. To see the arguments
for Phase 1 run:

`./phase1.sh --help` 

The default arguments for Phase 1 will run a domain knowledge version of Go-Explore Phase 1 on
Montezuma's Revenge. However the default parameters do not correspond to any experiment actually
presented in the paper. To reproduce Phase 1 experiments from the paper run one of
`./phase1_montezuma_domain.sh` `./phase1_montezuma_no_domain.sh` or `./phase1_pitfall_domain.sh`.

Phase 1 produces a folder called `results` and subfolders for each experiment of the form
`0000_fb6be589a3dc44c1b561336e04c6b4cb` where the first element is an automatically increasing
experiment id and the second element is a random string that helps prevent race condition issues if
two experiments are started at the same time and assigned the same id.

To generate demonstrations call `./gen_demo.sh <phase1_result_folder> <destination> --game <game>`. Where `<game>` is one of ""montezuma"" (default) or ""pitfall"". The destination
will be a directory containing a `.demo` file and a `.mp4` file corresponding to the video of the
demonstration.

To robustify (run Phase 2) put a set of `.demo` files from different runs of Phase 1 into a folder
(we used 10 for Montezuma and 4 for Pitfall a single demonstration can also work but is less
likely to succeed). Then run `./phase2.sh <game> <demo_folder> <results_folder>` where the game is 
one of `MontezumaRevenge` or `Pitfall`. This should work with `mpirun` if you are using distributed 
training (we used 16 GPUs). The indicator of success for Phase 2 is when one of the 
`max_starting_point` displayed in the log has reached a value near 0 (values less than around 80 are
typically good). You may then test the performance of your trained neural network using 
`./phase2_test.sh <game> <neural_net> <test_results_folder>`
where <neural_net> is one of the files produced by Phase 2 and printed in the log as `Saving to ...`.
This will produce `.json` files for each possible number of no-ops (from 0 to 30) with scores levels
and exact action sequences produced by the test runs.
</neural_net></test_results_folder></neural_net></game></results_folder></demo_folder></game></game></game></destination></phase1_result_folder></library_name>"
"Abstract:  Stack Overflow (SO) is the most popular question-and-answer website for
software developers providing a large amount of copyable code snippets. Using
those snippets raises maintenance and legal issues. SO's license (CC BY-SA 3.0)
requires attribution i.e. referencing the original question or answer and
requires derived work to adopt a compatible license. While there is a heated
debate on SO's license model for code snippets and the required attribution
little is known about the extent to which snippets are copied from SO without
proper attribution. We present results of a large-scale empirical study
analyzing the usage and attribution of non-trivial Java code snippets from SO
answers in public GitHub (GH) projects. We followed three different approaches
to triangulate an estimate for the ratio of unattributed usages and conducted
two online surveys with software developers to complement our results. For the
different sets of projects that we analyzed the ratio of projects containing
files with a reference to SO varied between 3.3% and 11.9%. We found that at
most 1.8% of all analyzed repositories containing code from SO used the code in
a way compatible with CC BY-SA 3.0. Moreover we estimate that at most a
quarter of the copied code snippets from SO are attributed as required. Of the
surveyed developers almost one half admitted copying code from SO without
attribution and about two thirds were not aware of the license of SO code
snippets and its implications.
# Go-Explore

Paper located at: [arxiv.org/abs/1901.10995](https://arxiv.org/abs/1901.10995)

## Requirements

Tested with Python 3.6. `requirements.txt` gives the exact libraries and versions used on a test machine
able to run all phases. Unless otherwise specified libraries can be installed using `pip install <library_name>`.

**Required libraries for Phase 1:**
- matplotlib
- loky==2.3.1
- dataclasses
- tqdm
- gym
- opencv-python

These libraries are sufficient to run Go-Explore Phase 1 with custom environments which you may model after `goexplore_py/pitfall_env.py` and `goexplore_py/montezuma_env.py`.

The ALE/atari-py is not part of Go-Explore. If you are interested in running Go-Explore on Atari environments (for example to reproduce our experiments) you may install `gym[atari]` instead of just `gym`. Doing so will install atari-py. atari-py is licensed under GPLv2.

**Additional libraries for demo generation:**
- ffmpeg (non-Python library install using package manager)
- imageio
- fire

Additionally to run `gen_demo` you will need to clone [openai/atari-demo](https://github.com/openai/atari-demo) and
put a copy or link of the subfolder `atari_demo` at `gen_demo/atari_demo` in this codebase.

E.g. you could run:

`git clone https://github.com/openai/atari-demo`

`cp -r atari-demo/atari_demo gen_demo`

**Additional libraries for Phase 2:**
- [openmpi](https://www.open-mpi.org/software/ompi/v4.0/) (non-Python library install for source or using package manager)
- tensorflow-gpu
- pandas
- horovod (install using `HOROVOD_WITH_TENSORFLOW=1 pip install horovod`
- baselines (ignore mujoco-related errors)

Additionally to run Phase 2 you will need to clone [uber-research/atari-reset](https://github.com/uber-research/atari-reset) (note: this is an improved fork of the original project which you can find at [openai/atari-reset](https://github.com/openai/atari-reset)) and
put it copy it or link to it as `atari_reset` in the root folder for this project.
E.g. you could run:

`git clone https://github.com/uber-research/atari-reset atari_reset`

## Usage

Running Phase 1 of Go-Explore can be done using the `phase1.sh` script. To see the arguments
for Phase 1 run:

`./phase1.sh --help` 

The default arguments for Phase 1 will run a domain knowledge version of Go-Explore Phase 1 on
Montezuma's Revenge. However the default parameters do not correspond to any experiment actually
presented in the paper. To reproduce Phase 1 experiments from the paper run one of
`./phase1_montezuma_domain.sh` `./phase1_montezuma_no_domain.sh` or `./phase1_pitfall_domain.sh`.

Phase 1 produces a folder called `results` and subfolders for each experiment of the form
`0000_fb6be589a3dc44c1b561336e04c6b4cb` where the first element is an automatically increasing
experiment id and the second element is a random string that helps prevent race condition issues if
two experiments are started at the same time and assigned the same id.

To generate demonstrations call `./gen_demo.sh <phase1_result_folder> <destination> --game <game>`. Where `<game>` is one of ""montezuma"" (default) or ""pitfall"". The destination
will be a directory containing a `.demo` file and a `.mp4` file corresponding to the video of the
demonstration.

To robustify (run Phase 2) put a set of `.demo` files from different runs of Phase 1 into a folder
(we used 10 for Montezuma and 4 for Pitfall a single demonstration can also work but is less
likely to succeed). Then run `./phase2.sh <game> <demo_folder> <results_folder>` where the game is 
one of `MontezumaRevenge` or `Pitfall`. This should work with `mpirun` if you are using distributed 
training (we used 16 GPUs). The indicator of success for Phase 2 is when one of the 
`max_starting_point` displayed in the log has reached a value near 0 (values less than around 80 are
typically good). You may then test the performance of your trained neural network using 
`./phase2_test.sh <game> <neural_net> <test_results_folder>`
where <neural_net> is one of the files produced by Phase 2 and printed in the log as `Saving to ...`.
This will produce `.json` files for each possible number of no-ops (from 0 to 30) with scores levels
and exact action sequences produced by the test runs.
</neural_net></test_results_folder></neural_net></game></results_folder></demo_folder></game></game></game></destination></phase1_result_folder></library_name>"
"Abstract:  Deep generative models for graph-structured data offer a new angle on the
problem of chemical synthesis: by optimizing differentiable models that
directly generate molecular graphs it is possible to side-step expensive
search procedures in the discrete and vast space of chemical structures. We
introduce MolGAN an implicit likelihood-free generative model for small
molecular graphs that circumvents the need for expensive graph matching
procedures or node ordering heuristics of previous likelihood-based methods.
Our method adapts generative adversarial networks (GANs) to operate directly on
graph-structured data. We combine our approach with a reinforcement learning
objective to encourage the generation of molecules with specific desired
chemical properties. In experiments on the QM9 chemical database we
demonstrate that our model is capable of generating close to 100% valid
compounds. MolGAN compares favorably both to recent proposals that use
string-based (SMILES) representations of molecules and to a likelihood-based
method that directly generates graphs albeit being susceptible to mode
collapse.
# MolGAN
Tensorflow implementation of MolGAN: An implicit generative model for small molecular graphs (https://arxiv.org/abs/1805.11973)

## Overview
This library contains a Tensorflow implementation of MolGAN: An implicit generative model for small molecular graphs as presented in [[1]](#citation)(https://arxiv.org/abs/1805.11973).
## Dependencies

* **python&gt;=3.6**
* **tensorflow&gt;=1.7.0**: https://tensorflow.org
* **rdkit**: https://www.rdkit.org
* **numpy**

## Structure
* [data](https://github.com/nicola-decao/MolGAN/tree/master/data): should contain your datasets. If you run `download_dataset.sh` the script will download the dataset used for the paper (then you should run `utils/sparse_molecular_dataset.py` to conver the dataset in a graph format used by MolGAN models).
* [example](https://github.com/nicola-decao/MolGAN/blob/master/example.py): Example code for using the library within a Tensorflow project. **NOTE: these are NOT the experiments on the paper!**
* [models](https://github.com/nicola-decao/MolGAN/tree/master/models): Class for Models. Both VAE and (W)GAN are implemented.
* [optimizers](https://github.com/nicola-decao/MolGAN/tree/master/optimizers): Class for Optimizers for both VAE (W)GAN and RL.

## Usage
Please have a look into at the [example](https://github.com/nicola-decao/MolGAN/blob/master/example.py).

Please cite [[1](#citation)] in your work when using this library in your experiments.

## Feedback
For questions and comments feel free to contact [Nicola De Cao](mailto:nicola.decao@gmail.com).

## License
MIT

## Citation
```
[1] De Cao N. and Kipf T. (2018).MolGAN: An implicit generative 
model for small molecular graphs. ICML 2018 workshop on Theoretical
Foundations and Applications of Deep Generative Models.
```

BibTeX format:
```
@article{de2018molgan
  title={{MolGAN: An implicit generative model for small
  molecular graphs}}
  author={De Cao Nicola and Kipf Thomas}
  journal={ICML 2018 workshop on Theoretical Foundations 
  and Applications of Deep Generative Models}
  year={2018}
}

```"
"Abstract:  Suppose that a compound Poisson process is observed discretely in time and
assume that its jump distribution is supported on the set of natural numbers.
In this paper we propose a non-parametric Bayesian approach to estimate the
intensity of the underlying Poisson process and the distribution of the jumps.
We provide a MCMC scheme for obtaining samples from the posterior. We apply our
method on both simulated and real data examples and compare its performance
with the frequentist plug-in estimator proposed by Buchmann and Grübel. On a
theoretical side we study the posterior from the frequentist point of view and
prove that as the sample size $n\rightarrow\infty$ it contracts around the
`true' data-generating parameters at rate $1/\sqrt{n}$ up to a $\log n$
factor.
# MolGAN
Tensorflow implementation of MolGAN: An implicit generative model for small molecular graphs (https://arxiv.org/abs/1805.11973)

## Overview
This library contains a Tensorflow implementation of MolGAN: An implicit generative model for small molecular graphs as presented in [[1]](#citation)(https://arxiv.org/abs/1805.11973).
## Dependencies

* **python&gt;=3.6**
* **tensorflow&gt;=1.7.0**: https://tensorflow.org
* **rdkit**: https://www.rdkit.org
* **numpy**

## Structure
* [data](https://github.com/nicola-decao/MolGAN/tree/master/data): should contain your datasets. If you run `download_dataset.sh` the script will download the dataset used for the paper (then you should run `utils/sparse_molecular_dataset.py` to conver the dataset in a graph format used by MolGAN models).
* [example](https://github.com/nicola-decao/MolGAN/blob/master/example.py): Example code for using the library within a Tensorflow project. **NOTE: these are NOT the experiments on the paper!**
* [models](https://github.com/nicola-decao/MolGAN/tree/master/models): Class for Models. Both VAE and (W)GAN are implemented.
* [optimizers](https://github.com/nicola-decao/MolGAN/tree/master/optimizers): Class for Optimizers for both VAE (W)GAN and RL.

## Usage
Please have a look into at the [example](https://github.com/nicola-decao/MolGAN/blob/master/example.py).

Please cite [[1](#citation)] in your work when using this library in your experiments.

## Feedback
For questions and comments feel free to contact [Nicola De Cao](mailto:nicola.decao@gmail.com).

## License
MIT

## Citation
```
[1] De Cao N. and Kipf T. (2018).MolGAN: An implicit generative 
model for small molecular graphs. ICML 2018 workshop on Theoretical
Foundations and Applications of Deep Generative Models.
```

BibTeX format:
```
@article{de2018molgan
  title={{MolGAN: An implicit generative model for small
  molecular graphs}}
  author={De Cao Nicola and Kipf Thomas}
  journal={ICML 2018 workshop on Theoretical Foundations 
  and Applications of Deep Generative Models}
  year={2018}
}

```"
"Abstract:  Learning to rank with biased click data is a well-known challenge. A variety
of methods has been explored to debias click data for learning to rank such as
click models result interleaving and more recently the unbiased
learning-to-rank framework based on inverse propensity weighting. Despite their
differences most existing studies separate the estimation of click bias
(namely the \textit{propensity model}) from the learning of ranking algorithms.
To estimate click propensities they either conduct online result
randomization which can negatively affect the user experience or offline
parameter estimation which has special requirements for click data and is
optimized for objectives (e.g. click likelihood) that are not directly related
to the ranking performance of the system. In this work we address those
problems by unifying the learning of propensity models and ranking models. We
find that the problem of estimating a propensity model from click data is a
dual problem of unbiased learning to rank. Based on this observation we
propose a Dual Learning Algorithm (DLA) that jointly learns an unbiased ranker
and an \textit{unbiased propensity model}. DLA is an automatic unbiased
learning-to-rank framework as it directly learns unbiased ranking models from
biased click data without any preprocessing. It can adapt to the change of bias
distributions and is applicable to online learning. Our empirical experiments
with synthetic and real-world data show that the models trained with DLA
significantly outperformed the unbiased learning-to-rank algorithms based on
result randomization and the models trained with relevance signals extracted by
click models.
# Overview #
This is an implementation of the inverse propensity weighting algorithm (IPW_rank) and the Dual Learning Algorithm (DLA) for unbiased learning to rank &lt;1&gt;. Please cite the following paper if you plan to use it for your project：
    
*	Qingyao Ai Keping Bi Cheng Luo Jiafeng Guo W. Bruce Croft. 2018. Unbiased Learning to Rank with Unbiased Propensity Estimation. In Proceedings of SIGIR '18
    	
The dual learning algorithm is an online learning framework that directly learns unbiased ranking models from click data. Here we implement both the ranking model and the propensity estimator with multi-layer feed-forward neural network. Please refer to the paper for more details.

### Requirements: ###
    1. To run DLA in ./DLA/ and the python scripts in ./scripts/ python 2.7+ and Tensorflow v1.4+ are needed.

### Data Preparation for the Initial Ranker ###
	For simplicity here we show the instruction of data preparation for SVMrank (the intial ranker) on Yahoo letor (the simulation experiment in the paper) and attached the corresponding scripts in /scripts/. You can extend the scripts or write your own code to prepare the data for other letor datasets and learning algorithms.

    1. Download Yahoo Letor dataset 1 from (http://webscope.sandbox.yahoo.co).

    2. Decompressed the files and put the data into a single directory. The directory should be like the follows:
    	<letor_data_path>: # the directory of letor data
    		/set1.train.txt # the data used for training the initial ranker
    		/set1.valid.txt # the data used for validation
    		/set1.test.txt # the data used for testing

    3. Randomly sample 1% of data (lines) from set1.train.txt and replace it. We only use 10% of the training data to train the initial ranker.

    4. Train SVMrank with the data and output the model. For detailed training instructions please refer to https://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html.

    5. Run the SVMrank model on the train/valid/test data and output the corresponding scores. Then you should have a directory with the output scores like:
    	<inital_rank_path>: # the directory for SVMrank outputs
    		/train.predict # the SVMrank output for documents in the training data
    		/valid.predict # the SVMrank output for documents in the validation data
    		/test.predict # the SVMrank output for documents in the test data

    6. Generate the rank lists in the initial retrieval process using the SVMrank outputs and prepare the data for the models:
    	python ./scripts/Prepare_yahoo_letor_data_set1.py <letor_data_path> <inital_rank_path> <input_data_path> <rank_cut>
<letor_data_path>: the directory of letor data. 
    		<inital_rank_path>: the directory for SVMrank outputs. 
    		<input_data_path>: the directory for the inputs of DLA and IPW_rank. 
    		<rank_cut>: the number of top documents we keep for each query. It is 10 for the paper.
    
    After the data preparation we will have the following files in <input_data_path>:
    	<input_data_path>/settings.json:
    		The settings we used to prepare the data

    	<input_data_path>/train/:
    		1. <input_data_path>/train/train.feature:
    			The feature data.

				<doc_id> <feature_id>:<feature_val> <feature_id>:<feature_val> ... <feature_id>:<feature_val>
<doc_id> = the identifier of the document. For example ""test_2_5"" means the 5th document for the query with identifier ""2"" in the original test set of the Yahoo letor data.
					<feature_id> = an integer identifier for each feature from 0 to 699
					<feature_val> = the real feature value

				Each line represents a different document. 

    		2. <input_data_path>/train/train.init_list:
    			The initial rank lists for each query:
    			
    			<query_id> <feature_line_number_for_the_1st_doc> <feature_line_number_for_the_2nd_doc> ...  <feature_line_number_for_the_nth_doc>
<query_id> = the integer identifier for each query.
    				<feature_line_number_for_the_nth_doc> = the line number (start from 0) of the feature file (train.feature) in which the features of the Nth document for this query is stored.
    			
    			Each line represents a rank list generated by the SVMrank for the query. Documents are represented with their feature line number in the feature file and are sorted by the decending order based on the ranking scores produced by SVMrank.
    			
    		3. <input_data_path>/train/train.gold_list:
                The golden rank lists for each query:
                
                <query_id> <doc_idx_in_initial_list> <doc_idx_in_initial_list> ...  <doc_idx_in_initial_list>
<query_id> = the integer identifier for each query.
                    <doc_idx_in_initial_list> = the index (start from 0) of the document in the initial rank list (stored in train.init_list) for the query. For example <doc_idx_in_initial_list> = 1 means the 2rd document in the initial list of the query 
                
                Each line represents a golden rank list generated by reranking the initial rank list according document annotations for the query. Documents are represented with their index in the initial list of the corresponding query in train.init_list and are sorted by the decending order based on human relevance annotations.

    		4. <input_data_path>/train/train.weights:
                The annotated relevance value for documents in the initial list of each query.

                <query_id> <relevance_value_for_the_1st_doc> <relevance_value_for_the_2nd_doc> ...  <relevance_value_for_the_nth_doc>
<query_id> = the integer identifier for each query.
                    <relevance_value__for_the_nth_doc> = the human annotated relevance value of the Nth documents in the initial list of the corresponding query. For 5-level relevance judgments it should be one of the value from {01234}.
                

    		5. <input_data_path>/train/train.initial_scores:
                The ranking scores produced by SVMrank for documents in the initial list of each query.

                <query_id> <ranking_scores_for_the_1st_doc> <ranking_scores_for_the_2nd_doc> ...  <ranking_scores_for_the_nth_doc>
<query_id> = the integer identifier for each query.
                    <ranking_scores_for_the_nth_doc> = the ranking scores produced by SVMrank for the Nth documents in the initial list of the corresponding query.

    		6. <input_data_path>/train/train.qrels:
                The relevance judgement file used for evaluation.

                <query_id> 0 <doc_id> <relevance_value>
<query_id> = the integer identifier for each query.
                    <doc_id> = the identifier of the document. For example ""test_2_5"" means the 5th document for the query with identifier ""2"" in the original test set of the Yahoo letor data.
                    <relevance_value> = the human annotated relevance value for the corresponding query-document pair. For 5-level relevance judgments it should be one of the value from {01234}.

    		7. <input_data_path>/train/train.trec.gold_list:
                The golden rank lists in TREC format.

                <query_id> Q0 <doc_id> <rank> <relevance_value> Gold

                    <query_id> = the integer identifier for each query.
                    <doc_id> = the identifier of the document. For example ""test_2_5"" means the 5th document for the query with identifier ""2"" in the original test set of the Yahoo letor data.
                    <rank> = the rank (start from 1) of the document in the ranked list of the query.
                    <relevance_value> = the human annotated relevance value for the corresponding query-document pair. For 5-level relevance judgments it should be one of the value from {01234}.

    		8. <input_data_path>/train/train.trec.init_list:
                The initial rank lists in TREC format.

                <query_id> Q0 <doc_id> <rank> <ranking_scores> RankSVM

                    <query_id> = the integer identifier for each query.
                    <doc_id> = the identifier of the document. For example ""test_2_5"" means the 5th document for the query with identifier ""2"" in the original test set of the Yahoo letor data.
                    <rank> = the rank (start from 1) of the document in the ranked list of the query.
                    <ranking_scores> = the ranking scores produced by SVMrank for the corresponding query-document pair. 

            * Please notice that the query sequence in train.init_list train.gold_list train.weights and train.initial_scores must be the same.
    		
    	<input_data_path>/valid/:
            Similar to <input_data_path>/train/ except that this directory is built for the validation data.

    	<input_data_path>/test/:
            Similar to <input_data_path>/train/ except that this directory is built for the test data.

### Data Preparation for DLA/IPW_rank ###
    1. Create a click model for the generation of simulated clicks:
        python ./Unbiased_LTR/click_model.py <click_model_name> <neg_click_prob> <pos_click_prob> <rel_grad_num> <eta>
<click_model_name> = the name of the click model it could be ""position_biased_model"" or ""user_browsing_model"".
            <neg_click_prob> = the click probability (from 0 to 1) of an irrelevant result after a user has examined it.
            <pos_click_prob> = the click probability (from 0 to 1) of an relevant result after a user has examined it. It must be greater or equal to <neg_click_prob>.
            <rel_grad_num> = the highest relevance level (an interger greater than 0). For example the highest relevance level for a 5-level relevance judgment is 4. 
            <eta> = A hyper-parameter that controls the severity of presentation biases. Please check Section 5.1 in the paper for more information.

        After running the program a new json file for the created click model will be stored in the current directory. An example created click model json file is shown in ./Example_files/ClickModel/pbm_0.1_1.0_4_1.0.json (<click_model_name>=""position_biased_model"" <neg_click_prob>=0.1 <pos_click_prob>=1.0 <rel_grad_num>=4 <eta>=1.0). 
                
    2. For IPW_rank estimate the position propensity based on simluated clicks and create a propensity estimator:
        python ./Unbiased_LTR/propensity_estimator.py <click_model_json_file> <input_data_path> <estimator_json_file>
<click_model_json_file> = The path for the json file of the click model (the output of previous step).
            <input_data_path> = The path of the processed model input data.
            <estimator_json_file> = The file path of the program outputs. It is a json file that stores the propensity parameters estimated in the randomization experiment.

        After running the program a new json file for the randomized propensity estimator will be stored in <estimator_json_file>. An example propensity estimator json file is shown in ./Example_files/PropensityEstimator/randomized_pbm_0.1_1.0_4_1.0.json
    

### Training/Testing DLA ###
    1. python ./Unbiased_LTR/DLA/main.py --<parameter_name> <parameter_value> --<parameter_name> <parameter_value> … 

        1. batch_size: Batch size used in training. Default 256
        2. train_list_cutoff: The number of documents to consider in each list during training. Default 10.
        3. max_train_iteration: Limit on the iterations of training (0: no limit).
        4. steps_per_checkpoint: How many training steps to do per checkpoint. Default 200.
        5. use_non_clicked_data: Set to True for estimating propensity weights for non-click data. Default false.
        6. decode: Set to “False"" for training on the training data and “True"" for testing on test data. Default “False"".
        7. decode_train: Set to ""True"" for testing on the training data. Default ""False"".
        8. data_dir: The data directory which should be the <input_data_path>.
        9. click_model_json: The josn file for the click model used to generate clicks (e.g. ./Example_files/ClickModel/pbm_0.1_1.0_4_1.0.json)
        10. train_dir: Model directory &amp; output directory.
        11. test_dir: The directory for output test results.
        12. hparams: Hyper-parameters for models (a string written in the Tensorflow required format) which include:

            1. learning_rate:  The learning rate in training. Default 0.5.
            2. hidden_layer_sizes: The number of neurons in each layer of a feed-forward neural network. 
            3. max_gradient_norm: Clip gradients to this norm. Default 5.0
            4. loss_func: The loss function for DLA. It could be 
                ""click_weighted_softmax_cross_entropy"": The IPW based softmax loss function.
                ""click_weighted_log_loss"": The IPW based sigmoid loss function.
                ""softmax_loss"": The softmax loss without inverse propensity weighting.
            5. logits_to_prob: The function used to convert logits to probability distributions. It could be
                ""softmax"": The softmax function.
                ""sigmoid"": The sigmoid function.
            6. l2_loss: The lambda for L2 regularization. Default 0.0
            7. ranker_learning_rate: The learning rate for ranker (-1 means same with learning_rate). Default -1.
            8. ranker_loss_weigh: Set the weight of unbiased ranking loss. Default 1.0.
            9. grad_strategy: Select gradient strategy. It could be:
                ""ada"": Adagrad.
                ""sgd"": Stochastic gradient descent.
            10. relevance_category_num: Select the number of relevance category. Default 5.
            11. use_previous_rel_prob: Set to True for using ranking features in denoise model. Default false.
            12. use_previous_clicks: Set to True for using ranking features in denoise model. Default false.
            13. split_gradients_for_denoise: Set to True for splitting gradient computation in denoise model. Default true.
        
        
    2. Evaluation

        1. After training with ""--decode False” generate test rank lists with ""--decode True”.
        2. TREC format rank lists for test data will be stored in <train_dir> with name “test.ranklist”
        3. Evaluate test rank lists with ground truth <input_data_path>/test/test.qrels using trec_eval or galago eval tool.

### Training/Testing IPW_rank ###
    1. python ./Unbiased_LTR/IPW_LTR/main.py --<parameter_name> <parameter_value> --<parameter_name> <parameter_value> … 

        1. batch_size: Batch size used in training. Default 256
        2. train_list_cutoff: The number of documents to consider in each list during training. Default 10.
        3. max_train_iteration: Limit on the iterations of training (0: no limit).
        4. steps_per_checkpoint: How many training steps to do per checkpoint. Default 200.
        5. use_non_clicked_data: Set to True for estimating propensity weights for non-click data. Default false.
        6. decode: Set to “False"" for training on the training data and “True"" for testing on test data. Default “False"".
        7. decode_train: Set to ""True"" for testing on the training data. Default ""False"".
        8. data_dir: The data directory which should be the <input_data_path>.
        9. click_model_json: The josn file for the click model used to generate clicks (e.g. ./Example_files/ClickModel/pbm_0.1_1.0_4_1.0.json). 
        10. train_dir: Model directory &amp; output directory.
        11. test_dir: The directory for output test results.
        12. estimator_json: The josn file for the propensity estimator used to train unbiased models (e.g. ./Example_files/PropensityEstimator/randomized_pbm_0.1_1.0_4_1.0.json). 
        13. hparams: Hyper-parameters for models (a string written in the Tensorflow required format) which include:

            1. learning_rate:  The learning rate in training. Default 0.5.
            2. hidden_layer_sizes: The number of neurons in each layer of a feed-forward neural network. 
            3. max_gradient_norm: Clip gradients to this norm. Default 5.0
            4. loss_func: The loss function for DLA. It could be 
                ""click_weighted_softmax_cross_entropy"": The IPW based softmax loss function.
                ""click_weighted_log_loss"": The IPW based sigmoid loss function.
                ""softmax_loss"": The softmax loss without inverse propensity weighting.
            5. l2_loss: The lambda for L2 regularization. Default 0.0
        
        
    2. Evaluation

        1. After training with ""--decode False” generate test rank lists with ""--decode True”.
        2. TREC format rank lists for test data will be stored in <train_dir> with name “test.ranklist”
        3. Evaluate test rank lists with ground truth <input_data_path>/test/test.qrels using trec_eval or galago eval tool.

### Example Parameter DLA/IPW_rank Settings ###
	
    learning_rate --&gt; 0.05
    steps_per_checkpoint --&gt; 500
    max_train_iteration --&gt; 10000
    loss_func --&gt; 'click_weighted_softmax_cross_entropy'

### Reference: ###
    &lt;1&gt; Qingyao Ai Keping Bi Cheng Luo Jiafeng Guo W. Bruce Croft. 2018. Unbiased Learning to Rank with Unbiased Propensity Estimation. In Proceedings of SIGIR '18</input_data_path></train_dir></input_data_path></parameter_value></parameter_name></parameter_value></parameter_name></input_data_path></train_dir></input_data_path></parameter_value></parameter_name></parameter_value></parameter_name></estimator_json_file></estimator_json_file></input_data_path></click_model_json_file></estimator_json_file></input_data_path></click_model_json_file></eta></rel_grad_num></pos_click_prob></neg_click_prob></click_model_name></eta></rel_grad_num></neg_click_prob></pos_click_prob></neg_click_prob></click_model_name></eta></rel_grad_num></pos_click_prob></neg_click_prob></click_model_name></input_data_path></input_data_path></input_data_path></input_data_path></ranking_scores></rank></doc_id></query_id></ranking_scores></rank></doc_id></query_id></input_data_path></relevance_value></rank></doc_id></query_id></relevance_value></rank></doc_id></query_id></input_data_path></relevance_value></doc_id></query_id></relevance_value></doc_id></query_id></input_data_path></ranking_scores_for_the_nth_doc></query_id></ranking_scores_for_the_nth_doc></ranking_scores_for_the_2nd_doc></ranking_scores_for_the_1st_doc></query_id></input_data_path></relevance_value__for_the_nth_doc></query_id></relevance_value_for_the_nth_doc></relevance_value_for_the_2nd_doc></relevance_value_for_the_1st_doc></query_id></input_data_path></doc_idx_in_initial_list></doc_idx_in_initial_list></query_id></doc_idx_in_initial_list></doc_idx_in_initial_list></doc_idx_in_initial_list></query_id></input_data_path></feature_line_number_for_the_nth_doc></query_id></feature_line_number_for_the_nth_doc></feature_line_number_for_the_2nd_doc></feature_line_number_for_the_1st_doc></query_id></input_data_path></feature_val></feature_id></doc_id></feature_val></feature_id></feature_val></feature_id></feature_val></feature_id></doc_id></input_data_path></input_data_path></input_data_path></input_data_path></rank_cut></input_data_path></inital_rank_path></letor_data_path></rank_cut></input_data_path></inital_rank_path></letor_data_path></inital_rank_path></letor_data_path>"
"Abstract:  Extracting relations from text corpora is an important task in text mining.
It becomes particularly challenging when focusing on weakly-supervised relation
extraction that is utilizing a few relation instances (i.e. a pair of
entities and their relation) as seeds to extract more instances from corpora.
Existing distributional approaches leverage the corpus-level co-occurrence
statistics of entities to predict their relations and require large number of
labeled instances to learn effective relation classifiers. Alternatively
pattern-based approaches perform bootstrapping or apply neural networks to
model the local contexts but still rely on large number of labeled instances
to build reliable models. In this paper we study integrating the
distributional and pattern-based methods in a weakly-supervised setting such
that the two types of methods can provide complementary supervision for each
other to build an effective unified model. We propose a novel co-training
framework with a distributional module and a pattern module. During training
the distributional module helps the pattern module discriminate between the
informative patterns and other patterns and the pattern module generates some
highly-confident instances to improve the distributional module. The whole
framework can be effectively optimized by iterating between improving the
pattern module and updating the distributional module. We conduct experiments
on two tasks: knowledge base completion with text corpora and corpus-level
relation extraction. Experimental results prove the effectiveness of our
framework in the weakly-supervised setting.
# REPEL
This is an implementation of the REPEL model proposed in the WWW 2018 paper [""Weakly-supervised Relation Extraction by Pattern-enhanced Embedding Learning""](https://arxiv.org/abs/1711.03226).

Given a text corpus and some target relations specified by a set of seed entity pairs REPEL will automatically discover some reliable textual patterns for each relation and also more relation instances under each relation. REPEL consists of two modules i.e. a pattern module and a distributional module. The pattern module aims at finding some highly reliable patterns for each target relation and the distributional module tries to learn entity embeddings and a relation classifier for prediction. During training we alternate between optimizing the pattern module and the distributional module so that they can mutually enhance each other. Once the training process convergences both modules can be used for relation extraction which find new relation instances from different perspectives.

We provide the codes for data preprocessing in the ""preprocess"" folder the codes of the distributional module in the ""embed"" folder and the codes of the pattern module in the ""pattern"" folder. The datasets used in the paper will be uploaded soon.

## Install
Our codes rely on two external packages which are the Eigen package and the GSL package.

#### Eigen
The [Eigen](http://eigen.tuxfamily.org/index.php?title=Main_Page) package is used for matrix and vector operations. To compile our codes users need to download the package.

#### GSL
The [GSL](https://www.gnu.org/software/gsl/) package is used for random number generation. Users need to download and install the package.

## Compile
After installing the two packages users need to modify the package paths in ""embed/makefile"". Then users may go to every folder and use the makefile to compile the codes.

## Data
To run REPEL users need to provide two files. The first file is a text corpus parsed by the Stanford Core NLP Package and linked by some entity linking tools. The second file is a set of entity pairs for each target relation.

We will upload the data used in the paper soon.

## Running
To run the REPEL model users may first use the running script in the ""preprocess"" folder for data preprocessing. Then users can use the running script in the main folder to run the model.
```
./run.sh
```

## Contact: 
If you have any questions about the codes and data please feel free to contact us.
```
Meng Qu qumn123@gmail.com
```

## Citation
```
@inproceedings{qu2018weakly
title={Weakly-supervised Relation Extraction by Pattern-enhanced Embedding Learning}
author={Qu Meng and Ren Xiang and Zhang Yu and Han Jiawei}
booktitle={Proceedings of the 2018 World Wide Web Conference on World Wide Web}
pages={1257--1266}
year={2018}
organization={International World Wide Web Conferences Steering Committee}
}
```"
"Abstract:  Obtaining large-scale annotated data for NLP tasks in the scientific domain
is challenging and expensive. We release SciBERT a pretrained contextualized
embedding model based on BERT (Devlin et al. 2018) to address the lack of
high-quality large-scale labeled scientific data. SciBERT leverages
unsupervised pretraining on a large multi-domain corpus of scientific
publications to improve performance on downstream scientific NLP tasks. We
evaluate on a suite of tasks including sequence tagging sentence
classification and dependency parsing with datasets from a variety of
scientific domains. We demonstrate statistically significant improvements over
BERT and achieve new state-of-the-art results on several of these tasks.
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/named-entity-recognition-bc5cdr)](https://paperswithcode.com/sota/named-entity-recognition-bc5cdr?p=scibert-pretrained-contextualized-embeddings)  
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/relation-extraction-chemprot)](https://paperswithcode.com/sota/relation-extraction-chemprot?p=scibert-pretrained-contextualized-embeddings)  
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/participant-intervention-comparison-outcome)](https://paperswithcode.com/sota/participant-intervention-comparison-outcome?p=scibert-pretrained-contextualized-embeddings)  
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/named-entity-recognition-ncbi-disease)](https://paperswithcode.com/sota/named-entity-recognition-ncbi-disease?p=scibert-pretrained-contextualized-embeddings)  
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/sentence-classification-paper-field)](https://paperswithcode.com/sota/sentence-classification-paper-field?p=scibert-pretrained-contextualized-embeddings)  
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/citation-intent-classification-scicite)](https://paperswithcode.com/sota/citation-intent-classification-scicite?p=scibert-pretrained-contextualized-embeddings)  
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/sentence-classification-sciencecite)](https://paperswithcode.com/sota/sentence-classification-sciencecite?p=scibert-pretrained-contextualized-embeddings)  
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/relation-extraction-scierc)](https://paperswithcode.com/sota/relation-extraction-scierc?p=scibert-pretrained-contextualized-embeddings)  
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/named-entity-recognition-scierc)](https://paperswithcode.com/sota/named-entity-recognition-scierc?p=scibert-pretrained-contextualized-embeddings)  
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/citation-intent-classification-acl-arc)](https://paperswithcode.com/sota/citation-intent-classification-acl-arc?p=scibert-pretrained-contextualized-embeddings)  
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/sentence-classification-acl-arc)](https://paperswithcode.com/sota/sentence-classification-acl-arc?p=scibert-pretrained-contextualized-embeddings)  
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/dependency-parsing-genia-las)](https://paperswithcode.com/sota/dependency-parsing-genia-las?p=scibert-pretrained-contextualized-embeddings)  
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/dependency-parsing-genia-uas)](https://paperswithcode.com/sota/dependency-parsing-genia-uas?p=scibert-pretrained-contextualized-embeddings)    
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/named-entity-recognition-jnlpba)](https://paperswithcode.com/sota/named-entity-recognition-jnlpba?p=scibert-pretrained-contextualized-embeddings)   
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/sentence-classification-pubmed-20k-rct)](https://paperswithcode.com/sota/sentence-classification-pubmed-20k-rct?p=scibert-pretrained-contextualized-embeddings)  
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/sentence-classification-scicite)](https://paperswithcode.com/sota/sentence-classification-scicite?p=scibert-pretrained-contextualized-embeddings)


# <p align=""center"">`SciBERT`</p>
`SciBERT` is a `BERT` model trained on scientific text.

* `SciBERT` is trained on papers from the corpus of [semanticscholar.org](https://semanticscholar.org). Corpus size is 1.14M papers 3.1B tokens. We use the full text of the papers in training not just abstracts.

* `SciBERT` has its own vocabulary (`scivocab`) that's built to best match the training corpus. We trained cased and uncased versions. We also include models trained on the original BERT vocabulary (`basevocab`) for comparison.

* It results in state-of-the-art performance on a wide range of scientific domain nlp tasks. The details of the evaluation are in the [paper](https://arxiv.org/abs/1903.10676). Evaluation code and data are included in this repo. 

### Downloading Trained Models
We release the tensorflow and the pytorch version of the trained models. The tensorflow version is compatible with code that works with the model from [Google Research](https://github.com/google-research/bert). The pytorch version is created using the [Hugging Face](https://github.com/huggingface/pytorch-pretrained-BERT) library and this repo shows how to use it in AllenNLP.  All combinations of `scivocab` and `basevocab` `cased` and `uncased` models are available below. Our evaluation shows that `scivocab-uncased` usually gives the best results.

#### Tensorflow Models
* __[`scibert-scivocab-uncased`](https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/tensorflow_models/scibert_scivocab_uncased.tar.gz) (Recommended)__
* [`scibert-scivocab-cased`](https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/tensorflow_models/scibert_scivocab_cased.tar.gz)
* [`scibert-basevocab-uncased`](https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/tensorflow_models/scibert_basevocab_uncased.tar.gz)
* [`scibert-basevocab-cased`](https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/tensorflow_models/scibert_basevocab_cased.tar.gz)

#### PyTorch Models
* __[`scibert-scivocab-uncased`](https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_uncased.tar) (Recommended)__
* [`scibert-scivocab-cased`](https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_cased.tar)
* [`scibert-basevocab-uncased`](https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_basevocab_uncased.tar)
* [`scibert-basevocab-cased`](https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_basevocab_cased.tar)

### Using SciBERT in your own model

SciBERT models include all necessary files to be plugged in your own model and are in same format as BERT.
If you are using Tensorflow refer to Google's [BERT repo](https://github.com/google-research/bert) and if you use PyTorch refer to [Hugging Face's repo](https://github.com/huggingface/pytorch-pretrained-BERT) where detailed instructions on using BERT models are provided. 

### Training new models using AllenNLP

To run experiments on different tasks and reproduce our results in the [paper](https://arxiv.org/abs/1903.10676) you need to first setup the Python 3.6 environment:

```pip install -r requirements.txt```

which will install dependencies like [AllenNLP](https://github.com/allenai/allennlp/).

Use the `scibert/train_allennlp_local.sh` script as an example of how to run an experiment (you'll need to modify paths and variable names like `TASK` and `DATASET`).

We include a broad set of scientific nlp datasets under the `data/` directory across the following tasks. Each task has a sub-directory of available datasets.
```
├── ner
│   ├── JNLPBA
│   ├── NCBI-disease
│   ├── bc5cdr
│   └── sciie
├── parsing
│   └── genia
├── pico
│   └── ebmnlp
└── text_classification
    ├── chemprot
    ├── citation_intent
    ├── mag
    ├── rct-20k
    ├── sci-cite
    └── sciie-relation-extraction
```

For example to run the model on the Named Entity Recognition (`NER`) task and on the `BC5CDR` dataset (BioCreative V CDR) modify the `scibert/train_allennlp_local.sh` script according to:
```
DATASET='bc5cdr'
TASK='ner'
...
```

Decompress the PyTorch model that you downloaded using  
`tar -xvf scibert_scivocab_uncased.tar`  
The results will be in the `scibert_scivocab_uncased` directory containing two files:
A vocabulary file (`vocab.txt`) and a weights file (`weights.tar.gz`).
Copy the files to your desired location and then set correct paths for `BERT_WEIGHTS` and `BERT_VOCAB` in the script:
```
export BERT_VOCAB=path-to/scibert_scivocab_uncased.vocab
export BERT_WEIGHTS=path-to/scibert_scivocab_uncased.tar.gz
```

Finally run the script:

```
./scibert/train_allennlp_local.sh [serialization-directory]
```

Where `[serialization-directory]` is the path to an output directory where the model files will be stored. 

### Citing

If you use `SciBERT` in your research please cite [SciBERT: Pretrained Contextualized Embeddings for Scientific Text](https://arxiv.org/abs/1903.10676).
```
@inproceedings{Beltagy2019SciBERT
  title={SciBERT: Pretrained Contextualized Embeddings for Scientific Text}
  author={Iz Beltagy and Arman Cohan and Kyle Lo}
  year={2019}
  Eprint={arXiv:1903.10676}
}
```

`SciBERT` is an open-source project developed by [the Allen Institute for Artificial Intelligence (AI2)](http://www.allenai.org).
AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering."
"Abstract:  Imaging in low light is challenging due to low photon count and low SNR.
Short-exposure images suffer from noise while long exposure can induce blur
and is often impractical. A variety of denoising deblurring and enhancement
techniques have been proposed but their effectiveness is limited in extreme
conditions such as video-rate imaging at night. To support the development of
learning-based pipelines for low-light image processing we introduce a dataset
of raw short-exposure low-light images with corresponding long-exposure
reference images. Using the presented dataset we develop a pipeline for
processing low-light images based on end-to-end training of a
fully-convolutional network. The network operates directly on raw sensor data
and replaces much of the traditional image processing pipeline which tends to
perform poorly on such data. We report promising results on the new dataset
analyze factors that affect performance and highlight opportunities for future
work. The results are shown in the supplementary video at
this https URL
# Learning-to-See-in-the-Dark

This is a Tensorflow implementation of Learning to See in the Dark in CVPR 2018 by [Chen Chen](http://cchen156.web.engr.illinois.edu/) [Qifeng Chen](http://cqf.io/) [Jia Xu](http://pages.cs.wisc.edu/~jiaxu/) and [Vladlen Koltun](http://vladlen.info/).  

[Project Website](http://web.engr.illinois.edu/~cchen156/SID.html)<br/>
[Paper](http://cchen156.web.engr.illinois.edu/paper/18CVPR_SID.pdf)<br/>

![teaser](images/fig1.png ""Sample inpainting results on held-out images"")

This code includes the default model for training and testing on the See-in-the-Dark (SID) dataset. 


## Demo Video
https://youtu.be/qWKUFK7MWvg

## Setup

### Requirement
Required python (version 2.7) libraries: Tensorflow (&gt;=1.1) + Scipy + Numpy + Rawpy.

Tested in Ubuntu + Intel i7 CPU + Nvidia Titan X (Pascal) with Cuda (&gt;=8.0) and CuDNN (&gt;=5.0). CPU mode should also work with minor changes but not tested.

### Dataset

**Update Aug 2018:** We found some misalignment with the ground-truth for image 10034 10045 10172. Please remove those images for quantitative results but they still can be used for qualitative evaluations.

We provide the dataset by Sony and Fuji cameras. To download the data you can run
```Shell
python download_dataset.py
```
or you can download it directly from Google drive for the [Sony](https://drive.google.com/file/d/10kpAcvldtcb9G2ze5hTcF1odzu4V_Zvh/view?usp=sharing) (25 GB)  and [Fuji](https://drive.google.com/file/d/12hvKCjwuilKTZPe9EZ7ZTb-azOmUA3HT/view?usp=sharing) (52 GB) sets. 

There is download limit by Google drive in a fixed period of time. If you cannot download because of this try these links: [Sony](https://drive.google.com/open?id=1G6VruemZtpOyHjOC5N8Ww3ftVXOydSXx) (25 GB)  and [Fuji](https://drive.google.com/open?id=1C7GeZ3Y23k1B8reRL79SqnZbRBc4uizH) (52 GB).

New: we provide file parts in [Baidu Drive](https://pan.baidu.com/s/1fk8EibhBe_M1qG0ax9LQZA) now. After you download all the parts you can combine them together by running: ""cat SonyPart* &gt; Sony.zip"" and ""cat FujiPart* &gt; Fuji.zip"".


The file lists are provided. In each row there are a short-exposed image path the corresponding long-exposed image path camera ISO and F number. Note that multiple short-exposed images may correspond to the same long-exposed image. 

The file name contains the image information. For example in ""10019_00_0.033s.RAF"" the first digit ""1"" means it is from the test set (""0"" for training set and ""2"" for validation set); ""0019"" is the image ID; the following ""00"" is the number in the sequence/burst; ""0.033s"" is the exposure time 1/30 seconds.  


### Testing
1. Clone this repository.
2. Download the pretrained models by running
```Shell
python download_models.py
```
3. Run ""python test_Sony.py"". This will generate results on the Sony test set.
4. Run ""python test_Fuji.py"". This will generate results on the Fuji test set.

By default the code takes the data in the ""./dataset/Sony/"" folder and ""./dataset/Fuji/"". If you save the dataset in other folders please change the ""input_dir"" and ""gt_dir"" at the beginning of the code. 

### Training new models
1. To train the Sony model run ""python train_Sony.py"". The result and model will be save in ""result_Sony"" folder by default. 
2. To train the Fuji model run ""python train_Fuji.py"". The result and model will be save in ""result_Fuji"" folder by default. 

By default the code takes the data in the ""./dataset/Sony/"" folder and ""./dataset/Fuji/"". If you save the dataset in other folders please change the ""input_dir"" and ""gt_dir"" at the beginning of the code.

Loading the raw data and proccesing by Rawpy takes significant more time than the backpropagation. By default the code will load all the groundtruth data processed by Rawpy into memory without 8-bit or 16-bit quantization. This requires at least 64 GB RAM for training the Sony model and 128 GB RAM for the Fuji model. If you need to train it on a machine with less RAM you may need to revise the code and use the groundtruth data on the disk. We provide the 16-bit groundtruth images processed by Rawpy: [Sony](https://drive.google.com/file/d/1wfkWVkauAsGvXtDJWX0IFDuDl5ozz2PM/view?usp=sharing) (12 GB)  and [Fuji](https://drive.google.com/file/d/1nJM0xYVnzmOZNacBRKebiXA4mBmiTjte/view?usp=sharing) (22 GB). 

## Questions
If you have questions about the code and data please email to cchen156@illinois.edu.

## Citation
If you use our code and dataset for research please cite our paper:

Chen Chen Qifeng Chen Jia Xu and Vladlen Koltun ""Learning to See in the Dark"" in CVPR 2018.

### License
MIT License."
"Abstract:  The problem of keyword spotting i.e. identifying keywords in a real-time
audio stream is mainly solved by applying a neural network over successive
sliding windows. Due to the difficulty of the task baseline models are usually
large resulting in a high computational cost and energy consumption level. We
propose a new method called SANAS (Stochastic Adaptive Neural Architecture
Search) which is able to adapt the architecture of the neural network
on-the-fly at inference time such that small architectures will be used when
the stream is easy to process (silence low noise ...) and bigger networks
will be used when the task becomes more difficult. We show that this adaptive
model can be learned end-to-end by optimizing a trade-off between the
prediction performance and the average computational cost per unit of time.
Experiments on the Speech Commands dataset show that this approach leads to a
high recognition level while being much faster (and/or energy saving) than
classical approaches where the network architecture is static.
# SANAS: Stochastic Adaptive Neural Architecture Search

Implementation of **SANAS** (see [paper on arXiv ](https://arxiv.org/abs/1811.06753)) a model able to dynamically adapt the architecture of a Deep Neural Network at test time for efficient sequence classification.

# Installation:
 - Create an environment with Python 3.6
 - `pip install -r requirements.txt`
   
# Speech commands dataset:
 - Download the Speech command v0.01 [archive](https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.01.tar.gz).
 - Extract the dataset and give the extracted folder path as `root_path` argument (defaults to `./data/speech_commands_v0.01`).
 - Implementation of the Speech Commands data processing is based on [honk](https://github.com/castorini/honk) credits goes to the authors!
 - Speech Commands dataset paper on [arXiv](https://arxiv.org/abs/1804.03209).
# Exemple run :

### Without mongo connection:
 - `python main.py with adam speech_commands gru kwscnn static=True use_mongo=False ex_path=<path_to_save_location>/runs` 
 - If no `ex_path` is specified logs and models will be saved under `./runs`

### With mongo connection:
 - Create json file containing the required connection informations:

 ```json
 {
  ""user"": ""Me""
  ""passwd"": ""MySecurePassword""
  ""host"": ""localhost""
  ""port"": ""27017""
  ""db"": ""sanas""
  ""collection"": ""runs""
}
```
 - `python main.py with adam speech_commands gru kwscnn static=True use_mongo=False mongo_config_path=<path_to_config>/mongo_config.json`
 - `mongo_config_path` defaults to `./resources/mongo_credentials.json`
 
### Without Visdom :
 - `python main.py with adam speech_commands gru kwscnn static=True use_visdom=False` 

### With Visdom :
 - Visdom will connect to `localhost:8097` by default. To specify the server create a config file:
 ```json
 {
  ""server"": ""http://localhost""
  ""port"": 8097
}
```
- `python main.py with adam speech_commands gru kwscnn static=True visdom_config_path=<path_to_config>/vis_config.json` 


# Data :

### Implementing a new dataset:

The `__get_item__(self idx)` of a dataset should return a tuple `(xy)` with:
- `x` of size `seq_len x feature_dims`. For example `feature_dims` for traditional images is `(CHW)`
- `y` of size `seq_len`.

It is possible to use the [PadCollate](https://github.com/TomVeniat/AdaptiveSequenceClassification/blob/master/src/commons/pytorch/data/collate.py#L20) class in the dataloader to pad each sequence to the length of the longest one in the sampled batch.
</path_to_config></path_to_config></path_to_save_location>"
"Abstract:  Improved distance measurements to millisecond pulsars can enhance pulsar
timing array (PTA) sensitivity to gravitational waves improve tests of general
relativity with binary pulsars improve constraints on fuzzy dark matter and
more. Here we report the parallax distance measurements to six Gaia DR2 objects
associated with International PTA pulsars J0437-4715 J1012+5307 J1024-0719
J1732-5049 J1910+1256 and J1843-1113. By multiplying the posteriors of the
PTA distances with the \gaia distance to the companion we improve the distance
measurements and provide a tentative detection of a previously unknown binary
companion to J1843-1113. Finally we recommend that future Gaia data releases
use J0437-4715 as a calibration point since its distance estimate in Gaia DR2
is relatively poor compared to pulsar timing measurements.
# gaia_pulsars

Here we look at the white dwarf companion to IPTA millisecond pulsars.
Our long-term goal is to achieve pc-scale distance measurements to the pulsars in order to do gravitational-wave interferometry get better constraints on fuzzy (light axionic) dark matter and tests of GR via binary pulsars.

Here we give all the codes to reproduce the results in Mingarelli Anderson Bedell and Spergel. Please cite our paper when you use our code.

In order to do the pulsar cross-match see our other repo: gaia_xmatch."
"Abstract:  Asynchronous distributed machine learning solutions have proven very
effective so far but always assuming perfectly functioning workers. In
practice some of the workers can however exhibit Byzantine behavior caused by
hardware failures software bugs corrupt data or even malicious attacks. We
introduce \emph{Kardam} the first distributed asynchronous stochastic gradient
descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of
two complementary components: a filtering and a dampening component. The first
is scalar-based and ensures resilience against $\frac{1}{3}$ Byzantine workers.
Essentially this filter leverages the Lipschitzness of cost functions and acts
as a self-stabilizer against Byzantine workers that would attempt to corrupt
the progress of SGD. The dampening component bounds the convergence rate by
adjusting to stale information through a generic gradient weighting scheme. We
prove that Kardam guarantees almost sure convergence in the presence of
asynchrony and Byzantine behavior and we derive its convergence rate. We
evaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead
with respect to non Byzantine-resilient solutions. We empirically show that
Kardam does not introduce additional noise to the learning procedure but does
induce a slowdown (the cost of Byzantine resilience) that we both theoretically
and empirically show to be less than $f/n$ where $f$ is the number of
Byzantine failures tolerated and $n$ the total number of workers.
Interestingly we also empirically observe that the dampening component is
interesting in its own right for it enables to build an SGD algorithm that
outperforms alternative staleness-aware asynchronous competitors in
environments with honest workers.
# gaia_pulsars

Here we look at the white dwarf companion to IPTA millisecond pulsars.
Our long-term goal is to achieve pc-scale distance measurements to the pulsars in order to do gravitational-wave interferometry get better constraints on fuzzy (light axionic) dark matter and tests of GR via binary pulsars.

Here we give all the codes to reproduce the results in Mingarelli Anderson Bedell and Spergel. Please cite our paper when you use our code.

In order to do the pulsar cross-match see our other repo: gaia_xmatch."
"Abstract:  Software development increasingly depends on libraries and frameworks to
increase productivity and reduce time-to-market. Despite this fact we still
lack techniques to assess developers expertise in widely popular libraries and
frameworks. In this paper we evaluate the performance of unsupervised (based
on clustering) and supervised machine learning classifiers (Random Forest and
SVM) to identify experts in three popular JavaScript libraries: facebook/react
mongodb/node-mongodb and socketio/socket.io. First we collect 13 features
about developers activity on GitHub projects including commits on source code
files that depend on these libraries. We also build a ground truth including
the expertise of 575 developers on the studied libraries as self-reported by
them in a survey. Based on our findings we document the challenges of using
machine learning classifiers to predict expertise in software libraries using
features extracted from GitHub. Then we propose a method to identify library
experts based on clustering feature data from GitHub; by triangulating the
results of this method with information available on Linkedin profiles we show
that it is able to recommend dozens of GitHub users with evidences of being
experts in the studied JavaScript libraries. We also provide a public dataset
with the expertise of 575 developers on the studied libraries.
# gaia_pulsars

Here we look at the white dwarf companion to IPTA millisecond pulsars.
Our long-term goal is to achieve pc-scale distance measurements to the pulsars in order to do gravitational-wave interferometry get better constraints on fuzzy (light axionic) dark matter and tests of GR via binary pulsars.

Here we give all the codes to reproduce the results in Mingarelli Anderson Bedell and Spergel. Please cite our paper when you use our code.

In order to do the pulsar cross-match see our other repo: gaia_xmatch."
"Abstract:  Building on the success of Quantum Monte Carlo techniques such as diffusion
Monte Carlo alternative stochastic approaches to solve electronic structure
problems have emerged over the last decade. The full configuration interaction
quantum Monte Carlo (FCIQMC) method allows one to systematically approach the
exact solution of such problems for cases where very high accuracy is desired.
The introduction of FCIQMC has subsequently led to the development of coupled
cluster Monte Carlo (CCMC) and density matrix quantum Monte Carlo (DMQMC)
allowing stochastic sampling of the coupled cluster wave function and the exact
thermal density matrix respectively. In this article we describe the HANDE-QMC
code an open-source implementation of FCIQMC CCMC and DMQMC including
initiator and semi-stochastic adaptations. We describe our code and demonstrate
its use on three example systems; a molecule (nitric oxide) a model solid (the
uniform electron gas) and a real solid (diamond). An illustrative tutorial is
also included.
# gaia_pulsars

Here we look at the white dwarf companion to IPTA millisecond pulsars.
Our long-term goal is to achieve pc-scale distance measurements to the pulsars in order to do gravitational-wave interferometry get better constraints on fuzzy (light axionic) dark matter and tests of GR via binary pulsars.

Here we give all the codes to reproduce the results in Mingarelli Anderson Bedell and Spergel. Please cite our paper when you use our code.

In order to do the pulsar cross-match see our other repo: gaia_xmatch."
"Abstract:  There has been a long history in electrical muscle stimulation (EMS) which
has been used for medical and interaction purposes. Human-computer interaction
(HCI) researchers are now working on various applications including virtual
reality (VR) notification and learning. For the electric signals applied to
the human body various types of waveforms have been considered and tested. In
typical applications pulses with short duration are applied however many
perspectives are required to be considered. In addition to the duration and
polarity of the pulse/waves the wave shapes can also be an essential factor to
consider. A problem of conventional EMS toolkits and systems are that they have
a limitation to the variety of signals that it can produce. For example some
may be limited to monophonic pulses. Furthermore they are usually limited to
rectangular pulses and a limited range of frequencies and other waveforms
cannot be produced. These kinds of limitations make us challenging to consider
variations of EMS signals in HCI research and applications. The purpose of
""{\it wavEMS}"" is to encourage testing of a variety of waveforms for EMS which
can be manipulated through audio output. We believe that this can help improve
HCI applications and to open up new application areas.
# wavEMS
EMS controls via audio signals  
  
EMS waves generated by audio.  
  
RN-52 piezo amp DC-DC convertors powered by 3.7V Li-ion battery.  

![wavems](https://github.com/rkmtlab/wavEMS/blob/master/images/wavems.jpg)

## How to Use  
Connect to wavEMS via Bluetooth and use audio output for controls.  
""wavEMS.pde"" provides a simple UI where you can test typical EMS waveforms.  
You can use any kind of audio but always be careful that some signals can be painful/dangerous.  
Please also go through these [terms](https://github.com/rkmtlab/multi-ems/blob/multi-ems-3.1.1/TERMSOFUSE.md) before usage.  
    
  
## Components  
RN-52: Bluetooth Module  
[Breakout+Amp](https://shop.emergeplus.jp/hachiware-btamp-kit/)  
or any other breakouts (but requires to remove the input resistor from the following Amplifier)  
  
IFJM-001: Piezo Amplifier  
[IFJM-001](https://www.marutsu.co.jp/pc/i/1099677/)  
  
LTC3124: DC-DC Covertor (used to convert 3.7V input to 12V)  
[LTC3124](https://strawberry-linux.com/catalog/items?code=13124)  
  
MIWI06-24D05: DC-DC Convertor (used to convert 12V to +5V and -5V)  
[MIWI06-24D05](http://akizukidenshi.com/catalog/g/gM-06536/) 
  
  
## Implementation  
- Connect battery to LTC3124.
- Connect 12V output of LTC3124 to MIWI06-24D05 and IFJM-001.
- Connect +5V output of MIWI06-24D05 to IFJM-001 and RN-52 and -5V to IFJM-001.
- Connect RN-52 output to IJFM-001 input.
- Use 3.9kΩ resistor for the IJFM-001 bus voltage adjusting pins.
- Include a current limiting fuse etc. for safety.  
  
![system](https://github.com/rkmtlab/wavEMS/blob/master/images/system.jpg)  
  
## Issues  
- due to the filter the amplitude are not the same for all waveforms  
- consumes high current. maybe changing the resistor (bus voltage) can slightly solve this.  
  
## Publications  
See the following for more details:  
Michinari Kono and Jun Rekimoto. 2019. wavEMS: Improving Signal Variation Freedom of Electrical Muscle Stimulation. 2019 IEEE Conference on Virtual Reality and 3D User Interfaces Workshop on Human Augmentation and its Applications (IEEE VR ’19). 4 pages. [https://arxiv.org/abs/1902.03184](https://arxiv.org/abs/1902.03184)  
  
Also see...  
Michinari Kono Yoshio Ishiguro Takashi Miyaki and Jun Rekimoto. 2018. Design and Study of a Multi-Channel Electrical Muscle Stimulation Toolkit for Human Augmentation. In Proceedings of the 9th Augmented Human International Conference (AH '18). ACM New York NY USA Article 11 8 pages. DOI: https://doi.org/10.1145/3174910.3174913  
and [multi-ems](https://github.com/rkmtlab/multi-ems)

The following article includes a survey of EMS in HCI and safety topics.

Michinari Kono Takumi Takahashi Hiromi Nakamura Takashi Miyaki and Jun Rekimoto. 2018. Design Guideline for Developing Safe Systems that Apply Electricity to the Human Body. ACM Trans. Comput.-Hum. Interact. 25 3 Article 19 (June 2018) 36 pages. DOI: https://doi.org/10.1145/3184743  
  
## Authors

Michinari Kono U-Tokyo ( mchkono[at]acm.org )

## Copyrights License      
  
Copyright (c) 2019 Michinari Kono  
Released under the MIT license "
"Abstract:  Interactive Fiction (IF) games are complex textual decision making problems.
This paper introduces NAIL an autonomous agent for general parser-based IF
games. NAIL won the 2018 Text Adventure AI Competition where it was evaluated
on twenty unseen games. This paper describes the architecture development and
insights underpinning NAIL's performance.
# NAIL agent
Navigate Acquire Interact Learn

NAIL is a general game-playing agent designed for parser-based interactive fiction games 
([Hausknecht et al. 2019](https://arxiv.org/abs/1902.04259)).
NAIL employs a simple heuristic: examine the current location to identify relevant objects
interact with the identified objects navigate to a new location and repeat.
Though simple this loop proves effective across a wide variety of games.

NAIL won first place in the 2018 Text-Based Adventure AI Competition
([Atkinson et al. 2018](https://arxiv.org/abs/1808.01262))
where it was evaluated on a set of twenty unknown parser-based IF games.

## Requirements
* Linux
* Python 3

## Installation
* Install basic build tools.
    * sudo apt-get update
    * sudo apt-get install build-essential
    * sudo apt-get install python3-dev

* Install [fastText](https://github.com/facebookresearch/fastText#building-fasttext-for-python)
    * pip3 install pybind11
    * git clone https://github.com/facebookresearch/fastText.git
    * cd fastText
    * pip3 install .
    * cd ..

* Install [Jericho](https://github.com/Microsoft/jericho)
    * pip3 install jericho
* Clone this nail_agent repository to your Linux machine.
* Download the NAIL agent's language model to the nail_agent/agent/affordance_extractors/language_model directory:
    * wget http://download.microsoft.com/download/B/8/8/B88DDDC1-F316-412A-94B3-025788436054/nail_agent_lm.zip
* unzip nail_agent_lm.zip
    * The unzipped directory should contain 1028 files.

* pip3 install numpy
* pip3 install fuzzywuzzy
* pip3 install spacy
* python3 -m spacy download en
* pip3 install python-Levenshtein

## Usage
* Obtain a z-machine game (like zork1.z5)
* cd nail_agent
* python3 run_nail_agent.py <path_to_game>

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to and actually do grant us
the rights to use your contribution. For details visit https://cla.microsoft.com.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
</path_to_game>"
"Abstract:  The governance of data used for biomedical research and clinical trials is an
important requirement for generating accurate results. To improve the
visibility of data quality and analysis we developed TrialChain a
blockchain-based platform that can be used to validate data integrity from
large biomedical research studies. We implemented a private blockchain using
the MultiChain platform and integrated it with a data science platform deployed
within a large research center. An administrative web application was built
with Python to manage the platform which was built with a microservice
architecture using Docker. The TrialChain platform was integrated during data
acquisition into our existing data science platform. Using NiFi data were
hashed and logged within the local blockchain infrastructure. To provide public
validation the local blockchain state was periodically synchronized to the
public Ethereum network. The use of a combined private/public blockchain
platform allows for both public validation of results while maintaining
additional security and lower cost for blockchain transactions. Original data
and modifications due to downstream analysis can be logged within TrialChain
and data assets or results can be rapidly validated when needed using API calls
to the platform. The TrialChain platform provides a data governance solution to
audit the acquisition and analysis of biomedical research data. The platform
provides cryptographic assurance of data authenticity and can also be used to
document data analysis.
# Scheduled Policy Optimization for Natural Language Communication with Intelligent Agents

## Models and Algorithms
See files under `walk_the_blocks/BlockWorldRoboticAgent/srcs/`

* `learn_by_ppo.py` 
   run this file for training you can change the schedule mechanism in the function `ppo_update()` these are the options:
   * do imitation every 50
   * do imitation based on rules
   * imitation 1 epoch and then RL 1 epoch

   example:
   `python learn_by_ppo.py -lr 0.0001 -max_epochs 2 -entropy_coef 0.05`
* `policy_model.py`
   the network achitecture and loss functions:
   * PPO Loss
   * Supervised Loss
   * Advantage Actor-Critic Loss

## Instructions
For the usage of the Block-world environment please refer to [https://github.com/clic-lab/blocks](https://github.com/clic-lab/blocks)

### Train the RL agents
* S-REIN
   * 
   
## If you use our code in your own research please cite the following paper
```
@article{xiong2018scheduled
  title={Scheduled Policy Optimization for Natural Language Communication with Intelligent Agents}
  author={Xiong Wenhan and Guo Xiaoxiao and Yu Mo and Chang Shiyu and Zhou Bowen and Wang William Yang}
  journal={arXiv preprint arXiv:1806.06187}
  year={2018}
}
```"
"Abstract:  Virtualization technologies have evolved along with the development of
computational environments since virtualization offered needed features at that
time such as isolation accountability resource allocation resource fair
sharing and so on. Novel processor technologies bring to commodity computers
the possibility to emulate diverse environments where a wide range of
computational scenarios can be run. Along with processors evolution system
developers have created different virtualization mechanisms where each new
development enhanced the performance of previous virtualized environments.
Recently operating system-based virtualization technologies captured the
attention of communities abroad (from industry to academy and research) because
their important improvements on performance area.
In this paper the features of three container-based operating systems
virtualization tools (LXC Docker and Singularity) are presented. LXC Docker
Singularity and bare metal are put under test through a customized single node
HPL-Benchmark and a MPI-based application for the multi node testbed. Also the
disk I/O performance Memory (RAM) performance Network bandwidth and GPU
performance are tested for the COS technologies vs bare metal. Preliminary
results and conclusions around them are presented and discussed.
# Scheduled Policy Optimization for Natural Language Communication with Intelligent Agents

## Models and Algorithms
See files under `walk_the_blocks/BlockWorldRoboticAgent/srcs/`

* `learn_by_ppo.py` 
   run this file for training you can change the schedule mechanism in the function `ppo_update()` these are the options:
   * do imitation every 50
   * do imitation based on rules
   * imitation 1 epoch and then RL 1 epoch

   example:
   `python learn_by_ppo.py -lr 0.0001 -max_epochs 2 -entropy_coef 0.05`
* `policy_model.py`
   the network achitecture and loss functions:
   * PPO Loss
   * Supervised Loss
   * Advantage Actor-Critic Loss

## Instructions
For the usage of the Block-world environment please refer to [https://github.com/clic-lab/blocks](https://github.com/clic-lab/blocks)

### Train the RL agents
* S-REIN
   * 
   
## If you use our code in your own research please cite the following paper
```
@article{xiong2018scheduled
  title={Scheduled Policy Optimization for Natural Language Communication with Intelligent Agents}
  author={Xiong Wenhan and Guo Xiaoxiao and Yu Mo and Chang Shiyu and Zhou Bowen and Wang William Yang}
  journal={arXiv preprint arXiv:1806.06187}
  year={2018}
}
```"
"Abstract:  Novel industrial wireless applications require wideband real-time channel
characterization due to complex multipath propagation. Rapid machine motion
leads to fast time variance of the channel's reflective behavior which must be
captured for radio channel characterization. Additionally inhomogeneous radio
channels demand highly flexible measurements. Existing approaches for radio
channel measurements either lack flexibility or wide-band real-time
performance with fast time variance. In this paper we propose a correlative
channel sounding approach utilizing a software-defined architecture. The
approach enables real-time wide-band measurements with fast time variance
immune to active interference. The desired performance is validated with a
demanding industrial application example.

# gr-corrsounder

![gr-corrsounder signal flow](gr-corrsounder-signalflow.png)

![gr-corrsounder architecture](gr-corrsounder-architecture.png)

# Requirements

 * GNU Radio
 * python package psutil
 * python package scipy

# Build/Install instructions

1. Install/Build GNU Radio (it is recommended to use [PyBOMBS](https://github.com/gnuradio/pybombs))

2. Get *gr-corrsounder* from github - `git clone https://github.com/inIT-HF/gr-corrsounder.git`

3. Optional: Change to which prefix *gr-corrsounder* shall be installed - `source ~/corrsounder_prefix/setup_env.sh`

4. Configure *gr-corrsounder* - `mkdir build &amp;&amp; cd build &amp;&amp; cmake ../`

5. Build and install *gr-corrsounder* - `make &amp;&amp; sudo make install` 

# Uninstall/Remove instructions

1. Navigate to gr-corrsounder/build

2. Optional: Change from which prefix *gr-corrsounder* shall be uninstalled - `source ~/corrsounder_prefix/setup_env.sh`

3. Uninstall *gr-corrsounder* - `sudo make uninstall`

4. Delete the gr-corrsounder folder

# Contributors

 * Niels Fliedner
 * Dimitri Block

# References
1. Niels Hendrik Fliedner Dimitri Block Uwe Meier “A Software-Defined Channel Sounder for Industrial Environments with Fast Time Variance”. Submitted to the 15th International Symposium on Wireless Communication Systems (ISWCS 2018). [Arxiv preprint](https://arxiv.org/abs/1805.01236)"
"Abstract:  Geostatistical modeling of petrophysical properties is a key step in modern
integrated oil and gas reservoir studies. Recently generative adversarial
networks (GAN) have been shown to be a successful method for generating
unconditional simulations of pore- and reservoir-scale models. This
contribution leverages the differentiable nature of neural networks to extend
GANs to the conditional simulation of three-dimensional pore- and
reservoir-scale models. Based on the previous work of Yeh et al. (2016) we use
a content loss to constrain to the conditioning data and a perceptual loss
obtained from the evaluation of the GAN discriminator network. The technique is
tested on the generation of three-dimensional micro-CT images of a Ketton
limestone constrained by two-dimensional cross-sections and on the simulation
of the Maules Creek alluvial aquifer constrained by one-dimensional sections.
Our results show that GANs represent a powerful method for sampling conditioned
pore and reservoir samples for stochastic reservoir evaluation workflows.
## GeoGAN: Conditioning of three-dimensional generative adversarial networks for pore and reservoir-scale models
*Authors*: [Lukas Mosser](mailto:lukas.mosser15@imperial.ac.uk)
[Olivier Dubrule](https://www.imperial.ac.uk/people/o.dubrule)
[Martin J. Blunt](https://www.imperial.ac.uk/people/m.blunt)  
*Department of Earth Science and Engineering Imperial College London*  

This is the code repository accompanying the publication:  
 *Conditioning of three-dimensional generative adversarial networks for pore and reservoir-scale models*
 [[ArXiv](http://arxiv.org/abs/1802.05622)]

## Datasets and pre-trained models

### Ketton Limestone Dataset
We provide two pre-trained GAN models. The first one is trained on the Ketton limestone training image presented here.  
If you decide to use this dataset for your own work please consider citing the following works:

*Stochastic reconstruction of an oolitic limestone by generative adversarial networks*[[ArXiv](https://arxiv.org/abs/1712.02854)]  
*Dynamic reservoir-condition microtomography of reactive transport in complex carbonates*[[Article](https://www.sciencedirect.com/science/article/pii/S0016703717300789)]

Due to their size we provide the necessary files via a [Google Drive](https://drive.google.com/open?id=1qxicm3wzpvijUEpyI3pTm2QPF520SZAw)  

#### Results

![Ketton Conditioned](figures/figure_1_ketton.png)

The figure above shows two samples (b/c) obtained by a conditioning a generative adversarial network to three-orthogonal cross-sections of the Ketton training image(a).  
Due to the stochastic nature of the optimization procedure the resulting images have distinctly different features away from the conditioning data.  
### Maules Creek Dataset

We have trained a generative adversarial network on the Maules Creek alluvial aquifer training image.  

The required model checkpoints are included in this repository.  
If you choose to use the Maules Creek training image please consider citing their originators at [trainingimages.org](www.trainingimages.org)  

#### Results

![Maules Creek](figures/fig_2.png)

We have conditioned 1024 realizations of the Maules Creek alluvial aquifer model and present mean and standard deviation maps of the resulting ensemble.  
The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed.  

## Requirements

The conditioning tool is based on the following libraries:
- [Python 2.7](https://anaconda.org/)
- [Pytorch 0.3](www.pytorch.org)
- [Scikit-Learn](www.scikit-learn.org)
- [tqdm](https://github.com/noamraph/tqdm)
- [numpy](www.numpy.org)
- [matplotlib](www.matplotlib.org)

We recommend using the [anaconda](https://anaconda.org/) distribution to install the required dependencies.

## Development

Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks.  
We welcome contributions and suggestions for the improvement and development of the tool.  

## Required Hardware

We recommend at least 16 GB of RAM and a modern CUDA capable NVIDIA graphics processor.  
All experiments apart from network training have been performed on an NVIDIA 960M.  
Training of the generative networks was performed on 8xNVIDIA K40 GPUs.  

## Support

The software is provided as is.  
If you have any questions please feel free to contact us via [[email](lukas.mosser15@imperial.ac.uk)] or [[twitter](https://twitter.com/porestar)]."
"Abstract:  We explore a recently proposed Variational Dropout technique that provided an
elegant Bayesian interpretation to Gaussian Dropout. We extend Variational
Dropout to the case when dropout rates are unbounded propose a way to reduce
the variance of the gradient estimator and report first experimental results
with individual dropout rates per weight. Interestingly it leads to extremely
sparse solutions both in fully-connected and convolutional layers. This effect
is similar to automatic relevance determination effect in empirical Bayes but
has a number of advantages. We reduce the number of parameters up to 280 times
on LeNet architectures and up to 68 times on VGG-like networks with a
negligible decrease of accuracy.
# Variational Dropout Sparsifies Deep Neural Networks

This repo contains the code for our ICML17 paper [Variational Dropout Sparsifies Deep Neural Networks](https://arxiv.org/abs/1701.05369) ([talk](https://vimeo.com/238221185) [slides](https://docs.google.com/presentation/d/1Lg86MnGbksn3AtehADxSG-FcrbT2DMmGmjHUqG-EQYw/edit?usp=sharing) [poster](http://ars-ashuha.ru/pdf/vdsdnn/svdo-poster.pdf) [blog-post](https://research.yandex.com/news/yandex-at-icml-2017-variational-dropout-sparsifies-deep-neural-networks)). 
We showed that Variational Dropout leads to extremely sparse solutions both in fully-connected and convolutional layers. 
Sparse VD reduced the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy. 
This effect is similar to the Automatic Relevance Determination effect in empirical Bayes.
However in Sparse VD the prior distribution remaines fixed so there is no additional risk of overfitting.

We visualize the weights of Sparse VD LeNet-5-Caffe network and demonstrate several filters of the first convolutional layer and a piece of the fully-connected layer :)

<p align=""center"">
<img height=""318"" src=""http://ars-ashuha.ru/pdf/vdsdnn/conv.gif""/>
<img height=""320"" src=""http://ars-ashuha.ru/pdf/vdsdnn/fc.gif""/>
</p>

## ICML 2017 Oral Presentation by Dmitry Molchanov

[![ICML 2017 Oral Presentation by Dmitry Molchanov](http://ars-ashuha.ru/images/icml2017-oral.png)](https://vimeo.com/238221185)

## MNIST Experiments 

The table containes the comparison of different sparsity-inducing techniques (Pruning (Han et al. 2015b;a) DNS (Guo et al. 2016) SWS (Ullrich et al. 2017)) on LeNet architectures.
Our method provides the highest level of sparsity with a similar accuracy

| Network       | Method   | Error | Sparsity per Layer  |  Compression |
| -------------: | -------- | ----- | ------------------- | :--------------: |
|               | Original | 1.64  |                     | 1              |
|               | Pruning  | 1.59  | 92.0 − 91.0 − 74.0  | 12             |
| LeNet-300-100 | DNS      | 1.99  | 98.2 − 98.2 − 94.5  | 56             |
|               | SWS      | 1.94  |                     | 23             |
| (ours)        | SparseVD | 1.92  | 98.9 − 97.2 − 62.0  | **68**         |
||||||
|               | Original | 0.8   |                     | 1              |
|               | Pruning  | 0.77  | 34 − 88 − 92.0 − 81 | 12             |
| LeNet-5       | DNS      | 0.91  | 86 − 97 − 99.3 − 96 | 111            |
|               | SWH      | 0.97  |                     | 200            |
| (ours)        | SparseVD | 0.75  | 67 − 98 − 99.8 − 95 | **280**        |


## CIFAR Experiments

The plot contains the accuracy and sparsity level for VGG-like architectures of different sizes.
The number of neurons and filters scales as _k_.
Dense networks were trained with Binary Dropout and Sparse VD networks were trained with Sparse Variational Dropout on all layers.
The overall sparsity level achieved by our method is reported as a dashed line.
The accuracy drop is negligible in most cases and the sparsity level is high especially in larger networks.

<p align=""center"">
<img height=""318"" src=""http://ars-ashuha.ru/pdf/vdsdnn/vgg.png""/>
</p>

# Environment setup

```(bash)
sudo apt install virtualenv python-pip python-dev
virtualenv venv --system-site-packages
source venv/bin/activate

pip install numpy tabulate 'ipython[all]' sklearn matplotlib seaborn  
pip install --upgrade https://github.com/Theano/Theano/archive/rel-0.9.0.zip
pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip
```

# Launch experiments 

```(bash)
source ~/venv/bin/activate
cd variational-dropout-sparsifies-dnn
THEANO_FLAGS='floatX=float32device=gpu0lib.cnmem=1' ipython ./experiments/<experiment>.py
```

PS: If you have CuDNN problem please look at this [issue](https://github.com/ars-ashuha/variational-dropout-sparsifies-dnn/issues/3).

# Further extensions

These two papers heavily rely on the Sparse Variational Dropout technique and extend it to other applications:
* [Structured Bayesian Pruning via Log-Normal Multiplicative Noise](https://arxiv.org/abs/1705.07283) provides a way to enforse _structured_ sparsity using a similar technique. This method allows to remove entire neurons and convolutional filters which results in lighter architectures and a significant inference speed-up with standard deep learning frameworks.
* [Bayesian Compression for Natural Language Processing](https://arxiv.org/abs/1810.10927) adapts the Sparse Variational Dropout techniques for sparsification of various recurrent architectures. 

**New:** Google AI Research recently has released [State of Sparsity in Deep Neural Networks](https://arxiv.org/abs/1902.09574) - a nice large scale study of sparsification methods. The code contains an implementation of [Sparse variational dropout on Tensorflow](https://github.com/google-research/google-research/blob/master/state_of_sparsity/layers/variational_dropout/nn.py#L585).

# Citation

If you found this code useful please cite our paper 

```
@article{molchanov2017variational
  title={Variational Dropout Sparsifies Deep Neural Networks}
  author={Molchanov Dmitry and Ashukha Arsenii and Vetrov Dmitry}
  journal={arXiv preprint arXiv:1701.05369}
  year={2017}
}
```
</experiment>"
"Abstract:  We present a physically-inspired model and an efficient algorithm to infer
hierarchical rankings of nodes in directed networks. It assigns real-valued
ranks to nodes rather than simply ordinal ranks and it formalizes the
assumption that interactions are more likely to occur between individuals with
similar ranks. It provides a natural statistical significance test for the
inferred hierarchy and it can be used to perform inference tasks such as
predicting the existence or direction of edges. The ranking is obtained by
solving a linear system of equations which is sparse if the network is; thus
the resulting algorithm is extremely efficient and scalable. We illustrate
these findings by analyzing real and synthetic data including datasets from
animal behavior faculty hiring social support networks and sports
tournaments. We show that our method often outperforms a variety of others in
both speed and accuracy in recovering the underlying ranks and predicting edge
directions.
# Variational Dropout Sparsifies Deep Neural Networks

This repo contains the code for our ICML17 paper [Variational Dropout Sparsifies Deep Neural Networks](https://arxiv.org/abs/1701.05369) ([talk](https://vimeo.com/238221185) [slides](https://docs.google.com/presentation/d/1Lg86MnGbksn3AtehADxSG-FcrbT2DMmGmjHUqG-EQYw/edit?usp=sharing) [poster](http://ars-ashuha.ru/pdf/vdsdnn/svdo-poster.pdf) [blog-post](https://research.yandex.com/news/yandex-at-icml-2017-variational-dropout-sparsifies-deep-neural-networks)). 
We showed that Variational Dropout leads to extremely sparse solutions both in fully-connected and convolutional layers. 
Sparse VD reduced the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy. 
This effect is similar to the Automatic Relevance Determination effect in empirical Bayes.
However in Sparse VD the prior distribution remaines fixed so there is no additional risk of overfitting.

We visualize the weights of Sparse VD LeNet-5-Caffe network and demonstrate several filters of the first convolutional layer and a piece of the fully-connected layer :)

<p align=""center"">
<img height=""318"" src=""http://ars-ashuha.ru/pdf/vdsdnn/conv.gif""/>
<img height=""320"" src=""http://ars-ashuha.ru/pdf/vdsdnn/fc.gif""/>
</p>

## ICML 2017 Oral Presentation by Dmitry Molchanov

[![ICML 2017 Oral Presentation by Dmitry Molchanov](http://ars-ashuha.ru/images/icml2017-oral.png)](https://vimeo.com/238221185)

## MNIST Experiments 

The table containes the comparison of different sparsity-inducing techniques (Pruning (Han et al. 2015b;a) DNS (Guo et al. 2016) SWS (Ullrich et al. 2017)) on LeNet architectures.
Our method provides the highest level of sparsity with a similar accuracy

| Network       | Method   | Error | Sparsity per Layer  |  Compression |
| -------------: | -------- | ----- | ------------------- | :--------------: |
|               | Original | 1.64  |                     | 1              |
|               | Pruning  | 1.59  | 92.0 − 91.0 − 74.0  | 12             |
| LeNet-300-100 | DNS      | 1.99  | 98.2 − 98.2 − 94.5  | 56             |
|               | SWS      | 1.94  |                     | 23             |
| (ours)        | SparseVD | 1.92  | 98.9 − 97.2 − 62.0  | **68**         |
||||||
|               | Original | 0.8   |                     | 1              |
|               | Pruning  | 0.77  | 34 − 88 − 92.0 − 81 | 12             |
| LeNet-5       | DNS      | 0.91  | 86 − 97 − 99.3 − 96 | 111            |
|               | SWH      | 0.97  |                     | 200            |
| (ours)        | SparseVD | 0.75  | 67 − 98 − 99.8 − 95 | **280**        |


## CIFAR Experiments

The plot contains the accuracy and sparsity level for VGG-like architectures of different sizes.
The number of neurons and filters scales as _k_.
Dense networks were trained with Binary Dropout and Sparse VD networks were trained with Sparse Variational Dropout on all layers.
The overall sparsity level achieved by our method is reported as a dashed line.
The accuracy drop is negligible in most cases and the sparsity level is high especially in larger networks.

<p align=""center"">
<img height=""318"" src=""http://ars-ashuha.ru/pdf/vdsdnn/vgg.png""/>
</p>

# Environment setup

```(bash)
sudo apt install virtualenv python-pip python-dev
virtualenv venv --system-site-packages
source venv/bin/activate

pip install numpy tabulate 'ipython[all]' sklearn matplotlib seaborn  
pip install --upgrade https://github.com/Theano/Theano/archive/rel-0.9.0.zip
pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip
```

# Launch experiments 

```(bash)
source ~/venv/bin/activate
cd variational-dropout-sparsifies-dnn
THEANO_FLAGS='floatX=float32device=gpu0lib.cnmem=1' ipython ./experiments/<experiment>.py
```

PS: If you have CuDNN problem please look at this [issue](https://github.com/ars-ashuha/variational-dropout-sparsifies-dnn/issues/3).

# Further extensions

These two papers heavily rely on the Sparse Variational Dropout technique and extend it to other applications:
* [Structured Bayesian Pruning via Log-Normal Multiplicative Noise](https://arxiv.org/abs/1705.07283) provides a way to enforse _structured_ sparsity using a similar technique. This method allows to remove entire neurons and convolutional filters which results in lighter architectures and a significant inference speed-up with standard deep learning frameworks.
* [Bayesian Compression for Natural Language Processing](https://arxiv.org/abs/1810.10927) adapts the Sparse Variational Dropout techniques for sparsification of various recurrent architectures. 

**New:** Google AI Research recently has released [State of Sparsity in Deep Neural Networks](https://arxiv.org/abs/1902.09574) - a nice large scale study of sparsification methods. The code contains an implementation of [Sparse variational dropout on Tensorflow](https://github.com/google-research/google-research/blob/master/state_of_sparsity/layers/variational_dropout/nn.py#L585).

# Citation

If you found this code useful please cite our paper 

```
@article{molchanov2017variational
  title={Variational Dropout Sparsifies Deep Neural Networks}
  author={Molchanov Dmitry and Ashukha Arsenii and Vetrov Dmitry}
  journal={arXiv preprint arXiv:1701.05369}
  year={2017}
}
```
</experiment>"
"Abstract:  The Kepler survey provides a statistical census of planetary systems out to
the habitable zone. Because most planets are non-transiting orbital
architectures are best estimated using simulated observations of ensemble
populations. Here we introduce EPOS the Exoplanet Population Observation
Simulator to estimate the prevalence and orbital architectures of multi-planet
systems based on the latest Kepler data release DR25. We estimate that at
least 42% of sun-like stars have nearly coplanar planetary systems with 7 or
more exoplanets. The fraction of stars with at least one planet within 1 au
could be as high as 100% depending on assumptions about the distribution of
single transiting planets. We estimate an occurrence rate of planets in the
habitable zone around sun-like stars of eta_earth=36+-14%. The innermost
planets in multi-planet systems are clustered around an orbital period of 10
days (0.1 au) reminiscent of the protoplanetary disk inner edge or could be
explained by a planet trap at that location. Only a small fraction of planetary
systems have the innermost planet at long orbital periods with fewer than ~8%
and ~3% having no planet interior to the orbit of Mercury and Venus
respectively. These results reinforce the view that the solar system is not a
typical planetary system but an outlier among the distribution of known
exoplanetary systems. We predict that at least half of the habitable zone
exoplanets are accompanied by (non-transiting) planets at shorter orbital
periods hence knowledge of a close-in exoplanet could be used as a way to
optimize the search for Earth-size planets in the Habitable Zone with future
direct imaging missions.
# Variational Dropout Sparsifies Deep Neural Networks

This repo contains the code for our ICML17 paper [Variational Dropout Sparsifies Deep Neural Networks](https://arxiv.org/abs/1701.05369) ([talk](https://vimeo.com/238221185) [slides](https://docs.google.com/presentation/d/1Lg86MnGbksn3AtehADxSG-FcrbT2DMmGmjHUqG-EQYw/edit?usp=sharing) [poster](http://ars-ashuha.ru/pdf/vdsdnn/svdo-poster.pdf) [blog-post](https://research.yandex.com/news/yandex-at-icml-2017-variational-dropout-sparsifies-deep-neural-networks)). 
We showed that Variational Dropout leads to extremely sparse solutions both in fully-connected and convolutional layers. 
Sparse VD reduced the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy. 
This effect is similar to the Automatic Relevance Determination effect in empirical Bayes.
However in Sparse VD the prior distribution remaines fixed so there is no additional risk of overfitting.

We visualize the weights of Sparse VD LeNet-5-Caffe network and demonstrate several filters of the first convolutional layer and a piece of the fully-connected layer :)

<p align=""center"">
<img height=""318"" src=""http://ars-ashuha.ru/pdf/vdsdnn/conv.gif""/>
<img height=""320"" src=""http://ars-ashuha.ru/pdf/vdsdnn/fc.gif""/>
</p>

## ICML 2017 Oral Presentation by Dmitry Molchanov

[![ICML 2017 Oral Presentation by Dmitry Molchanov](http://ars-ashuha.ru/images/icml2017-oral.png)](https://vimeo.com/238221185)

## MNIST Experiments 

The table containes the comparison of different sparsity-inducing techniques (Pruning (Han et al. 2015b;a) DNS (Guo et al. 2016) SWS (Ullrich et al. 2017)) on LeNet architectures.
Our method provides the highest level of sparsity with a similar accuracy

| Network       | Method   | Error | Sparsity per Layer  |  Compression |
| -------------: | -------- | ----- | ------------------- | :--------------: |
|               | Original | 1.64  |                     | 1              |
|               | Pruning  | 1.59  | 92.0 − 91.0 − 74.0  | 12             |
| LeNet-300-100 | DNS      | 1.99  | 98.2 − 98.2 − 94.5  | 56             |
|               | SWS      | 1.94  |                     | 23             |
| (ours)        | SparseVD | 1.92  | 98.9 − 97.2 − 62.0  | **68**         |
||||||
|               | Original | 0.8   |                     | 1              |
|               | Pruning  | 0.77  | 34 − 88 − 92.0 − 81 | 12             |
| LeNet-5       | DNS      | 0.91  | 86 − 97 − 99.3 − 96 | 111            |
|               | SWH      | 0.97  |                     | 200            |
| (ours)        | SparseVD | 0.75  | 67 − 98 − 99.8 − 95 | **280**        |


## CIFAR Experiments

The plot contains the accuracy and sparsity level for VGG-like architectures of different sizes.
The number of neurons and filters scales as _k_.
Dense networks were trained with Binary Dropout and Sparse VD networks were trained with Sparse Variational Dropout on all layers.
The overall sparsity level achieved by our method is reported as a dashed line.
The accuracy drop is negligible in most cases and the sparsity level is high especially in larger networks.

<p align=""center"">
<img height=""318"" src=""http://ars-ashuha.ru/pdf/vdsdnn/vgg.png""/>
</p>

# Environment setup

```(bash)
sudo apt install virtualenv python-pip python-dev
virtualenv venv --system-site-packages
source venv/bin/activate

pip install numpy tabulate 'ipython[all]' sklearn matplotlib seaborn  
pip install --upgrade https://github.com/Theano/Theano/archive/rel-0.9.0.zip
pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip
```

# Launch experiments 

```(bash)
source ~/venv/bin/activate
cd variational-dropout-sparsifies-dnn
THEANO_FLAGS='floatX=float32device=gpu0lib.cnmem=1' ipython ./experiments/<experiment>.py
```

PS: If you have CuDNN problem please look at this [issue](https://github.com/ars-ashuha/variational-dropout-sparsifies-dnn/issues/3).

# Further extensions

These two papers heavily rely on the Sparse Variational Dropout technique and extend it to other applications:
* [Structured Bayesian Pruning via Log-Normal Multiplicative Noise](https://arxiv.org/abs/1705.07283) provides a way to enforse _structured_ sparsity using a similar technique. This method allows to remove entire neurons and convolutional filters which results in lighter architectures and a significant inference speed-up with standard deep learning frameworks.
* [Bayesian Compression for Natural Language Processing](https://arxiv.org/abs/1810.10927) adapts the Sparse Variational Dropout techniques for sparsification of various recurrent architectures. 

**New:** Google AI Research recently has released [State of Sparsity in Deep Neural Networks](https://arxiv.org/abs/1902.09574) - a nice large scale study of sparsification methods. The code contains an implementation of [Sparse variational dropout on Tensorflow](https://github.com/google-research/google-research/blob/master/state_of_sparsity/layers/variational_dropout/nn.py#L585).

# Citation

If you found this code useful please cite our paper 

```
@article{molchanov2017variational
  title={Variational Dropout Sparsifies Deep Neural Networks}
  author={Molchanov Dmitry and Ashukha Arsenii and Vetrov Dmitry}
  journal={arXiv preprint arXiv:1701.05369}
  year={2017}
}
```
</experiment>"
"Abstract:  Splitting and rephrasing a complex sentence into several shorter sentences
that convey the same meaning is a challenging problem in NLP. We show that
while vanilla seq2seq models can reach high scores on the proposed benchmark
(Narayan et al. 2017) they suffer from memorization of the training set which
contains more than 89% of the unique simple sentences from the validation and
test sets. To aid this we present a new train-development-test data split and
neural models augmented with a copy-mechanism outperforming the best reported
baseline by 8.68 BLEU and fostering further progress on the task.
**Data and source code accompanying the paper ""Split and Rephrase: Better Evaluation and a Stronger Baseline"".**

Roee Aharoni and Yoav Goldberg ACL 2018 

The data and some of the scripts are based on the repository by Narayan et al.: https://github.com/shashiongithub/Split-and-Rephrase

This repository includes: 

- The proposed data split under `data/baseline-seq2seq-split-RDFs-relations.zip`.

- Scripts for: 

  - Training our proposed models using openNMT-py (under `src/training_scripts`)

  - Evaluating the models as proposed by Narayan et al. 2017 (under `src/evaluate.py`)

  - Creating the RDF-based data split to reduce overlap between the development and test set found in the original split (under `src/data/create_new_split.py`)

Feel free to reach out in `roee.aharoni@gmail.com` if you have any further questions! "
"Abstract:  Cryptocurrencies (or digital tokens digital currencies e.g. BTC ETH XRP
NEO) have been rapidly gaining ground in use value and understanding among
the public bringing astonishing profits to investors. Unlike other money and
banking systems most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations which makes a reliable credit
rating system for ICO projects necessary and urgent.
In this paper we introduce IcoRating the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2251 digital currencies to date such as white
paper content founding teams Github repositories websites etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting the proposed system
is able to identify scam ICO projects with 0.83 precision.
We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.
**Data and source code accompanying the paper ""Split and Rephrase: Better Evaluation and a Stronger Baseline"".**

Roee Aharoni and Yoav Goldberg ACL 2018 

The data and some of the scripts are based on the repository by Narayan et al.: https://github.com/shashiongithub/Split-and-Rephrase

This repository includes: 

- The proposed data split under `data/baseline-seq2seq-split-RDFs-relations.zip`.

- Scripts for: 

  - Training our proposed models using openNMT-py (under `src/training_scripts`)

  - Evaluating the models as proposed by Narayan et al. 2017 (under `src/evaluate.py`)

  - Creating the RDF-based data split to reduce overlap between the development and test set found in the original split (under `src/data/create_new_split.py`)

Feel free to reach out in `roee.aharoni@gmail.com` if you have any further questions! "
"Abstract:  Semi-supervised learning (SSL) provides a powerful framework for leveraging
unlabeled data when labels are limited or expensive to obtain. SSL algorithms
based on deep neural networks have recently proven successful on standard
benchmark tasks. However we argue that these benchmarks fail to address many
issues that these algorithms would face in real-world applications. After
creating a unified reimplementation of various widely-used SSL techniques we
test them in a suite of experiments designed to address these issues. We find
that the performance of simple baselines which do not use unlabeled data is
often underreported that SSL methods differ in sensitivity to the amount of
labeled and unlabeled data and that performance can degrade substantially when
the unlabeled dataset contains out-of-class examples. To help guide SSL
research towards real-world applicability we make our unified reimplemention
and evaluation platform publicly available.
# realistic-ssl-evaluation

This repository contains the code for
[Realistic Evaluation of Deep Semi-Supervised Learning Algorithms](https://arxiv.org/abs/1804.09170) by Avital Oliver\* Augustus Odena\* Colin Raffel\* Ekin D. Cubuk and Ian J. Goodfellow arXiv preprint arXiv:1804.09170.

If you use the code in this repository for a published research project please cite this paper.

The code is designed to run on Python 3 using the dependencies listed in `requirements.txt`.
You can install the dependencies by running `pip3 install -r requirements.txt`.

The latest version of this repository can be found
[here](https://github.com/brain-research/realistic-ssl-evaluation).

# Prepare datasets

For SVHN and CIFAR-10 we provide scripts to automatically download and preprocess the data.
We also provide a script to create ""label maps"" which specify which entries of the dataset should be treated as labeled and unlabeled. Both of these scripts use an explicitly chosen random seed so the same dataset order and label maps will be created each time. The random seeds can be overridden for example to test robustness to different labeled splits.
Run those scripts as follows:

```sh
python3 build_tfrecords.py --dataset_name=cifar10
python3 build_label_map.py --dataset_name=cifar10
python3 build_tfrecords.py --dataset_name=svhn
python3 build_label_map.py --dataset_name=svhn
```

For ImageNet 32x32 (only used in the fine-tuning experiment) you'll first need to download the 32x32 version of the ImageNet dataset by following the instructions [here](https://patrykchrabaszcz.github.io/Imagenet32/).
Unzip the resulting files and put them in a directory called 'data/imagenet_32'.
You'll then need to convert those files (which are pickle files) into .npy files.
You can do this by executing:

```sh
mkdir data/imagenet_32
unzip Imagenet32_train.zip -d data/imagenet_32
unzip Imagenet32_val.zip -d data/imagenet_32
python3 convert_imagenet.py
```

Then you can build the TFRecord files like so:

```sh
python3 build_tfrecords.py --dataset_name=imagenet_32
```

ImageNet32x32 is the only dataset which must be downloaded manually due to licensing issues.

# Running experiments

All of the experiments in our paper are accompanied by a .yml file in `runs/`.These .yml files are intended to be used with [tmuxp](https://github.com/tmux-python/tmuxp) which is a session manager for tmux.
They essentially provide a simple way to create a tmux session with all of the relevant tasks running (model training and evaluation).
The .yml files are named according to their corresponding figure/table/section in the paper.
For example if you want to run an experiment evaluating VAT with 500 labels as shown in Figure 3 you could run

```sh
tmuxp load runs/figure-3-svhn-500-vat.yml
```

Of course you can also run the code without using tmuxp.
Each .yml file specifies the commands needed for running each experiment.
For example the file listed above `runs/figure-3-svhn-500-vat.yml` runs

```sh
CUDA_VISIBLE_DEVICES=0 python3 train_model.py --verbosity=0 --primary_dataset_name='svhn' --secondary_dataset_name='svhn' --root_dir=/mnt/experiment-logs/figure-3-svhn-500-vat --n_labeled=500 --consistency_model=vat --hparam_string=""""  2&gt;&amp;1 | tee /mnt/experiment-logs/figure-3-svhn-500-vat_train.log
CUDA_VISIBLE_DEVICES=1 python3 evaluate_model.py --split=test --verbosity=0 --primary_dataset_name='svhn' --root_dir=/mnt/experiment-logs/figure-3-svhn-500-vat --consistency_model=vat --hparam_string=""""  2&gt;&amp;1 | tee /mnt/experiment-logs/figure-3-svhn-500-vat_eval_test.log
CUDA_VISIBLE_DEVICES=2 python3 evaluate_model.py --split=valid --verbosity=0 --primary_dataset_name='svhn' --root_dir=/mnt/experiment-logs/figure-3-svhn-500-vat --consistency_model=vat --hparam_string=""""  2&gt;&amp;1 | tee /mnt/experiment-logs/figure-3-svhn-500-vat_eval_valid.log
CUDA_VISIBLE_DEVICES=3 python3 evaluate_model.py --split=train --verbosity=0 --primary_dataset_name='svhn' --root_dir=/mnt/experiment-logs/figure-3-svhn-500-vat --consistency_model=vat --hparam_string=""""  2&gt;&amp;1 | tee /mnt/experiment-logs/figure-3-svhn-500-vat_eval_train.log
```

Note that these commands are formulated to write out results to `/mnt/experiment-logs`.
You will either need to create this directory or modify them to write to a different directory.
Further the .yml files are written to assume that this source tree lives in `/root/realistic-ssl-evaluation`.

## A note on reproducibility

While the focus of our paper is reproducibility ultimately exact comparison to the results in our paper will be conflated by subtle differences such as the version of TensorFlow used random seeds etc.
In other words simply copying the numbers stated in our paper may not provide a means for reliable comparison.
As a result if you'd like to use our implementation of baseline methods as a point of comparison for e.g. a new semi-supervised learning technique we'd recommend re-running our experiments from scratch in the same environment as your new technique.

# Simulating small validation sets

The following command runs evaluation on a set of checkpoints with multiple resamples of small
validation sets (as in figure 5 in the paper):

```sh
python3 evaluate_checkpoints.py --primary_dataset_name='cifar10' --checkpoints='/mnt/experiment-logs/section-4-3-cifar-fine-tuning/default/model.ckpt-1000000/mnt/.../model.ckpt-......'
```

Results are printed to stdout for each evaluation run and at the end a string representation of the entire list
of validation accuracies for each resampled validation set and each checkpoint is printed:

```
{'/mnt/experiment-logs/table-1-svhn-1000-pi-model-run-5/default/model.ckpt-500001': [0.86 0.93 0.92 0.91 0.9 0.94 0.91 0.88 0.88 0.89]}
```

# Disclaimer

This is not an official Google product."
"Abstract:  Multi-hop reasoning is an effective approach for query answering (QA) over
incomplete knowledge graphs (KGs). The problem can be formulated in a
reinforcement learning (RL) setup where a policy-based agent sequentially
extends its inference path until it reaches a target. However in an incomplete
KG environment the agent receives low-quality rewards corrupted by false
negatives in the training data which harms generalization at test time.
Furthermore since no golden action sequence is used for training the agent
can be misled by spurious search trajectories that incidentally lead to the
correct answer. We propose two modeling advances to address both issues: (1) we
reduce the impact of false negative supervision by adopting a pretrained
one-hop embedding model to estimate the reward of unobserved facts; (2) we
counter the sensitivity to spurious paths of on-policy RL by forcing the agent
to explore a diverse set of paths using randomly generated edge masks. Our
approach significantly improves over existing path-based KGQA models on several
benchmark datasets and is comparable or better than embedding-based models.
# Multi-Hop Knowledge Graph Reasoning with Reward Shaping

This repository contains the source code release of the paper: [Lin et. al. 2018. Multi-Hop Knowledge Graph Reasoning with Reward Shaping](https://arxiv.org/abs/1808.10568).

## Quick Start

### Environment Variables &amp; Dependencies
#### Use Docker
Build the docker image
```
docker build -&lt; Dockerfile -t multi_hop_kg:v1.0
```

Spin up a docker container and run experiments inside it.
```
nvidia-docker run -v `pwd`:/workspace/MultiHopKG -it multi_hop_kg:v1.0
```
*The rest of the readme assumes that one works interactively inside a container. If you prefer to run experiments outside a container please change the commands accordingly.*

#### Mannually Set up 
Alternatively you can install Pytorch (&gt;=0.4.1) manually and use the Makefile to set up the rest of the dependencies. 
```
make setup
```

### Process data
First unpack the data files 
```
tar xvzf data-release.tgz
```
and run the following command to preprocess the datasets.
```
./experiment.sh configs/<dataset>.sh --process_data <gpu-id>
```

`<dataset>` is the name of any dataset folder in the `./data` directory. In our experiments the five datasets used are: `umls` `kinship` `fb15k-237` `wn18rr` and `nell-995`. 
`<gpu-id>` is a non-negative integer number representing the GPU index.

### Train models
Then the following commands can be used to train the proposed models and baselines in the paper. By default dev set evaluation results will be printed when training terminates.

1. Train embedding-based models
```
./experiment-emb.sh configs/<dataset>-<emb_model>.sh --train <gpu-id>
```
The following embedding-based models are implemented: `distmult` `complex` and `conve`.

2. Train RL models (policy gradient)
```
./experiment.sh configs/<dataset>.sh --train <gpu-id>
```

3. Train RL models (policy gradient + reward shaping)
```
./experiment-rs.sh configs/<dataset>-rs.sh --train <gpu-id>
```

* Note: To train the RL models using reward shaping make sure 1) you have pre-trained the embedding-based models and 2) set the file path pointers to the pre-trained embedding-based models correctly ([example configuration file](configs/umls-rs.sh)).

### Evaluate pretrained models
To generate the evaluation results of a pre-trained model simply change the `--train` flag in the commands above to `--inference`. 

For example the following command performs inference with the RL models (policy gradient + reward shaping) and prints the evaluation results (on both dev and test sets).
```
./experiment-rs.sh configs/<dataset>-rs.sh --inference <gpu-id>
```

To print the inference paths generated by beam search during inference use the `--save_beam_search_paths` flag:
```
./experiment-rs.sh configs/<dataset>-rs.sh --inference <gpu-id> --save_beam_search_paths
```

* Note for the NELL-995 dataset: 

  On this dataset we split the original training data into `train.triples` and `dev.triples` and the final model to test has to be trained with these two files combined. 
  1. To obtain the correct test set results you need to add the `--test` flag to all data pre-processing training and inference commands.  
    ```
    ./experiment.sh configs/nell-995.sh --process_data <gpu-id> --test
    ./experiment-emb.sh configs/nell-995-conve.sh --train <gpu-id> --test
    ./experiment-rs.sh configs/NELL-995-rs.sh --train <gpu-id> --test
    ./experiment-rs.sh configs/NELL-995-rs.sh --inference <gpu-id> --test
    ```
  2. Leave out the `--test` flag during development.

### Change the hyperparameters
To change the hyperparameters and other experiment set up start from the [configuration files](configs).

### Notes on Implementation Details
We use mini-batch training in our experiments. To save the amount of paddings (which can cause memory issues and slow down computation for knowledge graphs that contain nodes with large fan-outs)
we group the action spaces of different nodes into buckets based on their sizes. Description of the bucket implementation can be found
[here](https://github.com/salesforce/MultiHopKG/blob/master/src/rl/graph_search/pn.py#L193) and 
[here](https://github.com/salesforce/MultiHopKG/blob/master/src/knowledge_graph.py#L164).

## Citation
If you find the resource in this repository helpful please cite
```
@inproceedings{LinRX2018:MultiHopKG 
  author = {Xi Victoria Lin and Richard Socher and Caiming Xiong} 
  title = {Multi-Hop Knowledge Graph Reasoning with Reward Shaping} 
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural
               Language Processing {EMNLP} 2018 Brussels Belgium October
               31-November 4 2018}
  year = {2018} 
}
```
</gpu-id></gpu-id></gpu-id></gpu-id></gpu-id></dataset></gpu-id></dataset></gpu-id></dataset></gpu-id></dataset></gpu-id></emb_model></dataset></gpu-id></dataset></gpu-id></dataset>"
"Abstract:  We present a compact but effective CNN model for optical flow called
PWC-Net. PWC-Net has been designed according to simple and well-established
principles: pyramidal processing warping and the use of a cost volume. Cast
in a learnable feature pyramid PWC-Net uses the cur- rent optical flow
estimate to warp the CNN features of the second image. It then uses the warped
features and features of the first image to construct a cost volume which is
processed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in
size and easier to train than the recent FlowNet2 model. Moreover it
outperforms all published optical flow methods on the MPI Sintel final pass and
KITTI 2015 benchmarks running at about 35 fps on Sintel resolution (1024x436)
images. Our models are available on this https URL.
[![License CC BY-NC-SA 4.0](https://img.shields.io/badge/license-CC4.0-blue.svg)](https://raw.githubusercontent.com/NVIDIA/FastPhotoStyle/master/LICENSE.md)
![Python 2.7](https://img.shields.io/badge/python-2.7-green.svg)

## PWC-Net: CNNs for Optical Flow Using Pyramid Warping and Cost Volume

### License
Copyright (C) 2018 NVIDIA Corporation. All rights reserved. Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).


### Usage

For Caffe users please refer to [Caffe/README.md](Caffe/README.md).

For PyTorch users please refer to [PyTorch/README.md](PyTorch/README.md)

The PyTorch implementation almost matches the Caffe implementation (average EPE on the final pass of the Sintel training set: 2.31 by Pytorch and 2.29 by Caffe). 

### Network Architecture

PWC-Net fuses several classic optical flow estimation techniques including image pyramid warping and cost volume in an end-to-end trainable deep neural networks for achieving state-of-the-art results.

![](network.png)


### Paper &amp; Citation
[Deqing Sun Xiaodong Yang Ming-Yu Liu and Jan Kautz. ""PWC-Net: CNNs for Optical Flow Using Pyramid Warping and Cost Volume."" CVPR 2018 or arXiv:1709.02371](https://arxiv.org/abs/1709.02371)

[Updated and extended version: ""Models Matter So Does Training: An Empirical Study of CNNs for Optical Flow Estimation."" 	arXiv:1809.05571](https://arxiv.org/abs/1809.05571)

[Project page link](http://research.nvidia.com/publication/2018-02_PWC-Net:-CNNs-for)

[Talk at robust vision challenge workshop](https://www.youtube.com/watch?v=vVU8XV0Ac_0)

[Talk at CVPR 2018 conference](https://youtu.be/LBJ20kxr1a0?t=421)
 

If you use PWC-Net please cite the following paper: 
```
@InProceedings{Sun2018PWC-Net
  author    = {Deqing Sun and Xiaodong Yang and Ming-Yu Liu and Jan Kautz}
  title     = {{PWC-Net}: {CNNs} for Optical Flow Using Pyramid Warping and Cost Volume}
  booktitle = CVPR
  year      = {2018}
}
```
or the arXiv paper
```
@article{sun2017pwc
  author={Sun Deqing and Yang Xiaodong and Liu Ming-Yu and Kautz Jan}
  title={{PWC-Net}: {CNNs} for Optical Flow Using Pyramid Warping and Cost Volume}
  journal={arXiv preprint arXiv:1709.02371}
  year={2017}
}
```
or the updated and extended version
```
@article{Sun2018:Model:Training:Flow
  author={Sun Deqing and Yang Xiaodong and Liu Ming-Yu and Kautz Jan}
  title={Models Matter So Does Training: An Empirical Study of CNNs for Optical Flow Estimation}
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}
  note = {to appear}
}
```
For multi-frame flow please also cite
```
@inproceedings{ren2018fusion
  title={A Fusion Approach for Multi-Frame Optical Flow Estimation}
  author={Ren Zhile and Gallo Orazio and Sun Deqing and Yang Ming-Hsuan and Sudderth Erik B and Kautz Jan}
  booktitle={Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)}
  year={2019}
}
```
### Related Work from NVIDIA 
[flownet2-pytorch](https://github.com/NVIDIA/flownet2-pytorch)

[Learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation (ECCV 2018)](https://github.com/NVlabs/learningrigidity)

### Contact
Deqing Sun (deqings@nvidia.com)"
"Abstract:  We present a new open source free semi-analytic model (SAM) of galaxy
formation Shark designed to be highly flexible and modular allowing easy
exploration of different physical processes and ways of modelling them. We
introduce the philosophy behind Shark and provide an overview of the physical
processes included in the model. Shark is written in C++11 and has been
parallelized with OpenMP. In the released version (v1.1) we implement several
different models for gas cooling active galactic nuclei stellar and
photo-ionisation feedback and star formation (SF). We demonstrate the basic
performance of Shark using the Planck15 cosmology SURFS simulations by
comparing against a large set of observations including: the stellar mass
function (SMF) and stellar-halo mass relation at z=0-4; the cosmic evolution of
the star formation rate density (SFRD) stellar mass atomic and molecular
hydrogen; local gas scaling relations; and structural galaxy properties
finding excellent agreement. Significant improvements over previous SAMs are
seen in the mass-size relation for disks/bulges the gas-stellar mass and
stellar mass-metallicity relations. To illustrate the power of Shark in
exploring the systematic effects of the galaxy formation modelling we quantify
how the scatter of the SF main sequence and the gas scaling relations changes
with the adopted SF law and the effect of the starbursts H$_2$ depletion
timescale on the SFRD and $\Omega_{\rm H_2}$. We compare Shark with other SAMs
and the hydrodynamical simulation EAGLE and find that SAMs have a much higher
halo baryon fractions due to large amounts of intra-halo gas which in the case
of EAGLE is in the intergalactic medium.
# shark

[![Build Status](https://travis-ci.org/ICRAR/shark.svg?branch=master)](https://travis-ci.org/ICRAR/shark)
[![Build Status](https://ci.appveyor.com/api/projects/status/4vy02t8q4h4xpr7k/branch/master?svg=true)](https://ci.appveyor.com/project/rtobar/shark/branch/master)
[![Documentation Status](https://readthedocs.org/projects/shark-sam/badge/?version=latest)](https://shark-sam.readthedocs.io/en/latest/?badge=latest)
[![ASCL.net entry](https://img.shields.io/badge/ascl-1811.005-blue.svg?colorB=262255)](http://ascl.net/1811.005)

shark is a new flexible semi-analytic model of galaxy formation.

For more information read [shark's documentation](https://shark-sam.readthedocs.io/).

## Citing

As you use shark for your projects
please cite the following paper
which is the first one describing shark in full:

```
@article{doi:10.1093/mnras/sty2440
    author = {Lagos Claudia del P and Tobar Rodrigo J and Robotham Aaron S G and Obreschkow Danail and Mitchell Peter D and Power Chris and Elahi Pascal J}
    title = {Shark: introducing an open source free and flexible semi-analytic model of galaxy formation}
    journal = {Monthly Notices of the Royal Astronomical Society}
    volume = {481}
    number = {3}
    pages = {3573-3603}
    year = {2018}
    doi = {10.1093/mnras/sty2440}
    URL = {http://dx.doi.org/10.1093/mnras/sty2440}
    eprint = {/oup/backfile/content_public/journal/mnras/481/3/10.1093_mnras_sty2440/1/sty2440.pdf}
}
```

You can also find it online at [ADS NASA](https://ui.adsabs.harvard.edu/?#abs/2018MNRAS.481.3573L/abstract)."
"Abstract:  Batch-splitting (data-parallelism) is the dominant distributed Deep Neural
Network (DNN) training strategy due to its universal applicability and its
amenability to Single-Program-Multiple-Data (SPMD) programming. However
batch-splitting suffers from problems including the inability to train very
large models (due to memory constraints) high latency and inefficiency at
small batch sizes. All of these can be solved by more general distribution
strategies (model-parallelism). Unfortunately efficient model-parallel
algorithms tend to be complicated to discover describe and to implement
particularly on large clusters. We introduce Mesh-TensorFlow a language for
specifying a general class of distributed tensor computations. Where
data-parallelism can be viewed as splitting tensors and operations along the
""batch"" dimension in Mesh-TensorFlow the user can specify any
tensor-dimensions to be split across any dimensions of a multi-dimensional mesh
of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting
of parallel operations coupled with collective communication primitives such as
Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel
model-parallel version of the Transformer sequence-to-sequence model. Using TPU
meshes of up to 512 cores we train Transformer models with up to 5 billion
parameters surpassing state of the art results on WMT'14 English-to-French
translation task and the one-billion-word language modeling benchmark.
Mesh-Tensorflow is available at this https URL .
# Mesh TensorFlow - Model Parallelism Made Easier

[![PyPI
version](https://badge.fury.io/py/mesh-tensorflow.svg)](https://badge.fury.io/py/mesh-tensorflow)
[![GitHub
Issues](https://img.shields.io/github/issues/tensorflow/mesh.svg)](https://github.com/tensorflow/mesh/issues)
[![Contributions
welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://opensource.org/licenses/Apache-2.0)
[![Travis](https://img.shields.io/travis/tensorflow/mesh.svg)](https://travis-ci.org/tensorflow/mesh)


# Introduction

Mesh TensorFlow (`mtf`) is a language for distributed deep learning capable of
specifying a broad class of distributed tensor computations.  The purpose of
Mesh TensorFlow is to formalize and implement distribution strategies for your
computation graph over your hardware/processors. For example: ""Split the batch
over rows of processors and split the units in the hidden layer across columns
of processors."" Mesh TensorFlow is implemented as a layer over TensorFlow.

Watch our [YouTube video](https://www.youtube.com/watch?v=HgGyWS40g-g).


## Do I need Mesh TensorFlow?

If you just want data-parallel training (batch-splitting) then you do not need
Mesh TensorFlow though Mesh TensorFlow can do this.  The most common reasons
for more sophisticated parallel computation are:

* The parameters of the model do not fit on one device - e.g. a
5-billion-parameter language model.

* An example is so large that the activations do not fit on one device. - e.g.
large images.  TODO(noam): we still need to implement spatially-partitioned
convolutions

* Lower-latency parallel inference (at batch size 1).

## The Mesh TensorFlow Approach to Distributed Computation

* A ""Mesh"" is an n-dimensional array of processors connected by a network.

* Each tensor is distributed (split and/or replicated) across all processors
  in a mesh.

* Tensor dimensions and mesh dimensions are named.  The layouts of all tensors
  follow from a set of user-defined layout rules which specify which
  tensor-dimensions are split across which mesh-dimensions.  This ensures that
  the corresponding dimensions in different tensors are split in the same
  manner.

* Layouts do not affect results - only performance.

* The implementation of an operation involves parallel computation on all
  processors in the mesh and sometimes also collective communication.  A
  processor usually just manipulates the slices of the input tensors already
  resident on that processor and produces the slice of the output that goes on
  that processor.

## Getting Started

### Installation

To install the latest stable version run

```sh
pip install mesh-tensorflow
```

To install the latest development version run

```sh
pip install -e ""git+https://github.com/tensorflow/mesh.git#egg=mesh-tensorflow""
```

Installing `mesh-tensorflow` does not automatically install or update
TensorFlow. We recommend installing it via `pip install tensorflow` or `pip
install tensorflow-gpu`. See TensorFlowâ€™s
[installation instructions for details](https://www.tensorflow.org/install/).
If you're using a development version of Mesh TensorFlow you may need to
use TensorFlow's nightly package (`tf-nightly`).

### Example Network (MNIST)

To illustrate let us consider a simple model for the MNIST image-classification
task.  Our network has one hidden layer with 1024 units and an output layer
with 10 units (corresponding to the 10 digit classes).

The code consists of two parts the first describing the mathematical
operations and the second describing the devices and tensor/computation layout.
For the full example see [`examples/mnist.py`](
https://github.com/tensorflow/mesh/blob/master/examples/mnist.py).
TODO(noam): verify that this code works.

```Python
# tf_images is a tf.Tensor with shape [100 28 28] and dtype tf.float32
# tf_labels is a tf.Tensor with shape [100] and dtype tf.int32
graph = mtf.Graph()
mesh = mtf.Mesh(graph ""my_mesh"")
batch_dim = mtf.Dimension(""batch"" 100)
rows_dim = mtf.Dimension(""rows"" 28)
cols_dim = mtf.Dimension(""cols"" 28)
hidden_dim = mtf.Dimension(""hidden"" 1024)
classes_dim = mtf.Dimension(""classes"" 10)
images = mtf.import_tf_tensor(
    mesh tf_images shape=[batch_dim rows_dim cols_dim])
labels = mtf.import_tf_tensor(mesh tf_labels [batch_dim])
w1 = mtf.get_variable(mesh ""w1"" [rows_dim cols_dim hidden_dim])
w2 = mtf.get_variable(mesh ""w2"" [hidden_dim classes_dim])
# einsum is a generalization of matrix multiplication (see numpy.einsum)
hidden = mtf.relu(mtf.einsum(images w1 output_shape=[batch_dim hidden_dim]))
logits = mtf.einsum(hidden w2 output_shape=[batch_dim classes_dim])
loss = mtf.reduce_mean(mtf.layers.softmax_cross_entropy_with_logits(
    logits mtf.one_hot(labels classes_dim) classes_dim))
w1_grad w2_grad = mtf.gradients([loss] [w1 w2])
update_w1_op = mtf.assign(w1 w1 - w1_grad * 0.001)
update_w2_op = mtf.assign(w2 w2 - w2_grad * 0.001)
```

In the code above we have built a Mesh TensorFlow graph which is simply
a Python structure.  We have completely defined the mathematical operations.
In the code below we specify the mesh of processors and the layout of the
computation.

```Python
devices = [""gpu:0"" ""gpu:1"" ""gpu:2"" ""gpu:3""]
mesh_shape = [(""all_processors"" 4)]
layout_rules = [(""batch"" ""all_processors"")]
mesh_impl = mtf.placement_mesh_impl.PlacementMeshImpl(
    mesh_shape layout_rules devices)
lowering = mtf.Lowering(graph {mesh:mesh_impl})
tf_update_ops = [lowering.lowered_operation(update_w1_op)
                 lowering.lowered_operation(update_w2_op)]
```

The particular layout above implements data-parallelism splitting the batch of
examples evenly across all four processors.  Any Tensor with a ""batch"" dimension
(e.g. `images` `h` `logits` and their gradients) is split in that dimension
across all processors while any tensor without a ""batch"" dimension (e.g. the
model parameters) is replicated identically on every processor.

Alternatively for model-parallelism we can set
`layout_rules=[(""hidden"" ""all_processors"")]`.  In this case
any tensor with a ""hidden"" dimension (e.g. `hidden` `w1` `w2`)  is split
while any other tensor (e.g. `image` `logits`) is fully replicated.

We can even combine data-parallelism and model-parallelism on a 2-dimensional
mesh of processors.  We split the batch along one dimension of the mesh and the
units in the hidden layer along the other dimension of the mesh as below.  In
this case the hidden layer is actually tiled between the four processors being
split in both the ""batch"" and ""hidden_units"" dimensions.

```Python
mesh_shape = [(""processor_rows"" 2) (""processor_cols"" 2)]
layout_rules = [(""batch"" ""processor_rows"") (""hidden"" ""processor_cols"")]
```

## Where does the network communication happen?

Some Mesh TensorFlow operations cause network communication.  For example an
einsum (generalized matrix multiplication) is computed as follows:

* On each processor compute the einsum of the slices of the two operands that
  are local to that processor.
* If no reduced-out dimensions are split then we are done.
* If reduced-out dimensions are split then perform an ""allreduce"" operation 
  on the resulting slices - summing across any mesh dimensions over which the
  reduced-out dimensions are split.

Where the allreduces happen depends will depend on the computation layout.
For example in a data-parallel layout where the ""batch"" dimension is split
allreduces will happen when computing the parameter gradients since this
involves matrix multiplications which reduce out the ""batch"" dimension.

## How do I pick a layout?

While results do not depend on layout (except in the realm of roundoff errors
and random seeds) performance and memory consumption depend heavily on layout.
One day we hope to automate the process of choosing a layout.  For now you
really need to fully understand the performance implications and pick one
yourself.  Mesh TensorFlow helps by accumulating and printing counters of
computation/communication.  To start here are some tricks/guidelines.

* It is illegal for two dimensions of the same tensor to be split across the
  same mesh dimension.
* For any compute-intense operation (e.g. einsum) make sure that all
  mesh-dimensions are used to split dimensions of the inputs or outputs.
  Otherwise computation is duplicated.
* To keep the ratio of compute/communication high (i.e. not be bandwidth-bound)
  split dimensions into large chunks.  This should be familiar in the
  data-parallelism case where we want a large batch size per processor to avoid
  spending most of our time communicating.

# The Mesh TensorFlow Language

Mesh TensorFlow (v0.0) is implemented as a Python library which can generate
part of a TensorFlow graph.  The user first builds a `mtf.Graph` (the analog of
a TensorFlow graph) made up of `mtf.Tensor`s and `mtf.Operation`s.  As in
TensorFlow this graph consists of simple Python objects.  The user then creates
a `mtf.Lowering` object which lowers the `mtf.Graph` into TensorFlow adding to
the default TensorFlow graph.

The Mesh TensorFlow language is nearly identical to TensorFlow with the
familiar notion of a Graph Tensors Operations and automatic gradient
computation.  The principal differences are as follows:

## Meshes replace devices

A `Mesh` is a n-dimensional array of processors with named dimensions.  Each
`Tensor` is assigned to a `Mesh` instead of a device.

## Tensor dimensions are named

Each `Tensor` has a static `Shape` which is a tuple of different ""Dimensions"".
A `Dimension` is a `(name size)` pair. For example the shape of a `Tensor`
representing a batch of images might be:

`[(""batch"" 100) (""rows"" 28"") (""cols"" 28) (""channels"" 3)]`.

## Layouts

A `Tensor` is laid out on its mesh with one slice on each processor.  A `Tensor`
""layout"" is an injective partial map specifying which dimensions of the tensor
are (evenly) split across which dimensions of the mesh.  No dimension of a
tensor may be split across two dimensions of its mesh and no two dimensions of a
tensor may be split across the same dimension of its mesh.  The user defines a
global set of layout rules in the form of (tensor-dimension-name
mesh-dimension-name) pairs.  A dimension of a tensor is split across a dimension
of its mesh if there is a matching rule.

### Example Layouts

Take our example `Tensor` `image_batch` with shape: 
`[(""batch"" 100) (""rows"" 28"") (""cols"" 28) (""channels"" 3)]`

Assume that this `Tensor` is assigned to a mesh of 8 processors with shape:
`[(""processor_rows"" 2) (""processor_cols"" 4)]`

* If we use an empty set of layout rules `[]` we get no splitting.  Each
  processor contains the whole `Tensor`.

* If we use the layout rules `""batch:processor_cols""` then the `""batch""`
  dimension of the `Tensor` is split across the `""processor_cols""` dimension of
  the batch.  This means that each processor contains a Tensor slice with shape
  `[25 28 28 3]`.  For example processors (0 3) and (1 3) contain
  identical slices - `image_batch[75:100 : : :]`.

* If we use the layout rules `""rows:processor_rows;cols:processor_cols""` 
  then the image is split in two dimensions with each processor containing one
  spatial tile with shape `[100 14 7 3]`.   For example processor (0 1)
  contains the slice `image_batch[: 0:14 7:14 :]`.

Some layout rules would lead to illegal layouts:

* `""batch:processor_rows;rows:processor_rows""` is illegal because two tensor
  dimensions could not be split across the same mesh dimension.

* `""channels:processor_rows""` is illegal because the size of the tensor
  dimension is not evenly divisible by the size of the mesh dimension.

## Einsum

Mesh TensorFlow uses Einstein-summation notation `mtf.einsum(inputs
output_shape)` using the (named) `Dimensions` as the symbols.  Matrix
multiplication broadcast sum-reduction and transposition can all be expressed
as special cases of `mtf.einsum` though the familiar interfaces are also
supported.  The operation is lowered to slice-wise `tf.einsum`s followed by
allreduce across any mesh-dimensions corresponding to the summed-out Tensor
dimensions.

## Reshape can be expensive

`mtf.reshape(x new_shape)` is used to change a `Tensor`'s shape potentially
leading to a new tensor layout and hence network communication.

# CPU/GPU/TPU implementations

Mesh TensorFlow works on CPU GPU and TPU.  The TPU implementation is very
different from the CPU/GPU implementation.

Multi-CPU/GPU meshes are implemented with `PlacementMeshImpl`.  In this case
Mesh TensorFlow emits separate TensorFlow operations placed on the different
devices all in one big TensorFlow graph.

TPU meshes are implemented in with `SimdMeshImpl`.  In this case
Mesh TensorFlow emits TensorFlow operations (and communication collectives) from
the perspective of one core and this same program runs on every core relying
on the fact that each core actually performs the same operations.  This
piggy-backs on the TPU data-parallelism infrastructure which operates the same
way.  This ""SIMD"" approach keeps the TensorFlow and XLA graphs from growing with
the number of cores.  The differences between cores are as follows:

* different slices of the variables (this works now)
* different positions in the collective communication (this works now)
* different slices of the infed and outfed tensors.  We currently work around
  this by requiring that all imported/exported tensors be fully-replicated.  In
  the future we should handle this correctly.

# Instructions for running on cloud-tpu

Note: It requires `tensorflow&gt;=1.11.0`.

## Prerequisite

Please go through the
[Transformer tutorial](https://cloud.google.com/tpu/docs/tutorials/transformer).

## Create VM and TPU instance in Cloud console

TODO(trandustinylc): update given mtf pypi package

```sh
ctpu up -name=ylc-mtf-donut -tf-version=nightly -tpu-size=v2-8 -zone=us-central1-b
```

## SSH into VM

```sh
git clone https://github.com/tensorflow/mesh.git
cd mesh/
pip install --user .
```

## Run the Transfomer model (no Tensor2Tensor dependencies)

```sh
pip install tensorflow_datasets

cd mesh/
DATA_DIR=gs://noam-mtf/data
MODEL_DIR=gs://noam-mtf/transformer_standalone
TPU=noam-mtf-donut

# MODEL HPARAMS AND DIRECTORY  (uncomment one)
# base model
MODEL=./transformer/gin/model_base.gin
# 5B parameters (too big for this dataset only trains with model-parallelism)
# MODEL=./transformer/gin/model_5b.gin

# UNCOMMENT ONE OF THESE
# Data-parallelism
LAYOUT=./transformer/gin/layout_data_parallel.gin
# Model-parallelism
# LAYOUT=./transformer/gin/layout_model_parallel.gin
# Data-parallelism and Model-Parallelism
# LAYOUT=./transformer/gin/layout_data_and_model_parallel.gin

# TRAIN
python examples/transformer_standalone.py \
  --tpu=$TPU --data_dir=$DATA_DIR --model_dir=$MODEL_DIR --gin_file=$MODEL \
  --gin_file=$LAYOUT --gin_param=""run.mode='train'""

# EVAL
python examples/transformer_standalone.py \
  --tpu=$TPU --data_dir=$DATA_DIR --model_dir=$MODEL_DIR --gin_file=$MODEL \
  --gin_file=$LAYOUT --gin_param=""run.mode='evaluate'""
```

The above code will train on the LM1B language modeling benchmark as specified
in `examples/transformer_standalone_defaults.gin`. To train a
sequence-to-sequence model on WMT14 en-de change `utils.run.dataset` to
`wmt_translate_ende/ende_subwords8k_t2t` and set `utils.run.mode` to `True`.
Note that the `wmt_translate_ende/ende_subwords8k_t2t` dataset was removed from
TensorFlow Datasets in
[commit 211cb6f](https://github.com/tensorflow/datasets/commit/211cb6f082c5cc3c482e37d70234142a8fda2db3)
so in order to train a model using this dataset you need to install a version of
TFDS before this commit. Then you can decode the WMT en-de development set
and evaluate it using [SacreBLEU](https://github.com/mjpost/sacreBLEU) like so:

```
# INFER
pip3 install sacrebleu
mkdir ~/input ~/output
DECODE_INPUT=/home/$USER/input/ende.dev
DECODE_OUTPUT=/home/$USER/output/ende.dev.out
~/.local/bin/sacrebleu -t wmt13 -l en-de --echo src &gt; $DECODE_INPUT
python examples/transformer_standalone.py \
  --tpu=$TPU --data_dir=$DATA_DIR --model_dir=$MODEL_DIR --gin_file=$MODEL \
  --gin_file=$LAYOUT \
  --gin_param=""decode_from_file.input_filename='$DECODE_INPUT'"" \
  --gin_param=""decode_from_file.output_filename='$DECODE_OUTPUT'"" \
  --gin_param=""run.mode='infer'""

# Compute BLEU score for dev set
cat $DECODE_OUTPUT | ~/.local/bin/sacrebleu -t wmt13 -l en-de -tok intl
```


## Run the Transfomer model with Tensor2Tensor config
```sh
git clone https://github.com/tensorflow/tensor2tensor.git
cd tensor2tensor/
pip install --user  .
```

Before running the model you need to prepare the training data and bucket for
storing checkpoints. Refer to the
[Transformer tutorial](https://cloud.google.com/tpu/docs/tutorials/transformer)
to learn how to generate the training data and create buckets.

```sh
CONF=mtf_transformer_paper_tr_0_mesh_8
NAME=ende_$CONF\_0828
MODEL=mtf_transformer
PROBLEM=translate_ende_wmt32k_packed

DATA_DIR=gs://xxxx
OUT_DIR=gs://xxxx
TPU_NAME=ylc-mtf-donut

tensor2tensor/bin/t2t-trainer \
  --model=$MODEL \
  --hparams_set=$CONF \
  --problem=$PROBLEM \
  --train_steps=10000 \
  --eval_steps=200 \
  --data_dir=$DATA_DIR \
  --output_dir=$OUT_DIR \
  --use_tpu=True \
  --cloud_tpu_name=$TPU_NAME
```


## Run the toy model without Tensor2Tensor dependencies

  This toy model contains two fully-connected layers which aim to train a
  identity function: f(x) = x. Since there are 8 TPU cores we can arbitrary
  change the FLAGS.mesh_shape and FLAGS.layout to achieve different
  data-parallelism and model-parallelism strategies.

```sh
MODEL_DIR=gs://xxxx
TPU_NAME=ylc-mtf-donut

# 2 ways data-parallelism and 4 ways model-parallelism.
# In this configuration we split the batch dimension into 2 cores and the
# hidden dimension into 4 cores.
python examples/toy_model_tpu.py \
  --tpu=$TPU \
  --model_dir=$MODEL_DIR \
  --io_size=8 \
  --hidden_size=8 \
  --mesh_shape='x:2;y:4' \
  --layout='batch:x;hidden:y'

# 8 ways model-parallelism.
# In this configuration We split the hidden dimension into 8 cores.
python examples/toy_model_tpu.py \
  --tpu=$TPU \
  --model_dir=$MODEL_DIR \
  --io_size=8 \
  --hidden_size=8 \
  --mesh_shape='all:8' \
  --layout='hidden:all'
```

## References

&gt; N. Shazeer Y. Cheng N. Parmar D. Tran A. Vaswani P. Koanantakool
&gt; P. Hawkins H. Lee M. Hong C. Young R. Sepassi and B. Hechtman.
&gt; [Mesh-TensorFlow: Deep learning for supercomputers.](https://arxiv.org/abs/1811.02084)
&gt; In _Neural Information Processing Systems_ 2018.

```none
@inproceedings{shazeer2018mesh
  author = {Noam Shazeer and Youlong Cheng and Niki Parmar and Dustin Tran and Ashish Vaswani and Penporn Koanantakool and Peter Hawkins and HyoukJoong Lee and Mingsheng Hong and Cliff Young and Ryan Sepassi and Blake Hechtman}
  title = {{Mesh-TensorFlow}: Deep Learning for Supercomputers}
  booktitle = {Neural Information Processing Systems}
  year = {2018}
}
```"
"Abstract:  Speech rhythms have been dealt with in three main ways: from the
introspective analyses of rhythm as a correlate of syllable and foot timing in
linguistics and applied linguistics through analyses of durations of segments
of utterances associated with consonantal and vocalic properties syllables
feet and words to models of rhythms in speech production and perception as
physical oscillations. The present study avoids introspection and
human-filtered annotation methods and extends the signal processing paradigm of
amplitude envelope spectrum analysis by adding an additional analytic step of
edge detection and postulating the co-existence of multiple speech rhythms in
rhythm zones marked by identifiable edges (Rhythm Zone Theory RZT). An
exploratory investigation of the utility of RZT is conducted suggesting that
native and non-native readings of the same text are distinct sub-genres of read
speech: a reading by a US native speaker and non-native readings by relatively
low-performing Cantonese adult learners of English. The study concludes by
noting that with the methods used RZT can distinguish between the speech
rhythms of well-defined sub-genres of native speaker reading vs. non-native
learner reading but needs further refinement in order to be applied to the
paradoxically more complex speech of low-performing language learners whose
speech rhythms are co-determined by non-fluency and disfluency factors in
addition to well-known linguistic factors of grammar vocabulary and discourse
constraints.
# CleverHans (latest release: v3.0.1)

<img alt=""cleverhans logo"" src=""https://github.com/tensorflow/cleverhans/blob/master/assets/logo.png?raw=true""/>

[![Build Status](https://travis-ci.org/tensorflow/cleverhans.svg?branch=master)](https://travis-ci.org/tensorflow/cleverhans)
[![Documentation Status](https://readthedocs.org/projects/cleverhans/badge/?version=latest)](https://cleverhans.readthedocs.io/en/latest/?badge=latest)

This repository contains the source code for CleverHans a Python library to
benchmark machine learning systems' vulnerability to
[adversarial examples](http://karpathy.github.io/2015/03/30/breaking-convnets/).
You can learn more about such vulnerabilities on the accompanying [blog](http://cleverhans.io).

The CleverHans library is under continual development always welcoming
[contributions](https://github.com/tensorflow/cleverhans#contributing)
of the latest attacks and defenses.
In particular we always welcome help towards resolving the [issues](https://github.com/tensorflow/cleverhans/issues)
currently open.

## Major updates coming to CleverHans

CleverHans will soon support 3 frameworks: JAX PyTorch and TF2.  The package
itself will focus on its initial principle: reference implementation of attacks
against machine learning models to help with benchmarking models against
adversarial examples. This repository will also contain two folders:
`tutorials/` for scripts demonstrating the features of CleverHans and
`defenses/` for scripts that contain authoritative implementations of defenses
in one of the 3 supported frameworks. The structure of the future repository
will look like this:

```
cleverhans/
  jax/
    attacks/
      ...
  tf2/
    attacks/
      ...
  torch/
    attacks/
      ...
defenses/
  jax/
    ...
  tf2/
    ...
  torch/
    ...
tutorials/
  jax/
    ...
  tf2/
    ...
  torch/
    ...
```

In the meanwhile all of these folders can be found in the correspond `future/`
subdirectory (e.g. `cleverhans/future/jax/attacks` or `defenses/future/jax/`).

A public milestone has been created to track the changes that are to be
implemented before the library version is incremented to v4. 

## Setting up CleverHans

### Dependencies

This library uses [TensorFlow](https://www.tensorflow.org/) to accelerate graph
computations performed by many machine learning models.
Therefore installing TensorFlow is a pre-requisite.

You can find instructions
[here](https://www.tensorflow.org/install/).
For better performance it is also recommended to install TensorFlow
with GPU support (detailed instructions on how to do this are available
in the TensorFlow installation documentation).

Installing TensorFlow will
take care of all other dependencies like `numpy` and `scipy`.

### Installation

Once dependencies have been taken care of you can install CleverHans using
`pip` or by cloning this Github repository.

#### `pip` installation

If you are installing CleverHans using `pip` run the following command
after installing TensorFlow:

```
pip install cleverhans
```

This will install the last version uploaded to
[Pypi](https://pypi.org/project/cleverhans).
If you'd instead like to install the bleeding edge version use:

```
pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans
```

#### Installation for development

If you want to make an editable installation of CleverHans so that you can
develop the library and contribute changes back first fork the repository
on GitHub and then clone your fork into a directory of your choice:

```
git clone https://github.com/tensorflow/cleverhans
```

You can then install the local package in ""editable"" mode in order to add it to
your `PYTHONPATH`:

```
cd cleverhans
pip install -e .
```

### Currently supported setups

Although CleverHans is likely to work on many other machine configurations we
currently [test it](https://travis-ci.org/tensorflow/cleverhans) it with Python
3.5 and TensorFlow {1.8 1.12} on Ubuntu 14.04.5 LTS (Trusty Tahr).
Support for Python 2.7 is deprecated.
CleverHans 3.0.1 supports Python 2.7 and the master branch is likely to
continue to work in Python 2.7 for some time but we no longer run the tests
in Python 2.7 and we do not plan to fix bugs affecting only Python 2.7 after
2019-07-04.
Support for TensorFlow prior to 1.12 is deprecated.
Backwards compatibility wrappers for these versions may be removed after 2019-07-07
and we will not fix bugs for those versions after that date.
Support for TensorFlow 1.7 and earlier is already deprecated: we do not fix
bugs for those versions and any remaining wrapper code for those versions
may be removed without further notice.

## Getting support

If you have a request for support please ask a question
on [StackOverflow](https://stackoverflow.com/questions/tagged/cleverhans)
rather than opening an issue in the GitHub tracker. The GitHub
issue tracker should *only* be used to report bugs or make feature requests.

## Contributing

Contributions are welcomed! To speed the code review process we ask that:
* New efforts and features be coordinated
on the mailing list for CleverHans development: [cleverhans-dev@googlegroups.com](https://groups.google.com/forum/#!forum/cleverhans-dev).
* When making code contributions to CleverHans you follow the
`PEP8 with two spaces` coding style (the same as the one used
by TensorFlow) in your pull requests.
In most cases this can be done by running `autopep8 -i --indent-size 2 <file>`
on the files you have edited.
You can check your code by running `nosestests cleverhans/devtools/tests/test_format.py` or check an individual file by running `pylint <file>` from inside the cleverhans repository root directory.
* When making your first pull request you [sign the Google CLA](https://cla.developers.google.com/clas)
* We do not accept pull requests that add git submodules because of [the
  problems that arise when maintaining git
  submodules](https://medium.com/@porteneuve/mastering-git-submodules-34c65e940407)

Bug fixes can be initiated through Github pull requests.

## Scripts: `scripts` directory

The `scripts` directory contains command line utilities.
In many cases you can use these to run CleverHans functionality on your
saved models without needing to write any of your own Python code.

You may want to set your `.bashrc` / `.bash_profile` file to add the
CleverHans `scripts` directory to your `PATH` environment variable
so that these scripts will be conveniently executable from any directory.

## Tutorials: `cleverhans_tutorials` directory

To help you get started with the functionalities provided by this library the
`cleverhans_tutorials/` folder comes with the following tutorials:
* **MNIST with FGSM** ([code](cleverhans_tutorials/mnist_tutorial_tf.py)): this
tutorial covers how to train a MNIST model using TensorFlow
craft adversarial examples using the [fast gradient sign method](https://arxiv.org/abs/1412.6572)
and make the model more robust to adversarial examples using adversarial training.
* **MNIST with FGSM using Keras** ([code](cleverhans_tutorials/mnist_tutorial_keras_tf.py)): this
tutorial covers how to define a MNIST model with Keras and train it using TensorFlow
craft adversarial examples using the [fast gradient sign method](https://arxiv.org/abs/1412.6572)
and make the model more robust to adversarial
examples using adversarial training.
* **MNIST with JSMA** ([code](cleverhans_tutorials/mnist_tutorial_jsma.py)): this second
tutorial covers how to define a MNIST model with Keras and train it using TensorFlow and
craft adversarial examples using the [Jacobian-based saliency map approach](https://arxiv.org/abs/1511.07528).
* **MNIST using a black-box attack** ([code](cleverhans_tutorials/mnist_blackbox.py)):
this tutorial implements the black-box
attack described in this [paper](https://arxiv.org/abs/1602.02697).
The adversary train a substitute model: a copy that imitates the black-box
model by observing the labels that the black-box model assigns to inputs chosen
carefully by the adversary. The adversary then uses the substitute
model’s gradients to find adversarial examples that are misclassified by the
black-box model as well.

NOTE: the tutorials are maintained carefully in the sense that we use
continuous integration to make sure they continue working. They are not
considered part of the API and they can change at any time without warning.
You should not write 3rd party code that imports the tutorials and expect
that the interface will not break. Only the main library is subject to
our six month interface deprecation warning rule.

NOTE: please write to cleverhans-dev@googlegroups.com before writing a new
tutorial. Because each new tutorial involves a large amount of duplicated
code relative to the existing tutorials and because every line of code
requires ongoing testing and maintenance indefinitely we generally prefer
not to add new tutorials. Each tutorial should showcase an extremely different
way of using the library. Just calling a different attack model or dataset
is not enough to justify maintaining a parallel tutorial.

## Examples : `examples` directory

The `examples/` folder contains additional scripts to showcase different uses
of the CleverHans library or get you started competing in different adversarial
example contests. We do not offer nearly as much ongoing maintenance or support
for this directory as the rest of the library and if code in here gets broken
we may just delete it without warning.

## List of attacks

You can find a full list attacks along with their function signatures at [cleverhans.readthedocs.io](http://cleverhans.readthedocs.io/)

## Reporting benchmarks

When reporting benchmarks please:
* Use a versioned release of CleverHans. You can find a list of released versions [here](https://github.com/tensorflow/cleverhans/releases).
* Either use the latest version or if comparing to an earlier publication use the same version as the earlier publication.
* Report which attack method was used.
* Report any configuration variables used to determine the behavior of the attack.

For example you might report ""We benchmarked the robustness of our method to
adversarial attack using v3.0.1 of CleverHans. On a test set modified by the
`FastGradientMethod` with a max-norm `eps` of 0.3 we obtained a test set accuracy of 71.3%.""

## Citing this work

If you use CleverHans for academic research you are highly encouraged
(though not required) to cite the following [paper](https://arxiv.org/abs/1610.00768):

```
@article{papernot2018cleverhans
  title={Technical Report on the CleverHans v2.1.0 Adversarial Examples Library}
  author={Nicolas Papernot and Fartash Faghri and Nicholas Carlini and
  Ian Goodfellow and Reuben Feinman and Alexey Kurakin and Cihang Xie and
  Yash Sharma and Tom Brown and Aurko Roy and Alexander Matyasko and
  Vahid Behzadan and Karen Hambardzumyan and Zhishuai Zhang and
  Yi-Lin Juang and Zhi Li and Ryan Sheatsley and Abhibhav Garg and
  Jonathan Uesato and Willi Gierke and Yinpeng Dong and David Berthelot and
  Paul Hendricks and Jonas Rauber and Rujun Long}
  journal={arXiv preprint arXiv:1610.00768}
  year={2018}
}
```

## About the name

The name CleverHans is a reference to a presentation by Bob Sturm titled
“Clever Hans Clever Algorithms: Are Your Machine Learnings Learning What You
Think?"" and the corresponding publication [""A Simple Method to Determine if a
Music Information Retrieval System is a
'Horse'.""](http://ieeexplore.ieee.org/document/6847693/) Clever Hans was a
horse that appeared to have learned to answer arithmetic questions but had in
fact only learned to read social cues that enabled him to give the correct
answer. In controlled settings where he could not see people's faces or receive
other feedback he was unable to answer the same questions. The story of Clever
Hans is a metaphor for machine learning systems that may achieve very high
accuracy on a test set drawn from the same distribution as the training data
but that do not actually understand the underlying task and perform poorly on
other inputs.

## Authors

This library is managed and maintained by Ian Goodfellow (Google Brain) and
Nicolas Papernot (Google Brain).

The following authors contributed 100 lines or more (ordered according to the GitHub contributors page):
* Ian Goodfellow (Google Brain)
* Nicolas Papernot (Google Brain)
* Nicholas Carlini (Google Brain)
* Fartash Faghri (University of Toronto)
* Tzu-Wei Sung (National Taiwan University)
* Alexey Kurakin (Google Brain)
* Reuben Feinman (New York University)
* Phani Krishna (Video Analytics Lab)
* David Berthelot (Google Brain)
* Tom Brown (Google Brain)
* Cihang Xie (Johns Hopkins)
* Yash Sharma (The Cooper Union)
* Aashish Kumar (HARMAN X)
* Aurko Roy (Google Brain)
* Alexander Matyasko (Nanyang Technological University)
* Anshuman Suri (Microsoft)
* Yen-Chen Lin (MIT)
* Vahid Behzadan (Kansas State)
* Jonathan Uesato (DeepMind)
* Haojie Yuan (University of Science &amp; Technology of China)
* Zhishuai Zhang (Johns Hopkins)
* Karen Hambardzumyan (YerevaNN)
* Jianbo Chen (UC Berkeley)
* Catherine Olsson (Google Brain)
* Aidan Gomez (University of Oxford)
* Zhi Li (University of Toronto)
* Yi-Lin Juang (NTUEE)
* Pratyush Sahay (formerly HARMAN X)
* Abhibhav Garg (IIT Delhi)
* Aditi Raghunathan (Stanford University)
* Yang Song (Stanford University)
* Riccardo Volpi (Italian Institute of Technology)
* Angus Galloway (University of Guelph)
* Yinpeng Dong (Tsinghua University)
* Willi Gierke (Hasso Plattner Institute)
* Bruno López
* Jonas Rauber (IMPRS)
* Paul Hendricks (NVIDIA)
* Ryan Sheatsley (Pennsylvania State University)
* Rujun Long (0101.AI)
* Bogdan Kulynych (EPFL)
* Erfan Noury (UMBC)
* Robert Wagner (Case Western Reserve University)

## Copyright

Copyright 2019 - Google Inc. OpenAI and Pennsylvania State University.
</file></file>"
"Abstract:  The Vision-and-Language Navigation (VLN) task entails an agent following
navigational instruction in photo-realistic unknown environments. This
challenging task demands that the agent be aware of which instruction was
completed which instruction is needed next which way to go and its
navigation progress towards the goal. In this paper we introduce a
self-monitoring agent with two complementary components: (1) visual-textual
co-grounding module to locate the instruction completed in the past the
instruction required for the next action and the next moving direction from
surrounding images and (2) progress monitor to ensure the grounded instruction
correctly reflects the navigation progress. We test our self-monitoring agent
on a standard benchmark and analyze our proposed approach through a series of
ablation studies that elucidate the contributions of the primary components.
Using our proposed method we set the new state of the art by a significant
margin (8% absolute increase in success rate on the unseen test set). Code is
available at this https URL .
## Self-Monitoring Navigation Agent for Vision-and-Language Navigation
<img src=""teaser/pytorch-logo-dark.png"" width=""10%""/> [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) 

This is the PyTorch implementation of our paper:
<img align=""right"" src=""teaser/einstein-scroll.png"" width=""5%""/>
<img align=""right"" src=""teaser/salesforce-research.jpg"" width=""12%""/>

**Self-Monitoring Navigation Agent via Auxiliary Progress Estimation**<br/>
[__***Chih-Yao Ma***__](https://chihyaoma.github.io/) [Jiasen Lu](https://www.cc.gatech.edu/~jlu347/) [Zuxuan Wu](http://zxwu.azurewebsites.net/) [Ghassan AlRegib](https://ghassanalregib.com/) [Zsolt Kira](https://www.cc.gatech.edu/~zk15/) 
[Richard Socher](https://www.socher.org/) [Caiming Xiong](http://www.stat.ucla.edu/~caiming/)<br/>
International Conference on Learning Representations (ICLR) 2019<br/>
**(Top 7% of reviews)**<br/>

[[arXiv](https://arxiv.org/abs/1901.03035)] [[GitHub](https://github.com/chihyaoma/selfmonitoring-agent)] [[Project](https://chihyaoma.github.io/project/2018/09/27/selfmonitoring.html)]
[[OpenReview](https://openreview.net/forum?id=r1GAsjC5Fm)]

<p align=""center"">
<img src=""teaser/selfmonitoring.png"" width=""100%""/>
</p>

## Follow-up work at CVPR 2019 (Oral)
Our follow-up work has been accepted at CVPR 2019 (Oral). 
Please check out here:

**The Regretful Agent: Heuristic-Aided Navigation through Progress Estimation**<br/>
[[arXiv](https://arxiv.org/abs/1903.01602)] [[GitHub](https://github.com/chihyaoma/regretful-agent)] [[Project](https://chihyaoma.github.io/project/2019/02/25/regretful.html)]

## Abstract
The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed which instruction is needed next which way to go and its navigation progress towards the goal. In this paper we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past the instruction required for the next action and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self- monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set).

<p align=""center"">
<img src=""teaser/selfmonitoring-demo.png"" width=""100%""/>
</p>

## Installation / Build Instructions

### Prerequisites

A C++ compiler with C++11 support is required. Matterport3D Simulator has several dependencies:
- [Ubuntu](https://www.ubuntu.com/) 14.04 16.04 18.04
- [OpenCV](http://opencv.org/) &gt;= 2.4 including 3.x 
- [OpenGL](https://www.opengl.org/)
- [OSMesa](https://www.mesa3d.org/osmesa.html)
- [GLM](https://glm.g-truc.net/0.9.8/index.html)
- [Numpy](http://www.numpy.org/)
- [pybind11](https://github.com/pybind/pybind11) for Python bindings
- [Doxygen](http://www.doxygen.org) for building documentation

E.g. installing dependencies on Ubuntu:
```
sudo apt-get install libopencv-dev python-opencv freeglut3 freeglut3-dev libglm-dev libjsoncpp-dev doxygen libosmesa6-dev libosmesa6 libglew-dev
```

For installing dependencies on MacOS please refer to [Installation on MacOS](install_macOS.md).

### Clone Repo
Clone the Self-Monitoring Agent repository:
```
# Make sure to clone with --recursive
git clone --recursive https://github.com/chihyaoma/selfmonitoring-agent.git
cd selfmonitoring-agent
```

If you didn't clone with the `--recursive` flag then you'll need to manually clone the pybind submodule from the top-level directory:
```
git submodule update --init --recursive
```

Note that our repository is based on the [Matterport3DSimulator](https://github.com/peteanderson80/Matterport3DSimulator) which was originally proposed with the Room-to-Roon dataset. 

### Directory Structure

- `connectivity`: Json navigation graphs.
- `img_features`: Storage for precomputed image features.
- `data`: You create a symlink to the Matterport3D dataset.
- `tasks`: Currently just the Room-to-Room (R2R) navigation task.

Other directories are mostly self-explanatory.

### Dataset Download

[Matterport3DSimulator](https://github.com/peteanderson80/Matterport3DSimulator) comes with both RGB images or precomputed ResNet image features.
For replicating our model performance you will ONLY need the precomputed image features.
You will however need the RGB images if you plan to visualize how the agent runs in the virtual environments.

#### RGB images from [Matterport3D Dataset](https://niessner.github.io/Matterport/)

Download the Matterport3D dataset which is available after requesting access [here](https://niessner.github.io/Matterport/). The provided download script allows for downloading of selected data types. Note that for the Matterport3D Simulator only the following data types are required (and can be selected with the download script):
- `matterport_skybox_images`

Create a symlink to the Matterport3D Dataset which should be structured as ```<matterdata>/v1/scans/<scanid>/matterport_skybox_images/*.jpg```:
```
ln -s <matterdata> data
```

Using symlinks will allow the same Matterport3D dataset installation to be used between multiple projects.

#### Precomputing ResNet Image Features from [Matterport3DSimulator](https://github.com/peteanderson80/Matterport3DSimulator)

To speed up model training times it is convenient to discretize heading and elevation into 30 degree increments and to precompute image features for each view.

We use the original precomputed image features as from [Matterport3DSimulator](https://github.com/peteanderson80/Matterport3DSimulator).
They provided image features with models pretrained from ImageNet and Places365.

Download and extract the tsv files into the `img_features` directory. You will only need the ImageNet features to replicate our results. We will uppack the zip file later.
- [ResNet-152-imagenet features [380K/2.9GB]](https://www.dropbox.com/s/715bbj8yjz32ekf/ResNet-152-imagenet.zip?dl=1)
- [ResNet-152-places365 features [380K/2.9GB]](https://www.dropbox.com/s/gox1rbdebyaa98w/ResNet-152-places365.zip?dl=1)

Empirically we found that using features from Places365 performs similar to the model using ImageNet features.

## Installation for R2R with PyTorch
Now that you have cloned the repo and download the image features needed. Let us unpack features and get things ready to run experiments.

### Create Anaconda enviorment
```bash
# change ""r2r"" to any name you prefer e.g. r2r-pytorch
conda create -n r2r python=3.6
```
Activate the enviorment you just created
```
source activate r2r
```
### Install special requirements for the R2R dataset
```
pip install -r tasks/R2R-pano/requirements.txt
```

### Install PyTorch for your Conda Env
Check the official [PyTorch website](http://pytorch.org/) for different CUDA version.
```
# with CUDA 10
conda install pytorch torchvision cuda100 -c pytorch

# MacOS without GPU
conda install pytorch torchvision -c pytorch
```

### Download R2R dataset
Download the original data from [MatterPort3DSimulator](https://github.com/peteanderson80/Matterport3DSimulator) and the synthetic data for data augmentation proposed by [Speaker-Follower](https://github.com/ronghanghu/speaker_follower) in NeurIPS 2018.
```bash
# download dataset
./tasks/R2R-pano/data/download.sh

# download the synthetic data from Speaker-Follower
./tasks/R2R-pano/data/download_precomputed_augmentation.sh

# if you haven't already download the precomputed image features otherwise skip this step
cd img_features
wget https://storage.googleapis.com/bringmeaspoon/img_features/ResNet-152-imagenet.zip

# unzip the file
unzip ResNet-152-imagenet.zip
cd ..
```


### Compile the Matterport3D Simulator
Let us compile the simulator so that we can call its functions in python.

Build OpenGL version using CMake:
```bash
mkdir build &amp;&amp; cd build
cmake ..

# Double-check if CMake find the proper path to your python
# if not remove the make files and use the cmake with option below instead
rm -rf *
cmake -DPYTHON_EXECUTABLE:FILEPATH=/path/to/your/bin/python ..

make
cd ../
```
Or build headless OSMESA version using CMake:
```
mkdir build &amp;&amp; cd build
cmake -DOSMESA_RENDERING=ON ..
make
cd ../
```

### Running Tests on simulator
Now that the compilation is completed. Let us make sure the installation of simulator is successful and can run smoothly.

```
build/tests
```
Or if you haven't installed the Matterport3D dataset you will need to skip the rendering tests:
```
build/tests exclude:[Rendering]
```
Refer to the [Catch](https://github.com/philsquared/Catch) documentation for additional usage and configuration options.

Minimum testing to see if the code can successfully run training.
```bash
python tasks/R2R-pano/main.py
```

Congradulations! You have completed the installation for the simulator and R2R dataset. You are now ready for training and reproducing our results.

### Training and reproduce results

#### Train on real data
To replicate the performance reported in our paper train proposed self-monitoring agent with:
```bash
# co-grounding + self-monitoring
CUDA_VISIBLE_DEVICES=0 python tasks/R2R-pano/main.py \
    --exp_name 'cogrounding-selfmonitoring-agent' \
    --batch_size 64 \
    --img_fc_use_angle 1 \
    --img_feat_input_dim 2176 \
    --img_fc_dim 1024 \
    --rnn_hidden_size 512 \
    --eval_every_epochs 5 \
    --use_ignore_index 1 \
    --arch 'self-monitoring' \
    --value_loss_weight 0.5 \
    --monitor_sigmoid 0 \
    --mse_sum 0 \
    --fix_action_ended 0
```

#### Train on synthetic data &amp; finetune on real data
Pre-train on synthetic data
```bash
# co-grounding + self-monitoring + pre-train on synthetic data
CUDA_VISIBLE_DEVICES=0 python tasks/R2R-pano/main.py \
    --exp_name 'cogrounding-selfmonitoring-agent' \
    --batch_size 64 \
    --img_fc_use_angle 1 \
    --img_feat_input_dim 2176 \
    --img_fc_dim 1024 \
    --rnn_hidden_size 512 \
    --eval_every_epochs 5 \
    --use_ignore_index 1 \
    --arch 'self-monitoring' \
    --value_loss_weight 0.5 \
    --monitor_sigmoid 0 \
    --mse_sum 0 \
    --fix_action_ended 0 \
    --train_data_augmentation 1 \
    --epochs_data_augmentation 300  # pre-train for 300 epochs
```

Once the training on synthetic data is completed we can now train on real data.
```bash
# co-grounding + self-monitoring + finetune on read data
CUDA_VISIBLE_DEVICES=0 python tasks/R2R-pano/main.py \
    --exp_name 'cogrounding-selfmonitoring-agent' \
    --batch_size 64 \
    --img_fc_use_angle 1 \
    --img_feat_input_dim 2176 \
    --img_fc_dim 1024 \
    --rnn_hidden_size 512 \
    --eval_every_epochs 5 \
    --use_ignore_index 1 \
    --arch 'self-monitoring' \
    --value_loss_weight 0.5 \
    --monitor_sigmoid 0 \
    --mse_sum 0 \
    --fix_action_ended 0 \
    --resume 'best' \  # resume from the best performing pre-trained model
    --max_num_epochs 500 \  # fine-tune until maximum 500 epochs
    --exp_name_secondary '_resume|best'
```

You can check the training process using TensorBoard.
```
cd tensorboard_logs/
tensorboard --logdir=pano-seq2seq
```

### Inference

#### Greedy decoding
The default inference model is set to be greedy decoding.

You should see the performance reproduced and match with numbers reported in our ablation study table.

#### Beam search
For fair comparison with [Speaker-Follower](https://arxiv.org/abs/1806.02724) we adopt beam search with the proposed self-monitoring agent.

Once training is completed we follow the name convention used above to resume the best performing model and use beam search during inference

```bash
CUDA_VISIBLE_DEVICES=0 python tasks/R2R-pano/main.py \
    --exp_name 'cogrounding-selfmonitoring-agent' \
    --batch_size 64 \
    --img_fc_use_angle 1 \
    --img_feat_input_dim 2176 \
    --img_fc_dim 1024 \
    --rnn_hidden_size 512 \
    --eval_every_epochs 5 \
    --use_ignore_index 1 \
    --arch 'self-monitoring' \
    --value_loss_weight 0.5 \
    --monitor_sigmoid 0 \
    --mse_sum 0 \
    --fix_action_ended 0 \
    --resume 'best' \  # resume from best performing model
    --eval_beam 1 \  # use beam search for evaluation
    --beam_size 15  # set beam size to 15
```

#### Progress inference
If the progress monitor output decreases the agent is required to move back to the previous viewpoint and select the action with next highest probability. 
We repeat this process until the selected action leads to increasing progress monitor output.

For convenience we implement this idea by taking advantage of the mini-batch processing. We precompute the progress monitor for a number of navigable directions but only selects the direction based on the order of action probabilites for these directions.
We make sure the agent does not *peek/sneak* into a direction that it has not yet visited before.

```bash
CUDA_VISIBLE_DEVICES=0 python tasks/R2R-pano/main.py \
    --exp_name 'cogrounding-selfmonitoring-agent' \
    --batch_size 64 \
    --img_fc_use_angle 1 \
    --img_feat_input_dim 2176 \
    --img_fc_dim 1024 \
    --rnn_hidden_size 512 \
    --eval_every_epochs 5 \
    --use_ignore_index 1 \
    --arch 'self-monitoring' \
    --value_loss_weight 0.5 \
    --monitor_sigmoid 0 \
    --mse_sum 0 \
    --fix_action_ended 0 \
    --resume 'best' \  # resume from best performing model
    --eval_only 1 \
    --progress_inference 1 \  # use progress inference for evaluation
    --beam_size 5  # this precomputes the progress monitor for 5 navigable directions
```

#### Reproducibility
Note that our results were originally produced using PyTorch 0.4 on a Titan Xp GPU. You may get slightly different results due to using different PyTorch versions different GPUs or different hyper-parameters.
The overall performance should be fairly robust even with different random seeds.
Please open an issue or contact [Chih-Yao Ma](https://chihyaoma.github.io) if you can not reproduce the results.

<p align=""center"">
<img src=""teaser/tb-training.png"" width=""100%""/>
</p>

## Acknowledgments
This research was partially supported by DARPAs Lifelong Learning Machines (L2M) program under Cooperative Agreement HR0011-18-2-001. We thank the authors from Speaker-Follower [arXiv](https://arxiv.org/abs/1806.02724) Ronghang Hu and Daniel Fried for communicating with us and providing details of the implementation and synthetic instructions for fair comparison.

## Citation
If you find this repository useful please cite our paper:

```
@inproceedings{ma2019selfmonitoring
    title={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation}
    author={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong}
    booktitle={Proceedings of the International Conference on Learning Representations (ICLR)}
    year={2019}
    url={https://arxiv.org/abs/1901.03035}
}
```</matterdata></scanid></matterdata>"
"Abstract:  Context. The 16 Myr old star 1SWASP J140747.93-394542.6 (V1400 Cen) underwent
a series of complex eclipses in May 2007 interpreted as the transit of a giant
Hill sphere filling debris ring system around a secondary companion J1407b. No
other eclipses have since been detected although other measurements have
constrained but not uniquely determined the orbital period of J1407b. Finding
another eclipse towards J1407 will help determine the orbital period of the
system the geometry of the proposed ring system and enable planning of further
observations to characterize the material within these putative rings. Aims. We
carry out a search for other eclipses in photometric data of J1407 with the aim
of constraining the orbital period of J1407b. Methods. We present photometry
from archival photographic plates from the Harvard DASCH survey and Bamberg
and Sonneberg Observatories in order to place additional constraints on the
orbital period of J1407b by searching for other dimming and eclipse events.
Using a visual inspection of all 387 plates and a period-folding algorithm we
performed a search for other eclipses in these data sets. Results. We find no
other deep eclipses in the data spanning from 1890 to 1990 nor in recent
time-series photometry from 2012-2018. Conclusions. We rule out a large
fraction of putative orbital periods for J1407b from 5 to 20 years. These
limits are still marginally consistent with a large Hill sphere filling ring
system surrounding a brown dwarf companion in a bound elliptical orbit about
J1407. Issues with the stability of any rings combined with the lack of
detection of another eclipse suggests that J1407b may not be bound to J1407.
# Excluding Periods
Algorithm to exclude possible orbital periods of the ringed substellar companion J1407b with long-baseline photometry.

This is a script to exclude possible orbital periods of the ringed companion of the the young star J1407 using photometry with a long baseline. A detailed description can be found in Mentel et. al. (2018 in preparation).

To obtain the exact results detailed in the aforementioned paper one would need to run the script with the complete data set. Obviously I could not upload the complete data set here. Instead I provided the publicly available AAVSO data on J1407 covering the star's photometry from 2012 to November of 2017 in order to show the principle modus operandi of the script. Running the .py-file will yield exemplary results of the PFA for test periods between 1 and 10 years."
"Abstract:  State representation learning aims at learning compact representations from
raw observations in robotics and control applications. Approaches used for this
objective are auto-encoders learning forward models inverse dynamics or
learning using generic priors on the state characteristics. However the
diversity in applications and methods makes the field lack standard evaluation
datasets metrics and tasks. This paper provides a set of environments data
generators robotic control tasks metrics and tools to facilitate iterative
state representation learning and evaluation in reinforcement learning
settings.
# S-RL Toolbox: Reinforcement Learning (RL) and State Representation Learning (SRL) Toolbox for Robotics

This repository was made to evaluate State Representation Learning methods using Reinforcement Learning. It integrates (automatic logging plotting saving loading of trained agent) various RL algorithms (PPO A2C ARS ACKTR DDPG DQN ACER CMA-ES SAC TRPO) along with different SRL methods (see [SRL Repo](https://github.com/araffin/srl-zoo)) in an efficient way (1 Million steps in 1 Hour with 8-core cpu and 1 Titan X GPU).

<img align=""right"" src=""imgs/SRL-RL-diag.svg"" width=""50%""/>

We also release customizable Gym environments for working with simulation (Kuka arm Mobile Robot in PyBullet running at 250 FPS on a 8-core machine) and real robots (Baxter Robot Robobo with ROS).

Related papers:
- ""Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics"" (Raffin et al. 2018) [https://arxiv.org/abs/1901.08651](https://arxiv.org/abs/1901.08651)
- ""S-RL Toolbox: Environments Datasets and Evaluation Metrics for State Representation Learning"" (Raffin et al. 2018) [https://arxiv.org/abs/1809.09369](https://arxiv.org/abs/1809.09369)

<a href=""https://youtu.be/qNsHMkIsqJc""><img src=""imgs/rl_toolboxplay.jpg""/></a>

## Main Features

- 10 RL algorithms ([Stable Baselines](https://github.com/hill-a/stable-baselines) included)
- logging / plotting / visdom integration / replay trained agent
- hyperparameter search (hyperband hyperopt)
- integration with State Representation Learning (SRL) methods (for feature extraction)
- visualisation tools (explore latent space display action proba live plot in the state space ...)
- robotics environments to compare SRL methods
- easy install using anaconda env or Docker images (CPU/GPU)

## Documentation

Documentation is available online: [https://s-rl-toolbox.readthedocs.io/](https://s-rl-toolbox.readthedocs.io/)

## Example

Here is a quick example of how to train a PPO2 agent on `MobileRobotGymEnv-v0` environment for 10 000 steps using 4 parallel processes:

```
python -m rl_baselines.train --algo ppo2 --no-vis --num-cpu 4 --num-timesteps 10000 --env MobileRobotGymEnv-v0
```


The complete command (logs will be saved in `logs/` folder):

```
python -m rl_baselines.train --algo rl_algo --env env1 --log-dir logs/ --srl-model raw_pixels --num-timesteps 10000 --no-vis
```

To use the robot's position as input instead of pixels just pass `--srl-model ground_truth` instead of `--srl-model raw_pixels`


## Installation

**Python 3 is required** (python 2 is not supported because of OpenAI baselines)

Note: we are using [Stable Baselines](https://github.com/hill-a/stable-baselines) a fork of OpenAI Baselines with unified interface and other improvements (e.g. tensorboard support).


### Using Anaconda

0. Download the project (note the `--recursive` argument because we are using git submodules):
```
git clone git@github.com:araffin/robotics-rl-srl.git --recursive
```

1. Install the swig library:
```
sudo apt-get install swig
```

2. Install the dependencies using `environment.yml` file (for anaconda users) in the current environment
```
conda env create --file environment.yml
source activate py35
```

[PyBullet Documentation](https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA)

### Using Docker

Please read the [documentation](https://s-rl-toolbox.readthedocs.io/) for more details.


## Reinforcement Learning

Several algorithms from [Stable Baselines](https://github.com/hill-a/stable-baselines) have been integrated along with some evolution strategies and SAC:

- A2C: A synchronous deterministic variant of Asynchronous Advantage Actor Critic (A3C).
- ACER: Sample Efficient Actor-Critic with Experience Replay
- ACKTR: Actor Critic using Kronecker-Factored Trust Region
- ARS: Augmented Random Search (https://arxiv.org/abs/1803.07055)
- CMA-ES: Covariance Matrix Adaptation Evolution Strategy
- DDPG: Deep Deterministic Policy Gradients
- DeepQ: DQN and variants (Double Dueling prioritized experience replay)
- PPO1: Proximal Policy Optimization (MPI Implementation)
- PPO2: Proximal Policy Optimization (GPU Implementation)
- SAC: Soft Actor Critic
- TRPO: Trust Region Policy Optimization (MPI Implementation)

Please read the [documentation](https://s-rl-toolbox.readthedocs.io/) for more details on how to train/load an agent on discrete/continuous actions and how to add your own rl algorithm.


### Hyperparameter Search

This repository also allows hyperparameter search using [hyperband](https://arxiv.org/abs/1603.06560) or [hyperopt](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf) for the implemented RL algorithms

for example here is the command for a hyperband search on PPO2 ground truth on the mobile robot environment:
```bash
python -m rl_baselines.hyperparam_search --optimizer hyperband --algo ppo2 --env MobileRobotGymEnv-v0 --srl-model ground_truth
```

## Environments

All the environments we propose follow the OpenAI Gym interface. We also extended this interface (adding extra methods) to work with SRL methods (see [State Representation Learning Models](#state-representation-learning-models)).

### Available Environments

| **Kuka environment**       | **Mobile Robot environment**       | **Racing car environment**       |
| -------------------------- | ---------------------------------- | -------------------------------- |
| <img src=""imgs/kuka.gif""/> | <img src=""imgs/mobile_robot.gif""/> | <img src=""imgs/racing_car.gif""/> |


| **Name**                          | **Action space (discrete)**                | **Action space (continuous)**                 | **Rewards**                                                                                                                                             | **ground truth**                                  |
| --------------------------------- | ------------------------------------------ | --------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- |
| **Kuka**<br/>**Button**            | 6 actions (3D cardinal direction)          | 3 axis (3D cardinal direction) <sup>(1)</sup> | 1 when target reached -1 when too far from target or when table is hit otherwise 0 <sup>(2)</sup> <sup>(3)</sup>                                                    | the XYZ position of the effector <sup>(4)</sup> |
| **Kuka**<br/>**RandButton**        | 6 actions (3D cardinal direction)          | 3 axis (3D cardinal direction) <sup>(1)</sup> | 1 when target reached -1 when too far from target or when table is hit otherwise 0 <sup>(2)</sup> <sup>(3)</sup>                                                    | the XYZ position of the effector <sup>(4)</sup> |
| **Kuka**<br/>**2Button**           | 6 actions (3D cardinal direction)          | 3 axis (3D cardinal direction) <sup>(1)</sup> | 1 when the first target is reached 1 when the second target is reached -1 when too far from target or when table is hit otherwise 0 <sup>(2)</sup> | the XYZ position of the effector <sup>(4)</sup> |
| **Kuka**<br/>**MovingButton**      | 6 actions (3D cardinal direction)          | 3 axis (3D cardinal direction) <sup>(1)</sup> | 1 when target reached -1 when too far from target or when table is hit otherwise 0 <sup>(2)</sup> <sup>(3)</sup>                                                    | the XYZ position of the effector <sup>(4)</sup> |
| **MobileRobot**<br/>               | 4 actions (2D cardinal direction)          | 2 axis (2D cardinal direction)                | 1 when target reached -1 for a wall hit otherwise 0 <sup>(2)</sup>                                                                                    | the XY position of the robot <sup>(4)</sup>      |
| **MobileRobot**<br/>**2Target**    | 4 actions (2D cardinal direction)          | 2 axis (2D cardinal direction)                | 1 when target reached -1 for a wall hit otherwise 0 <sup>(2)</sup>                                                                                    | the XY position of the robot <sup>(4)</sup>      |
| **MobileRobot**<br/>**1D**         | 2 actions (1D cardinal direction)          | 1 axis (1D cardinal direction)                | 1 when target reached -1 for a wall hit otherwise 0 <sup>(2)</sup>                                                                                    | the X position of the robot <sup>(4)</sup>        |
| **MobileRobot**<br/>**LineTarget** | 4 actions (2D cardinal direction)          | 2 axis (2D cardinal direction)                | 1 when target reached -1 for a wall hit otherwise 0 <sup>(2)</sup>                                                                                    | the XY position of the robot <sup>(4)</sup>      |
| **CarRacing**                     | 4 actions (left right accelerate brake) | 3 axis (stearing accelerate brake)          | -100 when out of bounds otherwise -0.1                                                                                                                 | the XY position of the car <sup>(4)</sup>        |

<sub><sup>1. The action space can use 6 axis arm joints control with the `--joints` flag</sup></sub><br/>
<sup><sup>2. The reward can be the euclidian distance to the target with the `--shape-reward` flag</sup></sup><br/>
<sup><sup>3. When using `--shape-reward` and ```--continuous``` the reward for hitting the button is 50 and for being out of bounds is -250. This is to prevent the agent hitting the table to stop the environment early and obtaining a higher reward</sup></sup><br/>
<sup><sup>4. The ground truth can be relative position from agent to the target by changing the `RELATIVE_POS` constant in the environment file</sup></sup>


the available environments are:
- Kuka arm: Here we have a Kuka arm which must reach a target here a button.
    - KukaButtonGymEnv-v0: Kuka arm with a single button in front.
    - KukaRandButtonGymEnv-v0: Kuka arm with a single button in front and some randomly positioned objects
    - Kuka2ButtonGymEnv-v0: Kuka arm with 2 buttons next to each others they must be pressed in the correct order (lighter button then darker button).
    - KukaMovingButtonGymEnv-v0: Kuka arm with a single button in front slowly moving left to right.
- Mobile robot: Here we have a mobile robot which reach a target position
    - MobileRobotGymEnv-v0: A mobile robot on a 2d terrain where it needs to reach a target position.
    - MobileRobot2TargetGymEnv-v0: A mobile robot on a 2d terrain where it needs to reach two target positions in the correct order (lighter target then darker target).
    - MobileRobot1DGymEnv-v0: A mobile robot on a 1d slider where it can only go up and down it must reach a target position.
    - MobileRobotLineTargetGymEnv-v0: A mobile robot on a 2d terrain where it needs to reach a colored band going across the terrain.
- Racing car: Here we have the interface for the Gym racing car environment. It must complete a racing course in the least time possible (only available in a terminal with X running)
    - CarRacingGymEnv-v0: A racing car on a racing course it must complete the racing course in the least time possible.
- Baxter: A baxter robot that must reach a target with its arms. (see [Working With Real Robots: Baxter and Robobo](https://s-rl-toolbox.readthedocs.io/en/latest/guide/real_robots.html))
    - Baxter-v0: A bridge to use a baxter robot with ROS (in simulation it uses Gazebo)
- Robobo: A Robobo robot that must reach a target position.
    - RoboboGymEnv-v0: A bridge to use a Robobo robot with ROS.

Please read the [documentation](https://s-rl-toolbox.readthedocs.io/) for more details (e.g. adding a custom environment).


## State Representation Learning Models

Please look the [SRL Repo](https://github.com/araffin/srl-zoo) to learn how to train a state representation model.
Then you must edit `config/srl_models.yaml` and set the right path to use the learned state representations.

The available state representation models are:
- ground_truth: Hand engineered features (e.g. robot position + target position for mobile robot env)
- raw_pixels: Learning a policy in an end-to-end manner directly from pixels to actions.
- autoencoder: an autoencoder from the raw pixels
- inverse: an inverse dynamics model
- forward: a forward dynamics model
- vae: a variational autoencoder from the raw pixels
- random: random features the feature extractor a convolutional network is fixed after random initialization.
- srl_combination: a model combining several losses (e.g. vae + forward + inverse...) for SRL
- supervised: A model trained with Ground Truth states as targets in a supervised setting.
- robotic_priors: Robotic Priors model
- pca: pca applied to the raw pixels
- multi_view_srl: a SRL model using views from multiple cameras as input with any of the above losses (e.g triplet and others)
- joints: the arm's joints angles (only for Kuka environments)
- joints_position: the arm's xyz position and joints angles (only for Kuka environments)

Please read the [documentation](https://s-rl-toolbox.readthedocs.io/) for more details (e.g. adding a custom SRL model).


## Troubleshooting
If a submodule is not downloaded:
```
git submodule update --init
```
If you have troubles installing mpi4py make sure you the following installed:
```
sudo apt-get install libopenmpi-dev openmpi-bin openmpi-doc
```

## Known issues

The inverse kinematics function has trouble finding a solution when the arm is fully straight and the arm must bend to reach the requested point.

## Acknowledgements

This work is supported by the [DREAM project](http://www.robotsthatdream.eu) through the European Union Horizon 2020 FET research and innovation program under grant agreement No 640891.

## Citation

If you use this toolbox please cite:

```
@article{Raffin18
  title={S-RL Toolbox: Environments Datasets and Evaluation Metrics for State Representation Learning}
  author={Raffin Antonin and Hill Ashley and Traor{\'e} Ren{\'e} and Lesort Timoth{\'e}e and D{\'\i}az-Rodr{\'\i}guez Natalia and Filliat David}
  journal={arXiv preprint arXiv:1809.09369}
  year={2018}
}
```"
"Abstract:  We consider the task of fine-grained sentiment analysis from the perspective
of multiple instance learning (MIL). Our neural model is trained on document
sentiment labels and learns to predict the sentiment of text segments i.e.
sentences or elementary discourse units (EDUs) without segment-level
supervision. We introduce an attention-based polarity scoring method for
identifying positive and negative text snippets and a new dataset which we call
SPOT (as shorthand for Segment-level POlariTy annotations) for evaluating
MIL-style sentiment models like ours. Experimental results demonstrate superior
performance against multiple baselines whereas a judgement elicitation study
shows that EDU-level opinion extraction produces more informative summaries
than sentence-based alternatives.
# S-RL Toolbox: Reinforcement Learning (RL) and State Representation Learning (SRL) Toolbox for Robotics

This repository was made to evaluate State Representation Learning methods using Reinforcement Learning. It integrates (automatic logging plotting saving loading of trained agent) various RL algorithms (PPO A2C ARS ACKTR DDPG DQN ACER CMA-ES SAC TRPO) along with different SRL methods (see [SRL Repo](https://github.com/araffin/srl-zoo)) in an efficient way (1 Million steps in 1 Hour with 8-core cpu and 1 Titan X GPU).

<img align=""right"" src=""imgs/SRL-RL-diag.svg"" width=""50%""/>

We also release customizable Gym environments for working with simulation (Kuka arm Mobile Robot in PyBullet running at 250 FPS on a 8-core machine) and real robots (Baxter Robot Robobo with ROS).

Related papers:
- ""Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics"" (Raffin et al. 2018) [https://arxiv.org/abs/1901.08651](https://arxiv.org/abs/1901.08651)
- ""S-RL Toolbox: Environments Datasets and Evaluation Metrics for State Representation Learning"" (Raffin et al. 2018) [https://arxiv.org/abs/1809.09369](https://arxiv.org/abs/1809.09369)

<a href=""https://youtu.be/qNsHMkIsqJc""><img src=""imgs/rl_toolboxplay.jpg""/></a>

## Main Features

- 10 RL algorithms ([Stable Baselines](https://github.com/hill-a/stable-baselines) included)
- logging / plotting / visdom integration / replay trained agent
- hyperparameter search (hyperband hyperopt)
- integration with State Representation Learning (SRL) methods (for feature extraction)
- visualisation tools (explore latent space display action proba live plot in the state space ...)
- robotics environments to compare SRL methods
- easy install using anaconda env or Docker images (CPU/GPU)

## Documentation

Documentation is available online: [https://s-rl-toolbox.readthedocs.io/](https://s-rl-toolbox.readthedocs.io/)

## Example

Here is a quick example of how to train a PPO2 agent on `MobileRobotGymEnv-v0` environment for 10 000 steps using 4 parallel processes:

```
python -m rl_baselines.train --algo ppo2 --no-vis --num-cpu 4 --num-timesteps 10000 --env MobileRobotGymEnv-v0
```


The complete command (logs will be saved in `logs/` folder):

```
python -m rl_baselines.train --algo rl_algo --env env1 --log-dir logs/ --srl-model raw_pixels --num-timesteps 10000 --no-vis
```

To use the robot's position as input instead of pixels just pass `--srl-model ground_truth` instead of `--srl-model raw_pixels`


## Installation

**Python 3 is required** (python 2 is not supported because of OpenAI baselines)

Note: we are using [Stable Baselines](https://github.com/hill-a/stable-baselines) a fork of OpenAI Baselines with unified interface and other improvements (e.g. tensorboard support).


### Using Anaconda

0. Download the project (note the `--recursive` argument because we are using git submodules):
```
git clone git@github.com:araffin/robotics-rl-srl.git --recursive
```

1. Install the swig library:
```
sudo apt-get install swig
```

2. Install the dependencies using `environment.yml` file (for anaconda users) in the current environment
```
conda env create --file environment.yml
source activate py35
```

[PyBullet Documentation](https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA)

### Using Docker

Please read the [documentation](https://s-rl-toolbox.readthedocs.io/) for more details.


## Reinforcement Learning

Several algorithms from [Stable Baselines](https://github.com/hill-a/stable-baselines) have been integrated along with some evolution strategies and SAC:

- A2C: A synchronous deterministic variant of Asynchronous Advantage Actor Critic (A3C).
- ACER: Sample Efficient Actor-Critic with Experience Replay
- ACKTR: Actor Critic using Kronecker-Factored Trust Region
- ARS: Augmented Random Search (https://arxiv.org/abs/1803.07055)
- CMA-ES: Covariance Matrix Adaptation Evolution Strategy
- DDPG: Deep Deterministic Policy Gradients
- DeepQ: DQN and variants (Double Dueling prioritized experience replay)
- PPO1: Proximal Policy Optimization (MPI Implementation)
- PPO2: Proximal Policy Optimization (GPU Implementation)
- SAC: Soft Actor Critic
- TRPO: Trust Region Policy Optimization (MPI Implementation)

Please read the [documentation](https://s-rl-toolbox.readthedocs.io/) for more details on how to train/load an agent on discrete/continuous actions and how to add your own rl algorithm.


### Hyperparameter Search

This repository also allows hyperparameter search using [hyperband](https://arxiv.org/abs/1603.06560) or [hyperopt](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf) for the implemented RL algorithms

for example here is the command for a hyperband search on PPO2 ground truth on the mobile robot environment:
```bash
python -m rl_baselines.hyperparam_search --optimizer hyperband --algo ppo2 --env MobileRobotGymEnv-v0 --srl-model ground_truth
```

## Environments

All the environments we propose follow the OpenAI Gym interface. We also extended this interface (adding extra methods) to work with SRL methods (see [State Representation Learning Models](#state-representation-learning-models)).

### Available Environments

| **Kuka environment**       | **Mobile Robot environment**       | **Racing car environment**       |
| -------------------------- | ---------------------------------- | -------------------------------- |
| <img src=""imgs/kuka.gif""/> | <img src=""imgs/mobile_robot.gif""/> | <img src=""imgs/racing_car.gif""/> |


| **Name**                          | **Action space (discrete)**                | **Action space (continuous)**                 | **Rewards**                                                                                                                                             | **ground truth**                                  |
| --------------------------------- | ------------------------------------------ | --------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- |
| **Kuka**<br/>**Button**            | 6 actions (3D cardinal direction)          | 3 axis (3D cardinal direction) <sup>(1)</sup> | 1 when target reached -1 when too far from target or when table is hit otherwise 0 <sup>(2)</sup> <sup>(3)</sup>                                                    | the XYZ position of the effector <sup>(4)</sup> |
| **Kuka**<br/>**RandButton**        | 6 actions (3D cardinal direction)          | 3 axis (3D cardinal direction) <sup>(1)</sup> | 1 when target reached -1 when too far from target or when table is hit otherwise 0 <sup>(2)</sup> <sup>(3)</sup>                                                    | the XYZ position of the effector <sup>(4)</sup> |
| **Kuka**<br/>**2Button**           | 6 actions (3D cardinal direction)          | 3 axis (3D cardinal direction) <sup>(1)</sup> | 1 when the first target is reached 1 when the second target is reached -1 when too far from target or when table is hit otherwise 0 <sup>(2)</sup> | the XYZ position of the effector <sup>(4)</sup> |
| **Kuka**<br/>**MovingButton**      | 6 actions (3D cardinal direction)          | 3 axis (3D cardinal direction) <sup>(1)</sup> | 1 when target reached -1 when too far from target or when table is hit otherwise 0 <sup>(2)</sup> <sup>(3)</sup>                                                    | the XYZ position of the effector <sup>(4)</sup> |
| **MobileRobot**<br/>               | 4 actions (2D cardinal direction)          | 2 axis (2D cardinal direction)                | 1 when target reached -1 for a wall hit otherwise 0 <sup>(2)</sup>                                                                                    | the XY position of the robot <sup>(4)</sup>      |
| **MobileRobot**<br/>**2Target**    | 4 actions (2D cardinal direction)          | 2 axis (2D cardinal direction)                | 1 when target reached -1 for a wall hit otherwise 0 <sup>(2)</sup>                                                                                    | the XY position of the robot <sup>(4)</sup>      |
| **MobileRobot**<br/>**1D**         | 2 actions (1D cardinal direction)          | 1 axis (1D cardinal direction)                | 1 when target reached -1 for a wall hit otherwise 0 <sup>(2)</sup>                                                                                    | the X position of the robot <sup>(4)</sup>        |
| **MobileRobot**<br/>**LineTarget** | 4 actions (2D cardinal direction)          | 2 axis (2D cardinal direction)                | 1 when target reached -1 for a wall hit otherwise 0 <sup>(2)</sup>                                                                                    | the XY position of the robot <sup>(4)</sup>      |
| **CarRacing**                     | 4 actions (left right accelerate brake) | 3 axis (stearing accelerate brake)          | -100 when out of bounds otherwise -0.1                                                                                                                 | the XY position of the car <sup>(4)</sup>        |

<sub><sup>1. The action space can use 6 axis arm joints control with the `--joints` flag</sup></sub><br/>
<sup><sup>2. The reward can be the euclidian distance to the target with the `--shape-reward` flag</sup></sup><br/>
<sup><sup>3. When using `--shape-reward` and ```--continuous``` the reward for hitting the button is 50 and for being out of bounds is -250. This is to prevent the agent hitting the table to stop the environment early and obtaining a higher reward</sup></sup><br/>
<sup><sup>4. The ground truth can be relative position from agent to the target by changing the `RELATIVE_POS` constant in the environment file</sup></sup>


the available environments are:
- Kuka arm: Here we have a Kuka arm which must reach a target here a button.
    - KukaButtonGymEnv-v0: Kuka arm with a single button in front.
    - KukaRandButtonGymEnv-v0: Kuka arm with a single button in front and some randomly positioned objects
    - Kuka2ButtonGymEnv-v0: Kuka arm with 2 buttons next to each others they must be pressed in the correct order (lighter button then darker button).
    - KukaMovingButtonGymEnv-v0: Kuka arm with a single button in front slowly moving left to right.
- Mobile robot: Here we have a mobile robot which reach a target position
    - MobileRobotGymEnv-v0: A mobile robot on a 2d terrain where it needs to reach a target position.
    - MobileRobot2TargetGymEnv-v0: A mobile robot on a 2d terrain where it needs to reach two target positions in the correct order (lighter target then darker target).
    - MobileRobot1DGymEnv-v0: A mobile robot on a 1d slider where it can only go up and down it must reach a target position.
    - MobileRobotLineTargetGymEnv-v0: A mobile robot on a 2d terrain where it needs to reach a colored band going across the terrain.
- Racing car: Here we have the interface for the Gym racing car environment. It must complete a racing course in the least time possible (only available in a terminal with X running)
    - CarRacingGymEnv-v0: A racing car on a racing course it must complete the racing course in the least time possible.
- Baxter: A baxter robot that must reach a target with its arms. (see [Working With Real Robots: Baxter and Robobo](https://s-rl-toolbox.readthedocs.io/en/latest/guide/real_robots.html))
    - Baxter-v0: A bridge to use a baxter robot with ROS (in simulation it uses Gazebo)
- Robobo: A Robobo robot that must reach a target position.
    - RoboboGymEnv-v0: A bridge to use a Robobo robot with ROS.

Please read the [documentation](https://s-rl-toolbox.readthedocs.io/) for more details (e.g. adding a custom environment).


## State Representation Learning Models

Please look the [SRL Repo](https://github.com/araffin/srl-zoo) to learn how to train a state representation model.
Then you must edit `config/srl_models.yaml` and set the right path to use the learned state representations.

The available state representation models are:
- ground_truth: Hand engineered features (e.g. robot position + target position for mobile robot env)
- raw_pixels: Learning a policy in an end-to-end manner directly from pixels to actions.
- autoencoder: an autoencoder from the raw pixels
- inverse: an inverse dynamics model
- forward: a forward dynamics model
- vae: a variational autoencoder from the raw pixels
- random: random features the feature extractor a convolutional network is fixed after random initialization.
- srl_combination: a model combining several losses (e.g. vae + forward + inverse...) for SRL
- supervised: A model trained with Ground Truth states as targets in a supervised setting.
- robotic_priors: Robotic Priors model
- pca: pca applied to the raw pixels
- multi_view_srl: a SRL model using views from multiple cameras as input with any of the above losses (e.g triplet and others)
- joints: the arm's joints angles (only for Kuka environments)
- joints_position: the arm's xyz position and joints angles (only for Kuka environments)

Please read the [documentation](https://s-rl-toolbox.readthedocs.io/) for more details (e.g. adding a custom SRL model).


## Troubleshooting
If a submodule is not downloaded:
```
git submodule update --init
```
If you have troubles installing mpi4py make sure you the following installed:
```
sudo apt-get install libopenmpi-dev openmpi-bin openmpi-doc
```

## Known issues

The inverse kinematics function has trouble finding a solution when the arm is fully straight and the arm must bend to reach the requested point.

## Acknowledgements

This work is supported by the [DREAM project](http://www.robotsthatdream.eu) through the European Union Horizon 2020 FET research and innovation program under grant agreement No 640891.

## Citation

If you use this toolbox please cite:

```
@article{Raffin18
  title={S-RL Toolbox: Environments Datasets and Evaluation Metrics for State Representation Learning}
  author={Raffin Antonin and Hill Ashley and Traor{\'e} Ren{\'e} and Lesort Timoth{\'e}e and D{\'\i}az-Rodr{\'\i}guez Natalia and Filliat David}
  journal={arXiv preprint arXiv:1809.09369}
  year={2018}
}
```"
"Abstract:  Recent studies have shown remarkable success in image-to-image translation
for two domains. However existing approaches have limited scalability and
robustness in handling more than two domains since different models should be
built independently for every pair of image domains. To address this
limitation we propose StarGAN a novel and scalable approach that can perform
image-to-image translations for multiple domains using only a single model.
Such a unified model architecture of StarGAN allows simultaneous training of
multiple datasets with different domains within a single network. This leads to
StarGAN's superior quality of translated images compared to existing models as
well as the novel capability of flexibly translating an input image to any
desired target domain. We empirically demonstrate the effectiveness of our
approach on a facial attribute transfer and a facial expression synthesis
tasks.
<p align=""center""><img src=""jpg/logo.jpg"" width=""40%""/></p>

--------------------------------------------------------------------------------
This repository provides a PyTorch implementation of [StarGAN](https://arxiv.org/abs/1711.09020). StarGAN can flexibly translate an input image to any desired target domain using only a single generator and a discriminator. The demo video for StarGAN can be found [here](https://www.youtube.com/watch?v=EYjdLppmERE).

<p align=""center""><img src=""jpg/main.jpg"" width=""100%""/></p>
<br/>

## Paper
[StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](https://arxiv.org/abs/1711.09020) <br/>
[Yunjey Choi](https://github.com/yunjey)<sup> 12</sup> [Minje Choi](https://github.com/mjc92)<sup> 12</sup> [Munyoung Kim](https://www.facebook.com/munyoung.kim.1291)<sup> 23</sup> [Jung-Woo Ha](https://www.facebook.com/jungwoo.ha.921)<sup> 2</sup> [Sung Kim](https://www.cse.ust.hk/~hunkim/)<sup> 24</sup> and [Jaegul Choo](https://sites.google.com/site/jaegulchoo/)<sup> 12</sup>    <br/>
<sup>1 </sup>Korea University <sup>2 </sup>Clova AI Research (NAVER Corp.) <sup>3 </sup>The College of New Jersey <sup> 4 </sup>HKUST  <br/>
IEEE Conference on Computer Vision and Pattern Recognition ([CVPR](http://cvpr2018.thecvf.com/)) 2018 (<b>Oral</b>) 

<br/>

## Dependencies
* [Python 3.5+](https://www.continuum.io/downloads)
* [PyTorch 0.4.0+](http://pytorch.org/)
* [TensorFlow 1.3+](https://www.tensorflow.org/) (optional for tensorboard)


<br/>

## Usage

### 1. Cloning the repository
```bash
$ git clone https://github.com/yunjey/StarGAN.git
$ cd StarGAN/
```

### 2. Downloading the dataset
To download the CelebA dataset:
```bash
$ bash download.sh celeba
```

To download the RaFD dataset you must request access to the dataset from [the Radboud Faces Database website](http://www.socsci.ru.nl:8180/RaFD2/RaFD?p=main). Then you need to create a folder structure as described [here](https://github.com/yunjey/StarGAN/blob/master/jpg/RaFD.md).

### 3. Training
To train StarGAN on CelebA run the training script below. See [here](https://github.com/yunjey/StarGAN/blob/master/jpg/CelebA.md) for a list of selectable attributes in the CelebA dataset. If you change the `selected_attrs` argument you should also change the `c_dim` argument accordingly.

```bash
$ python main.py --mode train --dataset CelebA --image_size 128 --c_dim 5 \
                 --sample_dir stargan_celeba/samples --log_dir stargan_celeba/logs \
                 --model_save_dir stargan_celeba/models --result_dir stargan_celeba/results \
                 --selected_attrs Black_Hair Blond_Hair Brown_Hair Male Young
```

To train StarGAN on RaFD:

```bash
$ python main.py --mode train --dataset RaFD --image_size 128 --c_dim 8 \
                 --sample_dir stargan_rafd/samples --log_dir stargan_rafd/logs \
                 --model_save_dir stargan_rafd/models --result_dir stargan_rafd/results
```

To train StarGAN on both CelebA and RafD:

```bash
$ python main.py --mode=train --dataset Both --image_size 256 --c_dim 5 --c2_dim 8 \
                 --sample_dir stargan_both/samples --log_dir stargan_both/logs \
                 --model_save_dir stargan_both/models --result_dir stargan_both/results
```

To train StarGAN on your own dataset create a folder structure in the same format as [RaFD](https://github.com/yunjey/StarGAN/blob/master/jpg/RaFD.md) and run the command:

```bash
$ python main.py --mode train --dataset RaFD --rafd_crop_size CROP_SIZE --image_size IMG_SIZE \
                 --c_dim LABEL_DIM --rafd_image_dir TRAIN_IMG_DIR \
                 --sample_dir stargan_custom/samples --log_dir stargan_custom/logs \
                 --model_save_dir stargan_custom/models --result_dir stargan_custom/results
```


### 4. Testing

To test StarGAN on CelebA:

```bash
$ python main.py --mode test --dataset CelebA --image_size 128 --c_dim 5 \
                 --sample_dir stargan_celeba/samples --log_dir stargan_celeba/logs \
                 --model_save_dir stargan_celeba/models --result_dir stargan_celeba/results \
                 --selected_attrs Black_Hair Blond_Hair Brown_Hair Male Young
```

To test StarGAN on RaFD:

```bash
$ python main.py --mode test --dataset RaFD --image_size 128 \
                 --c_dim 8 --rafd_image_dir data/RaFD/test \
                 --sample_dir stargan_rafd/samples --log_dir stargan_rafd/logs \
                 --model_save_dir stargan_rafd/models --result_dir stargan_rafd/results
```

To test StarGAN on both CelebA and RaFD:

```bash
$ python main.py --mode test --dataset Both --image_size 256 --c_dim 5 --c2_dim 8 \
                 --sample_dir stargan_both/samples --log_dir stargan_both/logs \
                 --model_save_dir stargan_both/models --result_dir stargan_both/results
```

To test StarGAN on your own dataset:

```bash
$ python main.py --mode test --dataset RaFD --rafd_crop_size CROP_SIZE --image_size IMG_SIZE \
                 --c_dim LABEL_DIM --rafd_image_dir TEST_IMG_DIR \
                 --sample_dir stargan_custom/samples --log_dir stargan_custom/logs \
                 --model_save_dir stargan_custom/models --result_dir stargan_custom/results
```
### 5. Pretrained model
To download a pretrained model checkpoint run the script below. The pretrained model checkpoint will be downloaded and saved into `./stargan_celeba_256/models` directory.

```bash
$ bash download.sh pretrained-celeba-256x256
```

To translate images using the pretrained model run the evaluation script below. The translated images will be saved into `./stargan_celeba_256/results` directory.

```bash
$ python main.py --mode test --dataset CelebA --image_size 256 --c_dim 5 \
                 --selected_attrs Black_Hair Blond_Hair Brown_Hair Male Young \
                 --model_save_dir='stargan_celeba_256/models' \
                 --result_dir='stargan_celeba_256/results'
```

<br/>

## Results

### 1. Facial Attribute Transfer on CelebA
<p align=""center""><img src=""jpg/result_celeba1.jpg"" width=""100%""/></p>

### 2. Facial Expression Synthesis on RaFD
<p align=""center""><img src=""jpg/result_rafd.jpg"" width=""100%""/></p>

### 3. Facial Expression Synthesis on CelebA
<p align=""center""><img src=""jpg/result_celeba2.jpg"" width=""100%""/></p>
<br/>

## Citation
If this work is useful for your research please cite our [paper](https://arxiv.org/abs/1711.09020):
```
@InProceedings{StarGAN2018
author = {Choi Yunjey and Choi Minje and Kim Munyoung and Ha Jung-Woo and Kim Sunghun and Choo Jaegul}
title = {StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation}
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
month = {June}
year = {2018}
}
```

<br/>

## Acknowledgement
This work was mainly done while the first author did a research internship at [Clova AI Research NAVER](https://clova.ai/en/research/research-area-detail.html?id=0). We thank all the researchers at NAVER especially Donghyun Kwak for insightful discussions."
"Abstract:  The DeepMind Control Suite is a set of continuous control tasks with a
standardised structure and interpretable rewards intended to serve as
performance benchmarks for reinforcement learning agents. The tasks are written
in Python and powered by the MuJoCo physics engine making them easy to use and
modify. We include benchmarks for several learning algorithms. The Control
Suite is publicly available at this https URL . A
video summary of all tasks is available at this http URL .
# segmentation

TensorFlow implementation of ENet (https://arxiv.org/pdf/1606.02147.pdf) based on the official Torch implementation (https://github.com/e-lab/ENet-training) and the Keras implementation by PavlosMelissinos (https://github.com/PavlosMelissinos/enet-keras) trained on the Cityscapes dataset (https://www.cityscapes-dataset.com/).

- Youtube video of results (https://youtu.be/HbPhvct5kvs):
- [![demo video with results](https://img.youtube.com/vi/HbPhvct5kvs/0.jpg)](https://youtu.be/HbPhvct5kvs)

- The results in the video can obviously be improved but because of limited computing resources (personally funded Azure VM) I did not perform any further hyperparameter tuning. 

****
You might get the error ""No gradient defined for operation 'MaxPoolWithArgmax_1' (op type: MaxPoolWithArgmax)"". To fix this I had to add the following code to the file /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.py:  
```
@ops.RegisterGradient(""MaxPoolWithArgmax"")  
def _MaxPoolGradWithArgmax(op grad unused_argmax_grad):  
  return gen_nn_ops._max_pool_grad_with_argmax(op.inputs[0] grad op.outputs[1] op.get_attr(""ksize"") op.get_attr(""strides"") padding=op.get_attr(""padding""))  
```

****
## Documentation:

preprocess_data.py:  
- ASSUMES: that all Cityscapes training (validation) image directories have been placed in data_dir/cityscapes/leftImg8bit/train (data_dir/cityscapes/leftImg8bit/val) and that all corresponding ground truth directories have been placed in data_dir/cityscapes/gtFine/train (data_dir/cityscapes/gtFine/val).
- DOES: script for performing all necessary preprocessing of images and labels.
*****

model.py:  
- ASSUMES: that preprocess_data.py has already been run.
- DOES: contains the ENet_model class.
*****

utilities.py:  
- ASSUMES: -
- DOES: contains a number of functions used in different parts of the project.
*****

train.py:  
- ASSUMES: that preprocess_data.py has already been run.
- DOES: script for training the model.
*****

run_on_sequence.py:  
- ASSUMES: that preprocess_data.py has already been run.
- DOES: runs a model checkpoint (set in line 56) on all frames in a Cityscapes demo sequence directory (set in line 30) and creates a video of the result.

****
## Training details:

- In the paper the authors suggest that you first pretrain the encoder to categorize downsampled regions of the input images I did however train the entire network from scratch.
- Batch size: 4.
- For all other hyperparameters I used the same values as in the paper.

- Training loss:
- ![training loss](https://raw.githubusercontent.com/fregu856/segmentation/master/training_logs/model_1/train_loss_per_epoch.png)

- Validation loss:
- ![validation loss](https://raw.githubusercontent.com/fregu856/segmentation/master/training_logs/model_1/val_loss_per_epoch.png)

- The results in the video above was obtained with the model at epoch 23 for which a checkpoint is included in segmentation/training_logs/best_model in the repo.

******
## Training on Microsoft Azure:

To train the model I used an NC6 virtual machine on Microsoft Azure. Below I have listed what I needed to do in order to get started and some things I found useful. For reference my username was 'fregu856':
- Download Cityscapes.

- Install docker-ce:
- - $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
- - $ sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable""
- - $ sudo apt-get update
- - $ sudo apt-get install -y docker-ce

- Install CUDA drivers (see ""Install CUDA drivers for NC VMs"" in https://docs.microsoft.com/en-us/azure/virtual-machines/linux/n-series-driver-setup):
- - $ CUDA_REPO_PKG=cuda-repo-ubuntu1604_8.0.61-1_amd64.deb
- - $ wget -O /tmp/${CUDA_REPO_PKG} http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/${CUDA_REPO_PKG} 
- - $ sudo dpkg -i /tmp/${CUDA_REPO_PKG}
- - $ rm -f /tmp/${CUDA_REPO_PKG}
- - $ sudo apt-get update
- - $ sudo apt-get install cuda-drivers
- - Reboot the VM

- Install nvidia-docker:
- - $ wget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker_1.0.1-1_amd64.deb
- - $ sudo dpkg -i /tmp/nvidia-docker*.deb &amp;&amp; rm /tmp/nvidia-docker*.deb
- - $ sudo nvidia-docker run --rm nvidia/cuda nvidia-smi

- Download the latest TensorFlow docker image with GPU support (tensorflow 1.3):
- - $ sudo docker pull tensorflow/tensorflow:latest-gpu

- Create start_docker_image.sh containing:
```
#!/bin/bash

# DEFAULT VALUES
GPUIDS=""0""
NAME=""fregu856_GPU""


NV_GPU=""$GPUIDS"" nvidia-docker run -it --rm \
        -p 5584:5584 \
        --name ""$NAME""""$GPUIDS"" \
        -v /home/fregu856:/root/ \
        tensorflow/tensorflow:latest-gpu bash
```

- /root/ will now be mapped to /home/fregu856 (i.e. $ cd -- takes you to the regular home folder). 

- To start the image:
- - $ sudo sh start_docker_image.sh 
- To commit changes to the image:
- - Open a new terminal window.
- - $ sudo docker commit fregu856_GPU0 tensorflow/tensorflow:latest-gpu
- To stop the image when itâ€™s running:
- - $ sudo docker stop fregu856_GPU0
- To exit the image without killing running code:
- - Ctrl-P + Q
- To get back into a running image:
- - $ sudo docker attach fregu856_GPU0
- To open more than one terminal window at the same time:
- - $ sudo docker exec -it fregu856_GPU0 bash

- To install the needed software inside the docker image:
- - $ apt-get update
- - $ apt-get install nano
- - $ apt-get install sudo
- - $ apt-get install wget
- - $ sudo apt-get install libopencv-dev python-opencv
- - Commit changes to the image (otherwise the installed packages will be removed at exit!)"
"Abstract:  Ransomware can prevent a user from accessing a device and its files until a
ransom is paid to the attacker most frequently in Bitcoin. With over 500 known
ransomware families it has become one of the dominant cybercrime threats for
law enforcement security professionals and the public. However a more
comprehensive evidence-based picture on the global direct financial impact of
ransomware attacks is still missing. In this paper we present a data-driven
method for identifying and gathering information on Bitcoin transactions
related to illicit activity based on footprints left on the public Bitcoin
blockchain. We implement this method on-top-of the GraphSense open-source
platform and apply it to empirically analyze transactions related to 35
ransomware families. We estimate the lower bound direct financial impact of
each ransomware family and find that from 2013 to mid-2017 the market for
ransomware payments has a minimum worth of USD 12768536 (22967.54 BTC). We
also find that the market is highly skewed with only a few number of players
responsible for the majority of the payments. Based on these research findings
policy-makers and law enforcement agencies can use the statistics provided to
understand the size of the illicit market and make informed decisions on how
best to address the threat.
# segmentation

TensorFlow implementation of ENet (https://arxiv.org/pdf/1606.02147.pdf) based on the official Torch implementation (https://github.com/e-lab/ENet-training) and the Keras implementation by PavlosMelissinos (https://github.com/PavlosMelissinos/enet-keras) trained on the Cityscapes dataset (https://www.cityscapes-dataset.com/).

- Youtube video of results (https://youtu.be/HbPhvct5kvs):
- [![demo video with results](https://img.youtube.com/vi/HbPhvct5kvs/0.jpg)](https://youtu.be/HbPhvct5kvs)

- The results in the video can obviously be improved but because of limited computing resources (personally funded Azure VM) I did not perform any further hyperparameter tuning. 

****
You might get the error ""No gradient defined for operation 'MaxPoolWithArgmax_1' (op type: MaxPoolWithArgmax)"". To fix this I had to add the following code to the file /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.py:  
```
@ops.RegisterGradient(""MaxPoolWithArgmax"")  
def _MaxPoolGradWithArgmax(op grad unused_argmax_grad):  
  return gen_nn_ops._max_pool_grad_with_argmax(op.inputs[0] grad op.outputs[1] op.get_attr(""ksize"") op.get_attr(""strides"") padding=op.get_attr(""padding""))  
```

****
## Documentation:

preprocess_data.py:  
- ASSUMES: that all Cityscapes training (validation) image directories have been placed in data_dir/cityscapes/leftImg8bit/train (data_dir/cityscapes/leftImg8bit/val) and that all corresponding ground truth directories have been placed in data_dir/cityscapes/gtFine/train (data_dir/cityscapes/gtFine/val).
- DOES: script for performing all necessary preprocessing of images and labels.
*****

model.py:  
- ASSUMES: that preprocess_data.py has already been run.
- DOES: contains the ENet_model class.
*****

utilities.py:  
- ASSUMES: -
- DOES: contains a number of functions used in different parts of the project.
*****

train.py:  
- ASSUMES: that preprocess_data.py has already been run.
- DOES: script for training the model.
*****

run_on_sequence.py:  
- ASSUMES: that preprocess_data.py has already been run.
- DOES: runs a model checkpoint (set in line 56) on all frames in a Cityscapes demo sequence directory (set in line 30) and creates a video of the result.

****
## Training details:

- In the paper the authors suggest that you first pretrain the encoder to categorize downsampled regions of the input images I did however train the entire network from scratch.
- Batch size: 4.
- For all other hyperparameters I used the same values as in the paper.

- Training loss:
- ![training loss](https://raw.githubusercontent.com/fregu856/segmentation/master/training_logs/model_1/train_loss_per_epoch.png)

- Validation loss:
- ![validation loss](https://raw.githubusercontent.com/fregu856/segmentation/master/training_logs/model_1/val_loss_per_epoch.png)

- The results in the video above was obtained with the model at epoch 23 for which a checkpoint is included in segmentation/training_logs/best_model in the repo.

******
## Training on Microsoft Azure:

To train the model I used an NC6 virtual machine on Microsoft Azure. Below I have listed what I needed to do in order to get started and some things I found useful. For reference my username was 'fregu856':
- Download Cityscapes.

- Install docker-ce:
- - $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
- - $ sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable""
- - $ sudo apt-get update
- - $ sudo apt-get install -y docker-ce

- Install CUDA drivers (see ""Install CUDA drivers for NC VMs"" in https://docs.microsoft.com/en-us/azure/virtual-machines/linux/n-series-driver-setup):
- - $ CUDA_REPO_PKG=cuda-repo-ubuntu1604_8.0.61-1_amd64.deb
- - $ wget -O /tmp/${CUDA_REPO_PKG} http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/${CUDA_REPO_PKG} 
- - $ sudo dpkg -i /tmp/${CUDA_REPO_PKG}
- - $ rm -f /tmp/${CUDA_REPO_PKG}
- - $ sudo apt-get update
- - $ sudo apt-get install cuda-drivers
- - Reboot the VM

- Install nvidia-docker:
- - $ wget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker_1.0.1-1_amd64.deb
- - $ sudo dpkg -i /tmp/nvidia-docker*.deb &amp;&amp; rm /tmp/nvidia-docker*.deb
- - $ sudo nvidia-docker run --rm nvidia/cuda nvidia-smi

- Download the latest TensorFlow docker image with GPU support (tensorflow 1.3):
- - $ sudo docker pull tensorflow/tensorflow:latest-gpu

- Create start_docker_image.sh containing:
```
#!/bin/bash

# DEFAULT VALUES
GPUIDS=""0""
NAME=""fregu856_GPU""


NV_GPU=""$GPUIDS"" nvidia-docker run -it --rm \
        -p 5584:5584 \
        --name ""$NAME""""$GPUIDS"" \
        -v /home/fregu856:/root/ \
        tensorflow/tensorflow:latest-gpu bash
```

- /root/ will now be mapped to /home/fregu856 (i.e. $ cd -- takes you to the regular home folder). 

- To start the image:
- - $ sudo sh start_docker_image.sh 
- To commit changes to the image:
- - Open a new terminal window.
- - $ sudo docker commit fregu856_GPU0 tensorflow/tensorflow:latest-gpu
- To stop the image when itâ€™s running:
- - $ sudo docker stop fregu856_GPU0
- To exit the image without killing running code:
- - Ctrl-P + Q
- To get back into a running image:
- - $ sudo docker attach fregu856_GPU0
- To open more than one terminal window at the same time:
- - $ sudo docker exec -it fregu856_GPU0 bash

- To install the needed software inside the docker image:
- - $ apt-get update
- - $ apt-get install nano
- - $ apt-get install sudo
- - $ apt-get install wget
- - $ sudo apt-get install libopencv-dev python-opencv
- - Commit changes to the image (otherwise the installed packages will be removed at exit!)"
"Abstract:  This paper presents a parameter estimation analysis of the seven binary black
hole mergers---GW170104 GW170608 GW170729 GW170809 GW170814 GW170818 and
GW170823---detected during the second observing run of the Advanced LIGO and
Virgo observatories using the gravitational-wave open data. We describe the
methodology for parameter estimation of compact binaries using
gravitational-wave data and we present the posterior distributions of the
inferred astrophysical parameters. We release our samples of the posterior
probability density function with tutorials on using and replicating our
results presented in this paper.
# Posterior samples of the parameters of binary black holes from Advanced LIGO--Virgo's second observing run

**Soumi De<sup>1</sup> Christopher M. Biwer<sup>2</sup> Collin D. Capano<sup>34</sup> Alexander H. Nitz<sup>34</sup> Duncan A. Brown<sup>1</sup>**

**<sup>1</sup>Department of Physics Syracuse University Syracuse NY 13244 USA**

**<sup>2</sup>Los Alamos National Laboratory Los Alamos NM 87545 USA**

**<sup>3</sup>Albert-Einstein-Institut Max-Planck-Institut for Gravitationsphysik D-30167 Hannover Germany**

**<sup>4</sup>Leibniz Universitat Hannover D-30167 Hannover Germany**

## License

![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png ""Creative Commons License"")

This work is licensed under a [https://creativecommons.org/licenses/by/4.0/deed.ast](https://creativecommons.org/licenses/by/4.0/deed.ast).

## Introduction

This is a public data release of posterior samples from the parameter estimation analysis of the seven binary black hole mergers---GW170104 GW170608 GW170729 GW170809 GW170814 GW170818 and GW170823---detected during the second observing run of the Advanced LIGO and Virgo observatories using the gravitational-wave open data. The analysis to generate the data is presented in the paper posted at [arxiv:1811.09232](https://arxiv.org/abs/1811.09232). We provide a notebook to demonstrate how to read the files containing the posterior samples handle the data tools for visualizing the data and commands for reconstructing figures 1 2 and 3 in the paper. We also provide the configuration files and sample scripts with command lines to replicate our analyses for the three events to generate these data.

We encourage use of these data in derivative works. If you use the material provided here please cite the companion paper for this data release using the following reference. The companion paper provides a description of the data and our analyses for generating these data.
```
@article{De:2018
      author         = ""De Soumi and Biwer C. M. and Capano Collin D. and Nitz Alexander H. and Brown Duncan A.""
      title          = ""{Posterior samples of the parameters of black hole mergers in the second Advanced LIGO--Virgo observing run}""
      year           = ""2018""
      eprint         = ""1811.09232""
      archivePrefix  = ""arXiv""
      primaryClass   = ""astro-ph.IM""
      SLACcitation   = ""%%CITATION = ARXIV:1811.09232;%%""
}
```

Please also cite [Biwer et al. (2018)](https://iopscience.iop.org/article/10.1088/1538-3873/aaef0b) using the following reference. This paper describes and validates the PyCBC Inference parameter estimation toolkit that was used for generating the data.
```
@article{Biwer_2019
	doi = {10.1088/1538-3873/aaef0b}
	year = 2019
	month = {jan}
	publisher = {{IOP} Publishing}
	volume = {131}
	number = {996}
	pages = {024503}
	author = {C. M. Biwer and Collin D. Capano and Soumi De and Miriam Cabero and Duncan A. Brown and Alexander H. Nitz and V. Raymond}
	title = {{PyCBC} Inference: A Python-based Parameter Estimation Toolkit for Compact Binary Coalescence Signals}
	journal = {Publications of the Astronomical Society of the Pacific}
}
```

The parameter estimation analyses to generate the posterior samples and construction of the figures to visualize the results have been performed with **PyCBC v1.12.3**.

The contents in the repository are organized as follows :

- ``data_release_o2_bbh_pe.ipynb`` : Notebook demonstrating tools to handle the released posteriors visualize them and reconstruct Figures in the paper arxiv:1811.09232

- ``posteriors`` : Directory having the posterior files
    - ``GW170104`` : Directory for GW170104
        - [gw170104_posteriors_thinned.hdf](https://github.com/gwastro/o2-bbh-pe/blob/master/posteriors/GW170104/gw170104_posteriors_thinned.hdf) : File containing posterior samples from the MCMC for measuring properties of GW170104. Also contains prior samples and PSDs used in the analysis.
    - ``GW170608`` : Directory for GW170608
        - [gw170608_posteriors_thinned.hdf](https://github.com/gwastro/o2-bbh-pe/blob/master/posteriors/GW170608/gw170608_posteriors_thinned.hdf) : File containing posterior samples from the MCMC for measuring properties of GW170608. Also contains prior samples and PSDs used in the analysis.
    - ``GW170729`` : Directory for GW170729
        - [gw170729_posteriors_thinned.hdf](https://github.com/gwastro/o2-bbh-pe/blob/master/posteriors/GW170729/gw170729_posteriors_thinned.hdf) : File containing posterior samples from the MCMC for measuring properties of GW170729. Also contains prior samples and PSDs used in the analysis.
    - ``GW170809`` : Directory for GW170809
        - [gw170809_posteriors_thinned.hdf](https://github.com/gwastro/o2-bbh-pe/blob/master/posteriors/GW170809/gw170809_posteriors_thinned.hdf) : File containing posterior samples from the MCMC for measuring properties of GW170809. Also contains prior samples and PSDs used in the analysis.
    - ``GW170814`` : Directory for GW170814
        - [gw170814_posteriors_thinned.hdf](https://github.com/gwastro/o2-bbh-pe/blob/master/posteriors/GW170814/gw170814_posteriors_thinned.hdf) : File containing posterior samples from the MCMC for measuring properties of GW170814. Also contains prior samples and PSDs used in the analysis.
    - ``GW170818`` : Directory for GW170818
        - [gw170818_posteriors_thinned.hdf](https://github.com/gwastro/o2-bbh-pe/blob/master/posteriors/GW170818/gw170818_posteriors_thinned.hdf) : File containing posterior samples from the MCMC for measuring properties of GW170818. Also contains prior samples and PSDs used in the analysis.
    - ``GW170823`` : Directory for GW170823
        - [gw170823_posteriors_thinned.hdf](https://github.com/gwastro/o2-bbh-pe/blob/master/posteriors/GW170823/gw170823_posteriors_thinned.hdf) : File containing posterior samples from the MCMC for measuring properties of GW170823. Also contains prior samples and PSDs used in the analysis.

- ``run_files`` : Directory having run scripts and configuration files to replicate the analyses
    - ``GW170104`` : Directory for GW170104
        - ``gw170104_inference.ini`` : Configuration file for GW170104 analysis
        - ``run_pycbc_inference_gw170104.sh`` : Run script for GW170104 analysis
    - ``GW170608`` : Directory for GW170608
        - ``gw170608_inference.ini`` : Configuration file for GW170608 analysis
        - ``run_pycbc_inference_gw170608.sh`` : Run script for GW170608 analysis
    - ``GW170729`` : Directory for GW170729
        - ``gw170729_inference.ini`` : Configuration file for GW170729 analysis
        - ``run_pycbc_inference_gw170729.sh`` : Run script for GW170729 analysis
    - ``GW170809`` : Directory for GW170809
        - ``gw170809_inference.ini`` : Configuration file for GW170809 analysis
        - ``run_pycbc_inference_gw170809.sh`` : Run script for GW170809 analysis
    - ``GW170814`` : Directory for GW170814 :
        - ``gw170814_inference.ini`` : Configuration file for GW170814 analysis
        - ``run_pycbc_inference_gw170814.sh`` : Run script for GW170814 analysis
    - ``GW170818`` : Directory for GW170818
        - ``gw170818_inference.ini`` : Configuration file for GW170818 analysis
        - ``run_pycbc_inference_gw170818.sh`` : Run script for GW170818 analysis
    - ``GW170823`` : Directory for GW170823
        - ``gw170823_inference.ini`` : Configuration file for GW170823 analysis
        - ``run_pycbc_inference_gw170823.sh`` : Run script for GW170823 analysis
    - ``run_pycbc_inference_extract_samples.sh`` : Contains command for extracting independent samples from the full chains obtained from the MCMC runs.
        
## Running the notebook in a Docker container

This notebook can be run from a PyCBC Docker container or a machine with PyCBC installed. Instructions for [downloading the docker container](http://gwastro.github.io/pycbc/latest/html/docker.html) are available from the [PyCBC home page.](https://pycbc.org/) To start a container with instance of Jupyter notebook run the commands
```sh
docker pull pycbc/pycbc-el7:v1.12.3
docker run -p 8888:8888 --name pycbc_notebook -it pycbc/pycbc-el7:v1.12.3 /bin/bash -l
```
Once the container has started this git repository can be downloaded with the command:
```sh
git clone https://github.com/gwastro/o2-bbh-pe.git
```
The notebook server can be started inside the container with the command:
```sh
jupyter notebook --ip 0.0.0.0 --no-browser
```
You can then connect to the notebook server at the URL printed by ``jupyter``. Navigate to the directory `o2-bbh-pe` in the cloned git repository and open [data_release_o2_bbh_pe.ipynb](https://github.com/gwastro/o2-bbh-pe/blob/master/data_release_o2_bbh_pe.ipynb) (this notebook).

## Acknowledgements
This research has made use of data from the Gravitational Wave Open Science Center [https://www.gw-openscience.org](https://www.gw-openscience.org). Computations were performed in the Syracuse University SUGWG cluster.

**Funding:** This work was supported by NSF awards PHY-1707954 (DAB SD) and PHY-1607169 (SD). SD was also supported by the Inaugural Kathy '73 and Stan '72 Walters Endowed Fund for Science Research Graduate Fellowship at Syracuse University. Computations were supported by Syracuse University and NSF award OAC-1541396.

## Authors' contributions
Conceptualization: DAB Methodology: SD CMB CDC AHN; Software: CMB CDC SD AHN DAB; Validation: CDC CMB AHN; Formal Analysis: SD; Investigation: SD CMB CDC AHN; Resources: DAB; Data Curation: DAB CDC CMB AHN and SD; Writing: SD CMB CDC DAB and AHN; Visualization: SD CMB CDC AHN; Supervision: DAB; Project Administration: DAB; Funding Acquisition: DAB."
"Abstract:  Bayesian models offer great flexibility for clustering
applications---Bayesian nonparametrics can be used for modeling infinite
mixtures and hierarchical Bayesian models can be utilized for sharing clusters
across multiple data sets. For the most part such flexibility is lacking in
classical clustering methods such as k-means. In this paper we revisit the
k-means clustering algorithm from a Bayesian nonparametric viewpoint. Inspired
by the asymptotic connection between k-means and mixtures of Gaussians we show
that a Gibbs sampling algorithm for the Dirichlet process mixture approaches a
hard clustering algorithm in the limit and further that the resulting
algorithm monotonically minimizes an elegant underlying k-means-like clustering
objective that includes a penalty for the number of clusters. We generalize
this analysis to the case of clustering multiple data sets through a similar
asymptotic argument with the hierarchical Dirichlet process. We also discuss
further extensions that highlight the benefits of our analysis: i) a spectral
relaxation involving thresholded eigenvectors and ii) a normalized cut graph
clustering algorithm that does not fix the number of clusters in the graph.
# DP-means clustering in R

This package implements the DP-means algorithm introduced by Kulis and Jordan in their article *[Revisiting k-means: New Algorithms via Bayesian Nonparametrics](https://arxiv.org/abs/1111.0352)*. Instead of specifying how many clusters to partition the data into like one would with k-means user specifies a penalty parameter λ which controls if/when new clusters are created during iterations:

![Effect of choice of lambda on clustering](lambdas.png)

The algorithm starts with a single cluster and then processes the data points creating new clusters when needed and then updates centers until convergence.

## Installation

```R
# install.packages(""devtools"")
devtools::install_github(""bearloga/dpmclust"")
```

## Usage

`dp_means()` returns an object with same class and components as `kmeans()` does which makes it easy to use other packages that support the `kmeans` object (e.g. [`autoplot()` in the `ggfortify` package](https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_pca.html)).

```R
y &lt;- dp_means(x lambda = 1)
# y$cluster
```

## Future Work

Need to implement [lambda means](https://ieeexplore.ieee.org/document/7899984) algorithm for choosing optimal λ."
"Abstract:  Coreference resolution is an important task for natural language
understanding and the resolution of ambiguous pronouns a longstanding
challenge. Nonetheless existing corpora do not capture ambiguous pronouns in
sufficient volume or diversity to accurately indicate the practical utility of
models. Furthermore we find gender bias in existing corpora and systems
favoring masculine entities. To address this we present and release GAP a
gender-balanced labeled corpus of 8908 ambiguous pronoun-name pairs sampled to
provide diverse coverage of challenges posed by real-world text. We explore a
range of baselines which demonstrate the complexity of the challenge the best
achieving just 66.9% F1. We show that syntactic structure and continuous neural
models provide promising complementary cues for approaching the challenge.
# GAP Coreference Dataset

GAP is a gender-balanced dataset containing 8908 coreference-labeled pairs of (ambiguous pronoun antecedent name) sampled from Wikipedia and released by [Google AI Language](https://ai.google/research/teams/language/) for the evaluation of coreference resolution in practical applications.

http://goo.gl/language/gap-coreference

## Motivation

Coreference resolution is an important task for natural language understanding and the resolution of ambiguous pronouns a longstanding challenge.
Nonetheless existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models.

[Google AI Language's](https://ai.google/research/teams/language/) GAP dataset is an evaluation benchmark comprising 8908 coreference-labeled pairs of (ambiguous pronoun antecedent name) sampled from Wikipedia to provide diverse coverage of challenges posed by real-world text.
Importantly GAP is gender-balanced to address the gender bias in coreference systems noted in our and other's analysis.

More details are available in [our paper](https://arxiv.org/abs/1810.05201) (which should be cited if you use or discuss GAP in your work):


<div class=""highlight highlight-source-shell""><pre>
@inproceedings{webster2018gap
  title =     {Mind the GAP: A Balanced Corpus of Gendered Ambiguou}
  author =    {Webster Kellie and Recasens Marta and Axelrod Vera and Baldridge Jason}
  booktitle = {Transactions of the ACL}
  year =      {2018}
  pages =     {to appear}
}
</pre></div>

## Dataset Description

The GAP dataset release comprises three .tsv files each with eleven columns.

The files are:
 * **test** 4000 pairs to be used for official evaluation
 * **development** 4000 pairs may be used for model development
 * **validation** 908 pairs may be used for parameter tuning

The columns contain:

Column | Header         | Description
:-----:|----------------|--------------------------------------------
1      | ID             | Unique identifer for an example (two pairs)
2      | Text           | Text containing the ambiguous pronoun and two candidate names. About a paragraph in length
3      | Pronoun        | The pronoun text
4      | Pronoun-offset | Character offset of Pronoun in Column 2 (Text)
5      | A ^            | The first name text
6      | A-offset       | Character offset of A in Column 2 (Text)
7      | A-coref        | Whether A corefers with the pronoun TRUE or FALSE
8      | B ^            | The second name text
9      | B-offset       | Character offset of B in Column 2 (Text)
10     | B-coref        | Whether B corefers with the pronoun TRUE or FALSE
11     | URL ^^         | The URL of the source Wikipedia page

^ Please note that systems should detect mentions for inference automatically and access labeled spans only to output predictions.

^^ Please also note that there are two task settings *snippet-context* in which the URL column may **not** be used and *page-context* where the URL and the denoted Wikipedia page may be used.

## Benchmarks

Performance on GAP may be benchmarked against the syntactic parallelism baseline from our above paper on the test set:

Task Setting      | M    | F    |  B     | O
:----------------:|------|------|--------|------
*snippet-context* | 69.4 | 64.4 | *0.93* | 66.9
*page-context*    | 72.3 | 68.8 | *0.95* | 70.6

where the metrics are F1 score on **M**asculine and **F**eminine examples **O**verall and a **B**ias factor calculated as **F** / **M**.

## Contact
To contact us please use gap-coreference@google.com"
"Abstract:  We propose a straightforward method that simultaneously reconstructs the 3D
facial structure and provides dense alignment. To achieve this we design a 2D
representation called UV position map which records the 3D shape of a complete
face in UV space then train a simple Convolutional Neural Network to regress
it from a single 2D image. We also integrate a weight mask into the loss
function during training to improve the performance of the network. Our method
does not rely on any prior face model and can reconstruct full facial geometry
along with semantic meaning. Meanwhile our network is very light-weighted and
spends only 9.8ms to process an image which is extremely faster than previous
works. Experiments on multiple challenging datasets show that our method
surpasses other state-of-the-art methods on both reconstruction and alignment
tasks by a large margin.
# Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network

<p align=""center"">
<img src=""Docs/images/prnet.gif""/>
</p>



This is an official python implementation of PRN. 

PRN is a method to jointly regress dense alignment and 3D face shape in an end-to-end manner. More examples on Multi-PIE and 300VW can be seen in [YouTube](https://youtu.be/tXTgLSyIha8) .

The main features are:

* **End-to-End**  our method can directly regress the 3D facial structure and dense alignment from a single image bypassing 3DMM fitting.

* **Multi-task**  By regressing position map the 3D geometry along with semantic meaning can be obtained. Thus we can effortlessly complete the tasks of dense alignment monocular 3D face reconstruction pose estimation etc.

* **Faster than real-time**  The method can run at over 100fps(with GTX 1080) to regress a position map.

* **Robust** Tested on facial images in unconstrained conditions.  Our method is robust to poses illuminations and occlusions. 

  

## Applications

### Basics(Evaluated in paper)

* #### Face Alignment

Dense alignment of both visible and non-visible points(including 68 key points). 

And the **visibility** of  points(1 for visible and 0 for non-visible).

![alignment](Docs/images/alignment.jpg)

* #### 3D Face Reconstruction

Get the 3D vertices and corresponding colours from a single image.  Save the result as mesh data(.obj) which can be opened with [Meshlab](http://www.meshlab.net/) or Microsoft [3D Builder](https://developer.microsoft.com/en-us/windows/hardware/3d-print/3d-builder-resources). Notice that the texture of non-visible area is distorted due to self-occlusion.

**New**: 

1. you can choose to output mesh with its original pose(default) or with front view(which means all output meshes are aligned)
2. obj file can now also written with texture map(with specified texture size) and you can set non-visible texture to 0. 



![alignment](Docs/images/reconstruct.jpg)



### More(To be added)

* #### 3D Pose Estimation

  Rather than only use 68 key points to calculate the camera matrix(easily effected by expression and poses) we use all vertices(more than 40K) to calculate a more accurate pose.

  #### ![pose](Docs/images/pose.jpg)

* #### Depth image

  ![pose](Docs/images/depth.jpg)

* #### Texture Editing

  * Data Augmentation/Selfie Editing

    modify special parts of input face eyes for example:

    ![pose](Docs/images/eye.jpg)

  * Face Swapping

    replace the texture with another then warp it to original pose and use Poisson editing to blend images.

    ![pose](Docs/images/swapping.jpg)

    




## Getting Started

### Prerequisite

* Python 2.7 (numpy skimage scipy)

* TensorFlow &gt;= 1.4

  Optional:

* dlib (for detecting face.  You do not have to install if you can provide bounding box information. )

* opencv2 (for showing results)

GPU is highly recommended. The run time is ~0.01s with GPU(GeForce GTX 1080) and ~0.2s with CPU(Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz).

### Usage

1. Clone the repository

```bash
git clone https://github.com/YadiraF/PRNet
cd PRNet
```

2. Download the PRN trained model at [BaiduDrive](https://pan.baidu.com/s/10vuV7m00OHLcsihaC-Adsw) or [GoogleDrive](https://drive.google.com/file/d/1UoE-XuW1SDLUjZmJPkIZ1MLxvQFgmTFH/view?usp=sharing) and put it into `Data/net-data`

3. Run the test code.(test AFLW2000 images)

   `python run_basics.py #Can run only with python and tensorflow`

4. Run with your own images

   `python demo.py -i <inputdir> -o <outputdir> --isDlib True  `

   run `python demo.py --help` for more details.

5. For Texture Editing Apps:

   `python demo_texture.py -i image_path_1 -r image_path_2 -o output_path   `

   run `python demo_texture.py --help` for more details.



## Training

The core idea of the paper is:

Using position map to represent face geometry&amp;alignment information then learning this with an Encoder-Decoder Network.

So the training steps:

1. generate position map ground truth.

   the example of generating position map of 300W_LP dataset can be seen in [generate_posmap_300WLP](https://github.com/YadiraF/face3d/blob/master/examples/8_generate_posmap_300WLP.py)

2. an encoder-decoder network to learn mapping from rgb image to position map.

   the weight mask can be found in the folder `Data/uv-data`

What you can custom:

1. the UV space of position map.

   you can change the parameterization method or change the resolution of UV space.

2. the backbone of encoder-decoder network

   this demo uses residual blocks. VGG mobile-net are also ok.

3. the weight mask

   you can change the weight to focus more on which part your project need more.

4. the training data

   if you have scanned 3d face it's better to train PRN with your own data. Before that you may need use ICP to align your face meshes.



## FQA

1. How to **speed up**?

   a. network inference part

   you can train a smaller network or use a smaller position map as input.

   b. render part

   you can refer to  [c++ version](https://github.com/YadiraF/face3d/blob/master/face3d/mesh/render.py). 

   c. other parts like detecting face writing obj

   the best way is to rewrite them in c++.

2. How to improve the **precision**?

   a. geometry precision.

   Due to the restriction of training data the precision of reconstructed face from this demo has little detail. You can train the network with your own detailed data or do post-processing like shape-from-shading to add details.

   b. texture precision.

   I just added an option to specify the texture size. When the texture size &gt; face size in original image and render new facial image with [texture mapping](https://github.com/YadiraF/face3d/blob/04869dcee1455d1fa5b157f165a6878c550cf695/face3d/mesh/render.py) there will be little resample error.

   

## Changelog

* 2018/7/19 add training part. can specify the resolution of the texture map.
* 2018/5/10 add texture editing examples(for data augmentation face swapping)
* 2018/4/28 add visibility of vertices output obj file with texture map depth image
* 2018/4/26 can output mesh with front view
* 2018/3/28 add pose estimation
* 2018/3/12  first release(3d reconstruction and dense alignment)



## License

Code: under MIT license.

Trained model file: please see [issue 28](https://github.com/YadiraF/PRNet/issues/28) thank [Kyle McDonald](https://github.com/kylemcdonald) for his answer.



## Citation

If you use this code please consider citing:

```
@inProceedings{feng2018prn
  title     = {Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network}
  author    = {Yao Feng and Fan Wu and Xiaohu Shao and Yanfeng Wang and Xi Zhou}
  booktitle = {ECCV}
  year      = {2018}
}
```



## Contacts

Please contact _fengyao@sjtu.edu.cn_  or open an issue for any questions or suggestions.

Thanks! (●'◡'●)



## Acknowledgements

- Thanks [BFM team](https://faces.dmi.unibas.ch/bfm/) [Xiangyu Zhu](http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm) and [Anil Bas](https://github.com/anilbas/3DMMasSTN) for sharing 3D data.
- Thanks Patrik Huber for sharing his work  [eos](https://github.com/patrikhuber/eos) which helps me a lot in studying 3D Face Reconstruction.
- Thanks the authors of  [3DMMasSTN](https://github.com/anilbas/3DMMasSTN) [DenseReg](https://github.com/ralpguler/DenseReg) [3dmm_cnn](https://github.com/anhttran/3dmm_cnn) [vrn](https://github.com/AaronJackson/vrn) [pix2vertex](https://github.com/matansel/pix2vertex) [face-alignment](https://github.com/1adrianb/face-alignment) for making their excellent works publicly available. 
</outputdir></inputdir>"
"Abstract:  Variational autoencoders were proven successful in domains such as computer
vision and speech processing. Their adoption for modeling user preferences is
still unexplored although recently it is starting to gain attention in the
current literature. In this work we propose a model which extends variational
autoencoders by exploiting the rich information present in the past preference
history. We introduce a recurrent version of the VAE where instead of passing
a subset of the whole history regardless of temporal dependencies we rather
pass the consumption sequence subset through a recurrent neural network. At
each time-step of the RNN the sequence is fed through a series of
fully-connected layers the output of which models the probability distribution
of the most likely future preferences. We show that handling temporal
information is crucial for improving the accuracy of the VAE: In fact our
model beats the current state-of-the-art by valuable margins because of its
ability to capture temporal dependencies among the user-consumption sequence
using the recurrent encoder still keeping the fundamentals of variational
autoencoders intact.
# Sequential Variational Autoencoders for Collaborative Filtering
This notebook accompanies the paper ""<a href=""http://www.wsdm-conference.org/2019/accepted-papers.php"">Sequential Variational Autoencoders for Collaborative Filtering</a>"" [ <a href=""https://arxiv.org/abs/1811.09975"">arXiv</a> ] [ <a href=""https://dl.acm.org/citation.cfm?id=3291007"">ACM Library</a> ] by Noveen Sachdeva Giuseppe Manco Ettore Ritacco and Vikram Pudi published at the 12th ACM International Conference on Web Search and Data Mining (WSDM '19).

The notebook provides PyTorch code for the proposed model ""SVAE"" along with the data preprocessing for the Movielens-1M dataset."
"Abstract:  Machine learning is a popular approach to signatureless malware detection
because it can generalize to never-before-seen malware families and polymorphic
strains. This has resulted in its practical use for either primary detection
engines or for supplementary heuristic detection by anti-malware vendors.
Recent work in adversarial machine learning has shown that deep learning models
are susceptible to gradient-based attacks whereas non-differentiable models
that report a score can be attacked by genetic algorithms that aim to
systematically reduce the score. We propose a more general framework based on
reinforcement learning (RL) for attacking static portable executable (PE)
anti-malware engines. The general framework does not require a differentiable
model nor does it require the engine to produce a score. Instead an RL agent
is equipped with a set of functionality-preserving operations that it may
perform on the PE file. Through a series of games played against the
anti-malware engine it learns which sequences of operations are likely to
result in evading the detector for any given malware sample. This enables
completely black-box attacks against static PE anti-malware and produces
functional evasive malware samples as a direct result. We show in experiments
that our method can attack a gradient-boosted machine learning model with
evasion rates that are substantial and appear to be strongly dependent on the
dataset. We demonstrate that attacks against this model appear to also evade
components of publicly hosted antivirus engines. Adversarial training results
are also presented: by retraining the model on evasive ransomware samples a
subsequent attack is 33% less effective. However there are overfitting dangers
when adversarial training which we note. We release code to allow researchers
to reproduce and improve this approach.
# Malware Env for OpenAI Gym
**************************
Citing
======

If you use this code in a publication please cite the following [paper](https://arxiv.org/abs/1801.08917):

```

Hyrum S. Anderson Anant Kharkar Bobby Filar David Evans Phil Roth ""Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning"" in ArXiv e-prints. Jan. 2018.

@ARTICLE{anderson2018learning
  author={Anderson Hyrum S and Kharkar Anant and Filar Bobby and Evans David and Roth Phil}
  title={Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning}
  journal={arXiv preprint arXiv:1801.08917}
  archivePrefix = ""arXiv""
  eprint = {1801.08917}
  primaryClass = ""cs.CR""
  keywords = {Computer Science - Cryptography and Security}
  year = 2018
  month = jan
  adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180108917A}
}
```


**This is a malware manipulation environment for OpenAI's ``gym``.** 
[OpenAI Gym](https://gym.openai.com/) is a toolkit for developing and comparing reinforcement 
learning algorithms. This makes it possible to write agents that learn 
to manipulate PE files (e.g. malware) to achieve some objective 
(e.g. bypass AV) based on a reward provided by taking specific manipulation
actions.

Objective
======
Create an AI that learns through reinforcement learning which functionality-preserving transformations to make on a malware sample to break through / bypass machine learning static-analysis malware detection.

![Breakout](https://github.com/matthiasplappert/keras-rl/raw/master/assets/breakout.gif?raw=true
""Breakout"")

Basics
======

There are two basic concepts in reinforcement learning: the environment (in our case the malware sample) and the agent (namely the algorithm used to change the environment). The agent sends `actions` to the environment and the environment replies with `observations` and `rewards` (that is a score).

This repo provides an environment for manipulating PE files and providing rewards that are based around bypassing AV.  An agent can be deployed that have already been written for the rich ``gym`` framework.  For example

* https://github.com/pfnet/chainerrl [recommended]
* https://github.com/matthiasplappert/keras-rl
 
Setup
=====
The EvadeRL framework is built on Python3.6 we recommend first creating a virtualenv (details can be found [here]) with Python3.6 then performing the following actions ensure you have the correct python libraries:

[here]: https://docs.python.org/3/tutorial/venv.html
```sh
pip install -r requirements.txt
```

EvadeRL also leverages a Library to Instrument Executable Formats aptly named [LIEF]. It allows our agent to modify the binary on-the-fly. To add it to your virtualenv just ```pip install``` one of their pre-built packages. Examples below:

[LIEF]: https://github.com/lief-project/LIEF

Linux
```
pip install https://github.com/lief-project/LIEF/releases/download/0.7.0/linux_lief-0.7.0_py3.6.tar.gz
```

OSX
```
pip install https://github.com/lief-project/LIEF/releases/download/0.7.0/osx_lief-0.7.0_py3.6.tar.gz
```

Once completed ensure you've moved malware samples into the 
```
gym_malware/gym_malware/envs/utils/samples/
```

If you are unsure where to acquire malware samples see the **Data Acquisition** section below. If you have samples in the correct directory you can check to see if your environment is correctly setup by running :

```
python test_agent_chainer.py
```

Note that if you are using Anaconda you may need to
```
conda install libgcc
```
in order for LIEF to operate properly.

Data Acquisition
=====
If you have a VirusTotal API key you may download samples to the `gym_malware/gym_malware/envs/utils/samples/` using the Python script `download_samples.py`.

Gym-Malware Environment
====
EvadeRL pits a reinforcement agent against the malware environment consisting of the following components:

* Action Space
* Independent Malware Classifier
* OpenAI framework malware environment (aka gym-malware)
 
Action Space
----
The moves or actions that can be performed on a malware sample in our environment consist of the following binary manipulations:
* append_zero
* append_random_ascii
* append_random_bytes
* remove_signature
* upx_pack
* upx_unpack
* change_section_names_from_list
* change_section_names_to random
* modify_export
* remove_debug
* break_optional_header_checksum

The agent will randomly select these actions in an attempt to bypass the classifier (info on default classifier below). Over time the agent learns which combinations lead to the highest rewards or learns a policy (*like an optimal plan of attack for any given observation*).

Independent Classifier
----
Included as a default model is a [gradient boosted decision trees model] trained on 50k malicious and 50k benign samples with the following features extracted:
* Byte-level data (e.g. histogram and entropy)
* Header
* Section
* Import/Exports


[gradient boosted decision trees model]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
"Abstract:  Many real-world vision problems suffer from inherent ambiguities. In clinical
applications for example it might not be clear from a CT scan alone which
particular region is cancer tissue. Therefore a group of graders typically
produces a set of diverse but plausible segmentations. We consider the task of
learning a distribution over segmentations given an input. To this end we
propose a generative segmentation model based on a combination of a U-Net with
a conditional variational autoencoder that is capable of efficiently producing
an unlimited number of plausible hypotheses. We show on a lung abnormalities
segmentation task and on a Cityscapes segmentation task that our model
reproduces the possible segmentation variants as well as the frequencies with
which they occur doing so significantly better than published approaches.
These models could have a high impact in real-world applications such as being
used as clinical decision-making algorithms accounting for multiple plausible
semantic segmentation hypotheses to provide possible diagnoses and recommend
further actions to resolve the present ambiguities.
# Probabilistic U-Net

Re-implementation of the model described in `A Probabilistic U-Net for Segmentation of Ambiguous Images' ([paper @ NeurIPS 2018](https://arxiv.org/abs/1806.05034)).

This was also a spotlight presentation at NeurIPS and a short video on the paper of similar content can be found [here](https://youtu.be/-cfFxQWfFrA) (4min).

The architecture of the Probabilistic U-Net is depicted below: subfigure a) shows sampling and b) the training setup: 
![](assets/architecture.png)

Below see samples conditioned on held-out validation set images from the (stochastic) CityScapes data set:
![](assets/10_image_16_sample.gif)

## Setup package in virtual environment

```
git clone https://github.com/SimonKohl/probabilistic_unet.git .
cd prob_unet/
virtualenv -p python3 venv
source venv/bin/activate
pip3 install -e .
```

## Install batch-generators for data augmentation
```
cd ..
git clone https://github.com/MIC-DKFZ/batchgenerators
cd batchgenerators
pip3 install nilearn scikit-image nibabel
pip3 install -e .
cd prob_unet
```

## Download &amp; preprocess the Cityscapes dataset

1) Create a login account on the Cityscapes website: https://www.cityscapes-dataset.com/
2) Once you've logged in download the train val and test annotations and images:
    - Annotations: [gtFine_trainvaltest.zip](https://www.cityscapes-dataset.com/file-handling/?packageID=1) (241MB)
    - Images: [leftImg8bit_trainvaltest.zip](https://www.cityscapes-dataset.com/file-handling/?packageID=3) (11GB)
3) unzip the data (unzip <name>_trainvaltest.zip) and adjust `raw_data_dir` (full path to unzipped files) and `out_dir` (full path to desired output directory) in `preprocessing_config.py`
4) bilinearly rescale the data to a resolution of 256 x 512 and save as numpy arrays by running
```
cd cityscapes
python3 preprocessing.py
cd ..
```

## Training

[skip to evaluation in case you only want to use the pretrained model.]  
modify `data_dir` and `exp_dir` in `scripts/prob_unet_config.py` then:
```
cd training
python3 train_prob_unet.py --config prob_unet_config.py
```

## Evaluation

Load your own trained model or use a pretrained model. A set of pretrained weights can be downloaded from [zenodo.org](https://zenodo.org/record/1419051#.W5utoOEzYUE) (187MB). After down-loading unpack the file via
`tar -xvzf pretrained_weights.tar.gz` e.g. in `/model`. In either case (using your own or the pretrained model) modify the `data_dir` and
`exp_dir` in `evaluation/cityscapes_eval_config.py` to match you paths.

then first write samples (defaults to 16 segmentation samples for each of the 500 validation images):
```
cd ../evaluation
python3 eval_cityscapes.py --write_samples
```
followed by their evaluation (which is multi-threaded and thus reasonably fast):
```
python3 eval_cityscapes.py --eval_samples
```
The evaluation produces a dictionary holding the results. These can be visualized by launching an ipython notbook:
```
jupyter notebook evaluation_plots.ipynb
```
The following results are obtained from the pretrained model using above notebook:
![](assets/validation_results.png) 

## Tests

The evaluation metrics are under test-coverage. Run the tests as follows:
```
cd ../tests/evaluation
python3 -m pytest eval_tests.py
```

## Deviations from original work

The code found in this repository was not used in the original paper and slight modifications apply:

- training on a single gpu (Titan Xp) instead of distributed training which is not supported in this implementation
- average-pooling rather than bilinear interpolation is used for down-sampling operations in the model
- the number of conv kernels is kept constant after the 3rd scale as opposed to strictly doubling it after each scale (for reduction of memory footprint)
- HeNormal weight initialization worked better than a orthogonal weight initialization


## How to cite this code
Please cite the original publication:
```
@article{kohl2018probabilistic
  title={A Probabilistic U-Net for Segmentation of Ambiguous Images}
  author={Kohl Simon AA and Romera-Paredes Bernardino and Meyer Clemens and De Fauw Jeffrey and Ledsam Joseph R and Maier-Hein Klaus H and Eslami SM and Rezende Danilo Jimenez and Ronneberger Olaf}
  journal={arXiv preprint arXiv:1806.05034}
  year={2018}
}
```

## License
The code is published under the [Apache License Version 2.0](LICENSE).</name>"
"Abstract:  Many applications in machine learning require optimizing a function whose
true gradient is unknown but where surrogate gradient information (directions
that may be correlated with but not necessarily identical to the true
gradient) is available instead. This arises when an approximate gradient is
easier to compute than the full gradient (e.g. in meta-learning or unrolled
optimization) or when a true gradient is intractable and is replaced with a
surrogate (e.g. in certain reinforcement learning applications or when using
synthetic gradients). We propose Guided Evolutionary Strategies a method for
optimally using surrogate gradient directions along with random search. We
define a search distribution for evolutionary strategies that is elongated
along a guiding subspace spanned by the surrogate gradients. This allows us to
estimate a descent direction which can then be passed to a first-order
optimizer. We analytically and numerically characterize the tradeoffs that
result from tuning how strongly the search distribution is stretched along the
guiding subspace and we use this to derive a setting of the hyperparameters
that works well across problems. Finally we apply our method to example
problems demonstrating an improvement over both standard evolutionary
strategies and first-order methods (that directly follow the surrogate
gradient). We provide a demo of Guided ES at
this https URL
# Guided Evolutionary Strategies

**Link to demo notebook:** [Guided ES
Demo](https://colab.sandbox.google.com/github/brain-research/guided-evolutionary-strategies/blob/master/Guided_Evolutionary_Strategies_Demo.ipynb)

**Link to paper:** [arXiv/1806.10230](https://arxiv.org/abs/1806.10230)

## Overview

Many applications in machine learning require optimizing a function whose true
gradient is unknown but where surrogate gradient information (directions that
may be correlated with but not necessarily identical to the true gradient) is
available instead. This arises when an approximate gradient is easier to compute
than the full gradient (e.g. in meta-learning or unrolled optimization) or when
a true gradient is intractable and is replaced with a surrogate (e.g. in certain
reinforcement learning applications or when using synthetic gradients).

Here we propose _Guided Evolutionary Strategies_ (Guided ES) a method for
optimally using surrogate gradient directions along with random search. We
define a search distribution for evolutionary strategies that is elongated along
a guiding subspace spanned by the surrogate gradients. This allows us to
estimate a descent direction which can then be passed to a first-order
optimizer.

This repository contains a colaboratory (colab) notebook with a demo of the
method on a toy problem (described below).

## Introduction

Imagine you have a function you would like to optimize but you only have access
to approximate gradients of the function. There are two approaches to
optimization. On one hand you could ignore the surrogate gradient information
entirely and perform zeroth-order optimization using methods such as
evolutionary strategies to estimate a descent direction. These methods exhibit
poor convergence properties when the parameter dimension is large. On the other
hand you could directly feed the surrogate gradients to a first-order
optimization algorithm. However bias in the surrogate gradients will interfere
with optimizing the target problem. Ideally we would like a method that
combines the complementary strengths of these two approaches: we would like to
combine the unbiased descent direction estimated with evolutionary strategies
with the low-variance estimate given by the surrogate gradient. We propose a
method for doing this called guided evolutionary strategies (Guided ES).

## Method

Our idea is to keep track of a low-dimensional subspace defined by the recent
history of surrogate gradients during optimization (inspired by quasi-Newton
methods) which we call the guiding subspace.

We then perform a finite difference random search (as in evolutionary
strategies) preferentially within this subspace. By concentrating our search
samples in a low-dimensional subspace where the true gradient has non-negligible
support we can dramatically reduce the variance of our search direction.

The figure panel (a) below depicts the geometry underlying our method. Instead
of the true gradient (blue arrow) we are given a surrogate gradient (white
arrow) which is correlated with the true gradient. We use this to form a guiding
distribution (denoted with white contours) and use this to draw samples (white
dots) which we use as part of a random search procedure.

![Schematic of guided evolutionary
strategies](images/fig1.png?raw=true ""Schematic of guided evolutionary strategies"")

In panel (b) we demonstrate the performance of the method on a toy problem. The
problem consists of a random quadratic function where we add an explicit bias
and random noise to the gradient. Following the gradient directly with SGD
(orange curve) starts fast but starts to diverge due to the bias in the
gradient. Performing evolutionary strategies (or an adaptive variant CMA-ES)
succeed in minimizing the true function but proceed slowly and ignore the
gradient information.

Guided ES on the other hand combines the strengths of these two approaches.

## Citation

If you use this code please consider citing our paper:

```
@article{
   maheswaranathan2018guided
   title = {Guided evolutionary strategies: escaping the curse of dimensionality in random search}
   author = {Niru Maheswaranathan and Luke Metz and George Tucker and Jascha Sohl-Dickstein}
   year = {2018}
   eprint = {arXiv:1806.10230}
   url = {https://arxiv.org/abs/1806.10230}
}
```

## Contact

Authors:

- Niru Maheswaranathan (nirum@google.com)
- Luke Metz (lmetz@google.com)
- George Tucker (gjt@google.com)
- Jascha Sohl-Dickstein (jaschasd@google.com)

This is not an officially supported Google product."
"Abstract:  Existing benchmarks for analytical database systems such as TPC-DS and TPC-H
are designed for static reporting scenarios. The main metric of these
benchmarks is the performance of running individual SQL queries over a
synthetic database. In this paper we argue that such benchmarks are not
suitable for evaluating database workloads originating from interactive data
exploration (IDE) systems where most queries are ad-hoc not based on
predefined reports and built incrementally. As a main contribution we present
a novel benchmark called IDEBench that can be used to evaluate the performance
of database systems for IDE workloads. As opposed to traditional benchmarks for
analytical database systems our goal is to provide more meaningful workloads
and datasets that can be used to benchmark IDE query engines with a particular
focus on metrics that capture the trade-off between query performance and
quality of the result. As a second contribution this paper evaluates and
discusses the performance results of selected IDE query engines using our
benchmark. The study includes two commercial systems as well as two research
prototypes (IDEA approXimateDB/XDB) and one traditional analytical database
system (MonetDB).
<p align=""center"">
<img align=""display: inline-block;"" src=""https://raw.githubusercontent.com/IDEBench/IDEBench-public/master/logo.png"" width=""500""/>
</p>
<p><br/>
For information on how to use IDEBench checkout our Wiki: http://github.com/IDEBench/IDEBench-public/wiki
 </p>

## License

MIT

## Cite
Users of IDEBench are requested to use the following BibTeX reference:
```
@misc{1804.02593
Author = {Philipp Eichmann and Carsten Binnig and Tim Kraska and Emanuel Zgraggen}
Title = {IDEBench: A Benchmark for Interactive Data Exploration}
Year = {2018}
Eprint = {arXiv:1804.02593}
}
```

## Publications

Eichmann Philipp Carsten Binnig Tim Kraska and Emanuel Zgraggen. ""IDEBench: A Benchmark for Interactive Data Exploration"".
[PDF](https://arxiv.org/abs/1804.02593)

Eichmann Philipp Emanuel Zgraggen Zheguang Zhao Carsten Binnig and Tim Kraska. ""Towards a Benchmark for Interactive Data Exploration."" IEEE Data Eng. Bull. 39 no. 4 (2016): 50-61.
[PDF](http://cs.brown.edu/~peichmann/downloads/bide_vision.pdf)"
"Abstract:  This paper describes an Open Source Software (OSS) project: PythonRobotics.
This is a collection of robotics algorithms implemented in the Python
programming language. The focus of the project is on autonomous navigation and
the goal is for beginners in robotics to understand the basic ideas behind each
algorithm. In this project the algorithms which are practical and widely used
in both academia and industry are selected. Each sample code is written in
Python3 and only depends on some standard modules for readability and ease of
use. It includes intuitive animations to understand the behavior of the
simulation.
<p align=""center"">
<img align=""display: inline-block;"" src=""https://raw.githubusercontent.com/IDEBench/IDEBench-public/master/logo.png"" width=""500""/>
</p>
<p><br/>
For information on how to use IDEBench checkout our Wiki: http://github.com/IDEBench/IDEBench-public/wiki
 </p>

## License

MIT

## Cite
Users of IDEBench are requested to use the following BibTeX reference:
```
@misc{1804.02593
Author = {Philipp Eichmann and Carsten Binnig and Tim Kraska and Emanuel Zgraggen}
Title = {IDEBench: A Benchmark for Interactive Data Exploration}
Year = {2018}
Eprint = {arXiv:1804.02593}
}
```

## Publications

Eichmann Philipp Carsten Binnig Tim Kraska and Emanuel Zgraggen. ""IDEBench: A Benchmark for Interactive Data Exploration"".
[PDF](https://arxiv.org/abs/1804.02593)

Eichmann Philipp Emanuel Zgraggen Zheguang Zhao Carsten Binnig and Tim Kraska. ""Towards a Benchmark for Interactive Data Exploration."" IEEE Data Eng. Bull. 39 no. 4 (2016): 50-61.
[PDF](http://cs.brown.edu/~peichmann/downloads/bide_vision.pdf)"
"Abstract:  Object detection in astronomical images generically referred to as source
finding is often performed before the object characterisation stage in
astrophysical processing work flows. In radio astronomy source finding has
historically been performed by bespoke off-line systems; however modern data
acquisition systems as well as those proposed for upcoming observatories such
as the Square Kilometre Array (SKA) will make this approach unfeasible. One
area where a change of approach is particularly necessary is in the design of
fast imaging systems for transient studies. This paper presents a number of
advances in accelerating and automating the source finding in such systems.
<p align=""center"">
<img align=""display: inline-block;"" src=""https://raw.githubusercontent.com/IDEBench/IDEBench-public/master/logo.png"" width=""500""/>
</p>
<p><br/>
For information on how to use IDEBench checkout our Wiki: http://github.com/IDEBench/IDEBench-public/wiki
 </p>

## License

MIT

## Cite
Users of IDEBench are requested to use the following BibTeX reference:
```
@misc{1804.02593
Author = {Philipp Eichmann and Carsten Binnig and Tim Kraska and Emanuel Zgraggen}
Title = {IDEBench: A Benchmark for Interactive Data Exploration}
Year = {2018}
Eprint = {arXiv:1804.02593}
}
```

## Publications

Eichmann Philipp Carsten Binnig Tim Kraska and Emanuel Zgraggen. ""IDEBench: A Benchmark for Interactive Data Exploration"".
[PDF](https://arxiv.org/abs/1804.02593)

Eichmann Philipp Emanuel Zgraggen Zheguang Zhao Carsten Binnig and Tim Kraska. ""Towards a Benchmark for Interactive Data Exploration."" IEEE Data Eng. Bull. 39 no. 4 (2016): 50-61.
[PDF](http://cs.brown.edu/~peichmann/downloads/bide_vision.pdf)"
"Abstract:  We present a new method to detect planetary transits from time-series
photometry the Transit Least Squares (TLS) algorithm. TLS searches for
transit-like features while taking the stellar limb darkening and planetary
ingress and egress into account. We have optimized TLS for both signal
detection efficiency (SDE) of small planets and computational speed. TLS
analyses the entire unbinned phase-folded light curve. We compensate for the
higher computational load by (i.) using algorithms like ""Mergesort"" (for the
trial orbital phases) and by (ii.) restricting the trial transit durations to a
smaller range that encompasses all known planets and using stellar density
priors where available. A typical K2 light curve including 80d of observations
at a cadence of 30min can be searched with TLS in ~10s real time on a standard
laptop computer as fast as the widely used Box Least Squares (BLS) algorithm.
We perform a transit injection-retrieval experiment of Earth-sized planets
around sun-like stars using synthetic light curves with 110ppm white noise per
30min cadence corresponding to a photometrically quiet KP=12 star observed
with Kepler. We determine the SDE thresholds for both BLS and TLS to reach a
false positive rate of 1% to be SDE~7 in both cases. The resulting true
positive (or recovery) rates are ~93% for TLS and ~76% for BLS implying more
reliable detections with TLS. We also test TLS with the K2 light curve of the
TRAPPIST-1 system and find six of seven Earth-sized planets using an iterative
search for increasingly lower signal detection efficiency the phase-folded
transit of the seventh planet being affected by a stellar flare. TLS is more
reliable than BLS in finding any kind of transiting planet but it is
particularly suited for the detection of small planets in long time series from
Kepler TESS and PLATO. We make our Python implementation of TLS publicly
available.
![Logo](https://raw.githubusercontent.com/hippke/tls/master/docs/source/logo.png)
### An optimized transit-fitting algorithm to search for periodic transits of small planets
[![Image](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/hippke/tls/blob/master/LICENSE)
[![Image](https://img.shields.io/badge/Python-2.7%20%26%203.5%2B-blue.svg)](https://pypi.org/project/transitleastsquares/)
[![Image](https://img.shields.io/badge/pip%20install-transitleastsquares-blue.svg)](https://pypi.org/project/transitleastsquares/)
[![Image](https://img.shields.io/badge/documentation-%E2%9C%93-blue.svg)](https://transitleastsquares.readthedocs.io/en/latest/index.html)
[![Image](https://img.shields.io/badge/tutorials-%E2%9C%93-blue.svg)](https://github.com/hippke/tls/tree/master/tutorials)
[![Image](https://img.shields.io/badge/arXiv-1901.02015-blue.svg)](https://arxiv.org/abs/1901.02015)
[![Build Status](https://travis-ci.com/hippke/tls.svg?branch=master)](https://travis-ci.com/hippke/tls)


## Motivation
We present a new method to detect planetary transits from time-series photometry the *Transit Least Squares* (TLS) algorithm. While the commonly used Box Least Squares [(BLS Kovács et al. 2002)](http://adsabs.harvard.edu/abs/2002A%26A...391..369K) algorithm searches for rectangular signals in stellar light curves *TLS* searches for transit-like features with stellar limb-darkening and including the effects of planetary ingress and egress. Moreover *TLS* analyses the entire unbinned data of the phase-folded light curve. These improvements yield a ~10 % higher detection efficiency (and similar false alarm rates) compared to BLS. The higher detection efficiency of our freely available Python implementation comes at the cost of higher computational load which we partly compensate by applying an optimized period sampling and transit duration sampling constrained to the physically plausible range. A typical Kepler K2 light curve worth of 90 d of observations at a cadence of 30 min can be searched with *TLS* in 10 seconds real time on a standard laptop computer just as with BLS.

![image](https://raw.githubusercontent.com/hippke/tls/master/docs/source/frontpage_rescaled.png)

## Installation

TLS can be installed conveniently using: `pip install transitleastsquares`

If you have multiple versions of Python and pip on your machine try: `pip3 install transitleastsquares`

The latest version can be pulled from github::
```
git clone https://github.com/hippke/tls.git
cd tls
python setup.py install
```

If the command `python` does not point to Python 3 on your machine you can try to replace the last line with `python3 setup.py install`. If you don't have `git` on your machine you can find installation instructions [here](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git). TLS also runs on Python 2 but without multi-threading.


Dependencies:
Python 3
[NumPy](http://www.numpy.org/)
[numba](http://numba.pydata.org/)
[batman-package](https://www.cfa.harvard.edu/~lkreidberg/batman/)
[tqdm](https://github.com/tqdm/tqdm)
optional:
[argparse](https://docs.python.org/3/library/argparse.html) (for the command line version)
[astroquery](https://astroquery.readthedocs.io/en/latest/) (for LD and stellar density priors from Kepler K1 K2 and TESS).

If you have trouble installing please [open an issue](https://github.com/hippke/tls/issues).


## Getting started
Here is a short animation of a real search for planets in Kepler K2 data. For more examples have a look at the [tutorials](https://github.com/hippke/tls/tree/master/tutorials) and the [documentation](https://transitleastsquares.readthedocs.io/en/latest/index.html).

![image](https://raw.githubusercontent.com/hippke/tls/master/docs/source/animation.gif)

## Attribution
Please cite [Hippke &amp; Heller (2019 A&amp;A 623 A39)](https://ui.adsabs.harvard.edu/#abs/2019A&amp;A...623A..39H/abstract) if you find this code useful in your research. The BibTeX entry for the paper is:

```
@ARTICLE{2019A&amp;A...623A..39H
       author = {{Hippke} Michael and {Heller} Ren{\'e}}
        title = ""{Optimized transit detection algorithm to search for periodic transits of small planets}""
      journal = {\aap}
         year = ""2019""
        month = ""Mar""
       volume = {623}
          eid = {A39}
        pages = {A39}
          doi = {10.1051/0004-6361/201834672}
archivePrefix = {arXiv}
       eprint = {1901.02015}
 primaryClass = {astro-ph.EP}
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2019A&amp;A...623A..39H}
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
```

## Contributing Code Bugfixes or Feedback
We welcome and encourage contributions. If you have any trouble [open an issue](https://github.com/hippke/tls/issues).

Copyright 2019 Michael Hippke &amp; René Heller."
"Abstract:  Time Series Classification (TSC) is an important and challenging problem in
data mining. With the increase of time series data availability hundreds of
TSC algorithms have been proposed. Among these methods only a few have
considered Deep Neural Networks (DNNs) to perform this task. This is surprising
as deep learning has seen very successful applications in the last years. DNNs
have indeed revolutionized the field of computer vision especially with the
advent of novel deeper architectures such as Residual and Convolutional Neural
Networks. Apart from images sequential data such as text and audio can also be
processed with DNNs to reach state-of-the-art performance for document
classification and speech recognition. In this article we study the current
state-of-the-art performance of deep learning algorithms for TSC by presenting
an empirical study of the most recent DNN architectures for TSC. We give an
overview of the most successful deep learning applications in various time
series domains under a unified taxonomy of DNNs for TSC. We also provide an
open source deep learning framework to the TSC community where we implemented
each of the compared approaches and evaluated them on a univariate TSC
benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By
training 8730 deep learning models on 97 time series datasets we propose the
most exhaustive study of DNNs for TSC to date.
# Deep Learning for Time Series Classification
This is the companion repository for [our paper](https://link.springer.com/article/10.1007%2Fs10618-019-00619-1) titled ""Deep learning for time series classification: a review"" published in [Data Mining and Knowledge Discovery](https://link.springer.com/journal/10618) also available on [ArXiv](https://arxiv.org/pdf/1809.04356v3.pdf). 

![architecture resnet](https://github.com/hfawaz/dl-4-tsc/blob/master/png/resnet-archi.png)

## Data 
The data used in this project comes from two sources: 
* The [UCR/UEA archive](http://timeseriesclassification.com/TSC.zip) which contains the 85 univariate time series datasets. 
* The [MTS archive](http://www.mustafabaydogan.com/files/viewcategory/20-data-sets.html) which contains the 13 multivariate time series datasets.

## Code 
The code is divided as follows: 
* The [main.py](https://github.com/hfawaz/dl-4-tsc/blob/master/main.py) python file contains the necessary code to run an experiement. 
* The [utils](https://github.com/hfawaz/dl-4-tsc/tree/master/utils) folder contains the necessary functions to read the datasets and visualize the plots.
* The [classifiers](https://github.com/hfawaz/dl-4-tsc/tree/master/classifiers) folder contains nine python files one for each deep neural network tested in our paper. 

To run a model on one dataset you should issue the following command: 
```
python3 main.py TSC Coffee fcn _itr_8
```
which means we are launching the [fcn](https://github.com/hfawaz/dl-4-tsc/blob/master/classifiers/fcn.py) model on the univariate UCR archive for the Coffee dataset (see [constants.py](https://github.com/hfawaz/dl-4-tsc/blob/master/utils/constants.py) for a list of possible options).

## Prerequisites
All python packages needed are listed in [pip-requirements.txt](https://github.com/hfawaz/dl-4-tsc/blob/master/utils/pip-requirements.txt) file and can be installed simply using the pip command. 

* [numpy](http://www.numpy.org/)  
* [pandas](https://pandas.pydata.org/)  
* [sklearn](http://scikit-learn.org/stable/)  
* [scipy](https://www.scipy.org/)  
* [matplotlib](https://matplotlib.org/)  
* [tensorflow-gpu](https://www.tensorflow.org/)  
* [keras](https://keras.io/)  
* [h5py](http://docs.h5py.org/en/latest/build.html)
* [keras_contrib](https://www.github.com/keras-team/keras-contrib.git)

## Results
Our [results](https://github.com/hfawaz/dl-4-tsc/tree/master/results) showed that a deep residual network architecture performs best for the time series classification task. 

The following table contains the averaged accuracy over 10 runs of each implemented model on the UCR/UEA archive with the standard deviation between parentheses. 

| Datasets                       | MLP       | FCN        | ResNet     | Encoder    | MCNN       | t-LeNet   | MCDCNN     | Time-CNN  | TWIESN     | 
|--------------------------------|-----------|------------|------------|------------|------------|-----------|------------|-----------|------------| 
| 50words                        | 68.4(7.1) | 62.7(6.1)  | 74.0(1.5)  | 72.3(1.0)  | 22.0(24.3) | 12.5(0.0) | 58.9(5.3)  | 62.1(1.0) | 49.6(2.6)  | 
| Adiac                          | 39.7(1.9) | 84.4(0.7)  | 82.9(0.6)  | 48.4(2.5)  | 2.2(0.6)   | 2.0(0.0)  | 61.0(8.7)  | 37.9(2.0) | 41.6(4.5)  | 
| ArrowHead                      | 77.8(1.2) | 84.3(1.5)  | 84.5(1.2)  | 80.4(2.9)  | 33.9(4.7)  | 30.3(0.0) | 68.5(6.7)  | 72.3(2.6) | 65.9(9.4)  | 
| Beef                           | 72.0(2.8) | 69.7(4.0)  | 75.3(4.2)  | 64.3(5.0)  | 20.0(0.0)  | 20.0(0.0) | 56.3(7.8)  | 76.3(1.1) | 53.7(14.9) | 
| BeetleFly                      | 87.0(2.6) | 86.0(9.7)  | 85.0(2.4)  | 74.5(7.6)  | 50.0(0.0)  | 50.0(0.0) | 58.0(9.2)  | 89.0(3.2) | 73.0(7.9)  | 
| BirdChicken                    | 77.5(3.5) | 95.5(3.7)  | 88.5(5.3)  | 66.5(5.8)  | 50.0(0.0)  | 50.0(0.0) | 58.0(10.3) | 60.5(9.0) | 74.0(15.6) | 
| CBF                            | 87.2(0.7) | 99.4(0.1)  | 99.5(0.3)  | 94.7(1.2)  | 33.2(0.1)  | 33.2(0.1) | 82.0(20.5) | 95.7(1.0) | 89.0(4.9)  | 
| Car                            | 76.7(2.6) | 90.5(1.4)  | 92.5(1.4)  | 75.8(2.0)  | 24.0(2.7)  | 31.7(0.0) | 73.0(3.0)  | 78.2(1.2) | 78.3(4.0)  | 
| ChlorineConcentration          | 80.2(1.1) | 81.4(0.9)  | 84.4(1.0)  | 57.3(1.1)  | 53.3(0.0)  | 53.3(0.0) | 64.3(3.8)  | 60.0(0.8) | 55.3(0.3)  | 
| CinC\_ECG\_torso                 | 84.0(1.0) | 82.4(1.2)  | 82.6(2.4)  | 91.1(2.7)  | 38.1(28.0) | 25.0(0.1) | 73.6(15.2) | 74.5(4.9) | 30.0(2.9)  | 
| Coffee                         | 99.6(1.1) | 100.0(0.0) | 100.0(0.0) | 97.9(1.8)  | 51.4(3.5)  | 53.6(0.0) | 98.2(2.5)  | 99.6(1.1) | 97.1(2.8)  | 
| Computers                      | 56.3(1.6) | 82.2(1.0)  | 81.5(1.2)  | 57.4(2.2)  | 52.2(4.8)  | 50.0(0.0) | 55.9(3.3)  | 54.8(1.5) | 62.9(4.1)  | 
| Cricket\_X                      | 59.1(1.1) | 79.2(0.7)  | 79.1(0.6)  | 69.4(1.6)  | 18.9(23.8) | 7.4(0.0)  | 49.5(5.3)  | 55.2(2.9) | 62.2(2.1)  | 
| Cricket\_Y                      | 60.0(0.8) | 78.7(1.2)  | 80.3(0.8)  | 67.5(1.0)  | 18.4(22.0) | 8.5(0.0)  | 49.7(4.3)  | 57.0(2.4) | 65.6(1.3)  | 
| Cricket\_Z                      | 61.7(0.8) | 81.1(1.0)  | 81.2(1.4)  | 69.2(1.0)  | 18.3(24.4) | 6.2(0.0)  | 49.8(3.6)  | 48.8(2.8) | 62.2(2.3)  | 
| DiatomSizeReduction            | 91.0(1.4) | 31.3(3.6)  | 30.1(0.2)  | 91.3(1.8)  | 30.1(0.7)  | 30.1(0.0) | 70.3(28.9) | 95.4(0.7) | 88.0(6.6)  | 
| DistalPhalanxOutlineAgeGroup   | 65.7(1.1) | 71.0(1.3)  | 71.7(1.3)  | 73.7(1.6)  | 46.8(0.0)  | 44.6(2.3) | 74.4(2.2)  | 75.2(1.4) | 71.0(2.1)  | 
| DistalPhalanxOutlineCorrect    | 72.6(1.3) | 76.0(1.5)  | 77.1(1.0)  | 74.1(1.4)  | 58.3(0.0)  | 58.3(0.0) | 75.3(1.8)  | 75.9(2.0) | 71.3(1.0)  | 
| DistalPhalanxTW                | 61.7(1.3) | 69.0(2.1)  | 66.5(1.6)  | 68.8(1.6)  | 30.2(0.0)  | 28.3(0.7) | 67.7(1.8)  | 67.3(2.8) | 60.9(3.0)  | 
| ECG200                         | 91.6(0.7) | 88.9(1.0)  | 87.4(1.9)  | 92.3(1.1)  | 64.0(0.0)  | 64.0(0.0) | 83.3(3.9)  | 81.4(1.3) | 84.2(5.1)  | 
| ECG5000                        | 92.9(0.1) | 94.0(0.1)  | 93.4(0.2)  | 94.0(0.2)  | 61.8(10.9) | 58.4(0.0) | 93.7(0.6)  | 92.8(0.2) | 91.9(0.2)  | 
| ECGFiveDays                    | 97.0(0.5) | 98.7(0.3)  | 97.5(1.9)  | 98.2(0.7)  | 49.9(0.3)  | 49.7(0.0) | 76.2(13.4) | 88.2(1.8) | 69.8(14.1) | 
| Earthquakes                    | 71.7(1.3) | 72.7(1.7)  | 71.2(2.0)  | 74.8(0.7)  | 74.8(0.0)  | 74.8(0.0) | 74.9(0.2)  | 70.0(1.9) | 74.8(0.0)  | 
| ElectricDevices                | 59.2(1.1) | 70.2(1.2)  | 72.9(0.9)  | 67.4(1.1)  | 33.6(19.8) | 24.2(0.0) | 64.4(1.2)  | 68.1(1.0) | 60.7(0.7)  | 
| FISH                           | 84.8(0.8) | 95.8(0.6)  | 97.9(0.8)  | 86.6(0.9)  | 13.4(1.3)  | 12.6(0.0) | 75.8(3.9)  | 84.9(0.5) | 87.5(3.4)  | 
| FaceAll                        | 79.3(1.1) | 94.5(0.9)  | 83.9(2.0)  | 79.3(0.8)  | 17.0(19.5) | 8.0(0.0)  | 71.7(2.3)  | 76.8(1.1) | 65.7(2.5)  | 
| FaceFour                       | 84.0(1.4) | 92.8(0.9)  | 95.5(0.0)  | 81.5(2.6)  | 26.8(5.7)  | 29.5(0.0) | 71.2(13.5) | 90.6(1.1) | 85.5(6.2)  | 
| FacesUCR                       | 83.3(0.3) | 94.6(0.2)  | 95.5(0.4)  | 87.4(0.4)  | 15.3(2.7)  | 14.3(0.0) | 75.6(5.1)  | 86.9(0.7) | 64.4(2.0)  | 
| FordA                          | 73.0(0.4) | 90.4(0.2)  | 92.0(0.4)  | 92.3(0.3)  | 51.3(0.0)  | 51.0(0.8) | 79.5(2.6)  | 88.1(0.7) | 52.8(2.1)  | 
| FordB                          | 60.3(0.3) | 87.8(0.6)  | 91.3(0.3)  | 89.0(0.5)  | 49.8(1.2)  | 51.2(0.0) | 53.3(2.9)  | 80.6(1.5) | 50.3(1.2)  | 
| Gun\_Point                      | 92.7(1.1) | 100.0(0.0) | 99.1(0.7)  | 93.6(3.2)  | 51.3(3.9)  | 49.3(0.0) | 86.7(9.6)  | 93.2(1.9) | 96.1(2.3)  | 
| Ham                            | 69.1(1.4) | 71.8(1.4)  | 75.7(2.7)  | 72.7(1.2)  | 50.6(1.4)  | 51.4(0.0) | 73.3(4.2)  | 71.1(2.0) | 72.3(6.3)  | 
| HandOutlines                   | 91.8(0.5) | 80.6(7.9)  | 91.1(1.4)  | 89.9(2.3)  | 64.1(0.0)  | 64.1(0.0) | 90.9(0.6)  | 88.8(1.2) | 66.0(0.7)  | 
| Haptics                        | 43.3(1.4) | 48.0(2.4)  | 51.9(1.2)  | 42.7(1.6)  | 20.9(3.5)  | 20.8(0.0) | 40.4(3.3)  | 36.6(2.4) | 40.4(4.5)  | 
| Herring                        | 52.8(3.9) | 60.8(7.7)  | 61.9(3.8)  | 58.6(4.8)  | 59.4(0.0)  | 59.4(0.0) | 60.0(5.2)  | 53.9(1.7) | 59.1(6.5)  | 
| InlineSkate                    | 33.7(1.0) | 33.9(0.8)  | 37.3(0.9)  | 29.2(0.9)  | 16.7(1.6)  | 16.5(1.1) | 21.5(2.2)  | 28.7(1.2) | 33.0(6.8)  | 
| InsectWingbeatSound            | 60.7(0.4) | 39.3(0.6)  | 50.7(0.9)  | 63.3(0.6)  | 15.8(14.2) | 9.1(0.0)  | 58.3(2.6)  | 58.3(0.6) | 43.7(2.0)  | 
| ItalyPowerDemand               | 95.4(0.2) | 96.1(0.3)  | 96.3(0.4)  | 96.5(0.5)  | 50.0(0.2)  | 49.9(0.0) | 95.5(1.9)  | 95.5(0.4) | 88.0(2.2)  | 
| LargeKitchenAppliances         | 47.3(0.6) | 90.2(0.4)  | 90.0(0.5)  | 61.9(2.6)  | 41.0(16.5) | 33.3(0.0) | 43.4(2.8)  | 66.6(5.0) | 77.9(1.8)  | 
| Lighting2                      | 67.0(2.1) | 73.9(1.4)  | 77.0(1.7)  | 69.2(4.6)  | 55.7(5.2)  | 54.1(0.0) | 63.0(5.9)  | 63.6(2.5) | 70.3(4.1)  | 
| Lighting7                      | 63.0(1.7) | 82.7(2.3)  | 84.5(2.0)  | 62.5(2.3)  | 31.0(11.3) | 26.0(0.0) | 53.4(5.9)  | 65.1(3.3) | 66.4(6.6)  | 
| MALLAT                         | 91.8(0.6) | 96.7(0.9)  | 97.2(0.3)  | 87.6(2.0)  | 13.5(3.7)  | 12.3(0.1) | 90.1(5.7)  | 92.0(0.7) | 59.6(9.8)  | 
| Meat                           | 89.7(1.7) | 85.3(6.9)  | 96.8(2.5)  | 74.2(11.0) | 33.3(0.0)  | 33.3(0.0) | 70.5(8.8)  | 90.2(1.8) | 96.8(2.0)  | 
| MedicalImages                  | 72.1(0.7) | 77.9(0.4)  | 77.0(0.7)  | 73.4(1.5)  | 51.4(0.0)  | 51.4(0.0) | 64.0(1.4)  | 67.6(1.1) | 64.9(2.7)  | 
| MiddlePhalanxOutlineAgeGroup   | 53.1(1.8) | 55.3(1.8)  | 56.9(2.1)  | 57.9(2.9)  | 18.8(0.0)  | 57.1(0.0) | 58.5(3.8)  | 56.6(1.5) | 58.1(2.6)  | 
| MiddlePhalanxOutlineCorrect    | 77.0(1.1) | 80.1(1.0)  | 80.9(1.2)  | 76.1(2.3)  | 57.0(0.0)  | 57.0(0.0) | 81.1(1.6)  | 76.6(1.3) | 74.4(2.3)  | 
| MiddlePhalanxTW                | 53.4(1.6) | 51.2(1.8)  | 48.4(2.0)  | 59.2(1.0)  | 27.3(0.0)  | 28.6(0.0) | 58.1(2.4)  | 54.9(1.7) | 53.9(2.9)  | 
| MoteStrain                     | 85.8(0.9) | 93.7(0.5)  | 92.8(0.5)  | 84.0(1.0)  | 50.8(4.0)  | 53.9(0.0) | 76.5(14.4) | 88.2(0.9) | 78.5(4.2)  | 
| NonInvasiveFatalECG\_Thorax1    | 91.6(0.4) | 95.6(0.3)  | 94.5(0.3)  | 91.6(0.4)  | 16.1(29.3) | 2.9(0.0)  | 90.5(1.2)  | 86.5(0.5) | 49.4(4.2)  | 
| NonInvasiveFatalECG\_Thorax2    | 91.7(0.3) | 95.3(0.3)  | 94.6(0.3)  | 93.2(0.9)  | 16.0(29.2) | 2.9(0.0)  | 91.5(1.5)  | 89.8(0.3) | 52.5(3.2)  | 
| OSULeaf                        | 55.7(1.0) | 97.7(0.9)  | 97.9(0.8)  | 57.6(2.0)  | 24.3(12.8) | 18.2(0.0) | 37.8(4.6)  | 46.2(2.7) | 59.5(5.4)  | 
| OliveOil                       | 66.7(3.8) | 72.3(16.6) | 83.0(8.5)  | 40.0(0.0)  | 38.0(4.2)  | 38.0(4.2) | 40.0(0.0)  | 40.0(0.0) | 79.0(6.1)  | 
| PhalangesOutlinesCorrect       | 73.5(2.1) | 82.0(0.5)  | 83.9(1.2)  | 76.7(1.4)  | 61.3(0.0)  | 61.3(0.0) | 80.3(1.1)  | 77.1(4.7) | 65.4(0.4)  | 
| Phoneme                        | 9.6(0.3)  | 32.5(0.5)  | 33.4(0.7)  | 17.2(0.8)  | 13.2(4.0)  | 11.3(0.0) | 13.0(1.0)  | 9.5(0.3)  | 12.8(1.4)  | 
| Plane                          | 97.8(0.5) | 100.0(0.0) | 100.0(0.0) | 97.6(0.8)  | 13.0(4.5)  | 13.4(1.4) | 96.5(3.2)  | 96.5(1.4) | 100.0(0.0) | 
| ProximalPhalanxOutlineAgeGroup | 85.6(0.5) | 83.1(1.3)  | 85.3(0.8)  | 84.4(1.3)  | 48.8(0.0)  | 48.8(0.0) | 83.8(0.8)  | 82.8(1.6) | 84.4(0.5)  | 
| ProximalPhalanxOutlineCorrect  | 73.3(1.8) | 90.3(0.7)  | 92.1(0.6)  | 79.1(1.8)  | 68.4(0.0)  | 68.4(0.0) | 87.3(1.8)  | 81.2(2.6) | 82.1(0.9)  | 
| ProximalPhalanxTW              | 76.7(0.7) | 76.7(0.9)  | 78.0(1.7)  | 81.2(1.1)  | 35.1(0.0)  | 34.6(1.0) | 79.7(1.3)  | 78.3(1.2) | 78.1(0.7)  | 
| RefrigerationDevices           | 37.9(2.1) | 50.8(1.0)  | 52.5(2.5)  | 48.8(1.9)  | 33.3(0.0)  | 33.3(0.0) | 36.9(3.8)  | 43.9(1.0) | 50.1(1.5)  | 
| ScreenType                     | 40.3(1.0) | 62.5(1.6)  | 62.2(1.4)  | 38.3(2.2)  | 34.1(2.4)  | 33.3(0.0) | 42.7(1.8)  | 38.9(0.9) | 43.1(4.7)  | 
| ShapeletSim                    | 50.3(3.1) | 72.4(5.6)  | 77.9(15.0) | 53.0(4.7)  | 50.0(0.0)  | 50.0(0.0) | 50.7(4.1)  | 50.0(1.3) | 61.7(10.2) | 
| ShapesAll                      | 77.1(0.5) | 89.5(0.4)  | 92.1(0.4)  | 75.8(0.9)  | 13.2(24.3) | 1.7(0.0)  | 61.3(5.3)  | 61.9(0.9) | 62.9(2.6)  | 
| SmallKitchenAppliances         | 37.1(1.9) | 78.3(1.3)  | 78.6(0.8)  | 59.6(1.8)  | 36.9(11.3) | 33.3(0.0) | 48.5(3.6)  | 61.5(2.7) | 65.6(1.9)  | 
| SonyAIBORobotSurface           | 67.2(1.3) | 96.0(0.7)  | 95.8(1.3)  | 74.3(1.9)  | 44.3(4.5)  | 42.9(0.0) | 65.3(10.9) | 68.7(2.3) | 63.8(9.9)  | 
| SonyAIBORobotSurfaceII         | 83.4(0.7) | 97.9(0.5)  | 97.8(0.5)  | 83.9(1.0)  | 59.4(7.4)  | 61.7(0.0) | 77.4(6.7)  | 84.1(1.7) | 69.7(4.3)  | 
| StarLightCurves                | 94.9(0.2) | 96.1(0.9)  | 97.2(0.3)  | 95.7(0.5)  | 65.4(16.1) | 57.7(0.0) | 93.9(1.2)  | 92.6(0.2) | 85.0(0.2)  | 
| Strawberry                     | 96.1(0.5) | 97.2(0.3)  | 98.1(0.4)  | 94.6(0.9)  | 64.3(0.0)  | 64.3(0.0) | 95.6(0.6)  | 95.9(0.3) | 89.5(2.0)  | 
| SwedishLeaf                    | 85.1(0.5) | 96.9(0.5)  | 95.6(0.4)  | 93.0(1.1)  | 11.8(13.2) | 6.5(0.4)  | 84.6(3.6)  | 88.4(1.1) | 82.5(1.4)  | 
| Symbols                        | 83.2(1.0) | 95.5(1.0)  | 90.6(2.3)  | 82.1(1.9)  | 22.6(16.9) | 17.4(0.0) | 75.6(11.5) | 81.0(0.7) | 75.0(8.8)  | 
| ToeSegmentation1               | 58.3(0.9) | 96.1(0.5)  | 96.3(0.6)  | 65.9(2.6)  | 50.5(2.7)  | 52.6(0.0) | 49.0(2.5)  | 59.5(2.2) | 86.5(3.2)  | 
| ToeSegmentation2               | 74.5(1.9) | 88.0(3.3)  | 90.6(1.7)  | 79.5(2.8)  | 63.2(30.9) | 81.5(0.0) | 44.3(15.2) | 73.8(2.8) | 84.2(4.6)  | 
| Trace                          | 80.7(0.7) | 100.0(0.0) | 100.0(0.0) | 96.0(1.8)  | 35.4(27.7) | 24.0(0.0) | 86.3(5.4)  | 95.0(2.5) | 95.9(1.9)  | 
| TwoLeadECG                     | 76.2(1.3) | 100.0(0.0) | 100.0(0.0) | 86.3(2.6)  | 50.0(0.0)  | 50.0(0.0) | 76.0(16.8) | 87.2(2.1) | 85.2(11.5) | 
| Two\_Patterns                   | 94.6(0.3) | 87.1(0.3)  | 100.0(0.0) | 100.0(0.0) | 40.3(31.1) | 25.9(0.0) | 97.8(0.6)  | 99.2(0.3) | 87.1(1.1)  | 
| UWaveGestureLibraryAll         | 95.5(0.2) | 81.7(0.3)  | 86.0(0.4)  | 95.4(0.1)  | 28.9(34.7) | 12.8(0.2) | 92.9(1.1)  | 91.8(0.4) | 55.6(2.5)  | 
| Wine                           | 56.5(7.1) | 58.7(8.3)  | 74.4(8.5)  | 50.0(0.0)  | 50.0(0.0)  | 50.0(0.0) | 50.0(0.0)  | 51.7(5.1) | 75.9(9.1)  | 
| WordsSynonyms                  | 59.8(0.8) | 56.4(1.2)  | 62.2(1.5)  | 61.3(0.9)  | 28.4(13.6) | 21.9(0.0) | 46.3(6.1)  | 56.6(0.8) | 49.0(3.0)  | 
| Worms                          | 45.7(2.4) | 76.5(2.2)  | 79.1(2.5)  | 57.1(3.7)  | 42.9(0.0)  | 42.9(0.0) | 42.6(5.5)  | 38.3(2.5) | 46.6(4.5)  | 
| WormsTwoClass                  | 60.1(1.5) | 72.6(2.7)  | 74.7(3.3)  | 63.9(4.4)  | 57.1(0.0)  | 55.7(4.5) | 57.0(1.9)  | 53.8(2.6) | 57.0(2.3)  | 
| synthetic\_control              | 97.6(0.4) | 98.5(0.3)  | 99.8(0.2)  | 99.6(0.3)  | 29.8(27.8) | 16.7(0.0) | 98.3(1.2)  | 99.0(0.4) | 87.4(1.6)  | 
| uWaveGestureLibrary\_X          | 76.7(0.3) | 75.4(0.4)  | 78.0(0.4)  | 78.6(0.4)  | 18.9(21.3) | 12.5(0.4) | 71.1(1.5)  | 71.1(1.1) | 60.6(1.5)  | 
| uWaveGestureLibrary\_Y          | 69.8(0.2) | 63.9(0.6)  | 67.0(0.7)  | 69.6(0.6)  | 23.7(24.0) | 12.1(0.0) | 63.6(1.2)  | 62.6(0.7) | 52.0(2.1)  | 
| uWaveGestureLibrary\_Z          | 69.7(0.2) | 72.6(0.5)  | 75.0(0.4)  | 71.1(0.5)  | 18.0(18.4) | 12.1(0.0) | 65.0(1.8)  | 64.2(0.9) | 56.5(2.0)  | 
| wafer                          | 99.6(0.0) | 99.7(0.0)  | 99.9(0.1)  | 99.6(0.0)  | 91.3(4.4)  | 89.2(0.0) | 99.2(0.3)  | 96.1(0.1) | 91.4(0.5)  | 
| yoga                           | 85.5(0.4) | 83.9(0.7)  | 87.0(0.9)  | 82.0(0.6)  | 53.6(0.0)  | 53.6(0.0) | 76.2(3.9)  | 78.1(0.7) | 60.7(1.9)  | 
| **Average\_Rank**               | 4.611765  | 2.682353   | 1.994118   | 3.682353   | 8.017647   | 8.417647  | 5.376471   | 4.970588  | 5.247059   | 
| **Wins**                       | 4         | 18         | 41         | 10         | 0          | 0         | 3          | 4         | 1          | 

The following table contains the averaged accuracy over 10 runs of each implemented model on the MTS archive with the standard deviation between parentheses. 

| Datasets              | MLP        | FCN        | ResNet     | Encoder    | MCNN      | t-LeNet    | MCDCNN     | Time-CNN   | TWIESN     | 
|-----------------------|------------|------------|------------|------------|-----------|------------|------------|------------|------------| 
| AUSLAN                | 93.3(0.5)  | 97.5(0.4)  | 97.4(0.3)  | 93.8(0.5)  | 1.1(0.0)  | 1.1(0.0)   | 85.4(2.7)  | 72.6(3.5)  | 72.4(1.6)  | 
| ArabicDigits          | 96.9(0.2)  | 99.4(0.1)  | 99.6(0.1)  | 98.1(0.1)  | 10.0(0.0) | 10.0(0.0)  | 95.9(0.2)  | 95.8(0.3)  | 85.3(1.4)  | 
| CMUsubject16          | 60.0(16.9) | 100.0(0.0) | 99.7(1.1)  | 98.3(2.4)  | 53.1(4.4) | 51.0(5.3)  | 51.4(5.0)  | 97.6(1.7)  | 89.3(6.8)  | 
| CharacterTrajectories | 96.9(0.2)  | 99.0(0.1)  | 99.0(0.2)  | 97.1(0.2)  | 5.4(0.8)  | 6.7(0.0)   | 93.8(1.7)  | 96.0(0.8)  | 92.0(1.3)  | 
| ECG                   | 74.8(16.2) | 87.2(1.2)  | 86.7(1.3)  | 87.2(0.8)  | 67.0(0.0) | 67.0(0.0)  | 50.0(17.9) | 84.1(1.7)  | 73.7(2.3)  | 
| JapaneseVowels        | 97.6(0.2)  | 99.3(0.2)  | 99.2(0.3)  | 97.6(0.6)  | 9.2(2.5)  | 23.8(0.0)  | 94.4(1.4)  | 95.6(1.0)  | 96.5(0.7)  | 
| KickvsPunch           | 61.0(12.9) | 54.0(13.5) | 51.0(8.8)  | 61.0(9.9)  | 54.0(9.7) | 50.0(10.5) | 56.0(8.4)  | 62.0(6.3)  | 67.0(14.2) | 
| Libras                | 78.0(1.0)  | 96.4(0.7)  | 95.4(1.1)  | 78.3(0.9)  | 6.7(0.0)  | 6.7(0.0)   | 65.1(3.9)  | 63.7(3.3)  | 79.4(1.3)  | 
| NetFlow               | 55.0(26.1) | 89.1(0.4)  | 62.7(23.4) | 77.7(0.5)  | 77.9(0.0) | 72.3(17.6) | 63.0(18.2) | 89.0(0.9)  | 94.5(0.4)  | 
| UWave                 | 90.1(0.3)  | 93.4(0.3)  | 92.6(0.4)  | 90.8(0.4)  | 12.5(0.0) | 12.5(0.0)  | 84.5(1.6)  | 85.9(0.7)  | 75.4(6.3)  | 
| Wafer                 | 89.4(0.0)  | 98.2(0.5)  | 98.9(0.4)  | 98.6(0.2)  | 89.4(0.0) | 89.4(0.0)  | 65.8(38.1) | 94.8(2.1)  | 94.9(0.6)  | 
| WalkvsRun             | 70.0(15.8) | 100.0(0.0) | 100.0(0.0) | 100.0(0.0) | 75.0(0.0) | 60.0(24.2) | 45.0(25.8) | 100.0(0.0) | 94.4(9.1)  | 
| **Average\_Rank**          | 5.208333   | 2.000000   | 2.875000   | 3.041667   | 7.583333  | 8.000000   | 6.833333   | 4.625000   | 4.833333   | 
| **Wins**                  | 0          | 5          | 3          | 0          | 0         | 0          | 0          | 0          | 2          | 

These results should give an insight of deep learning for TSC therefore encouraging researchers to consider the DNNs as robust classifiers for time series data. 

## Reference

If you re-use this work please cite:

```
@article{IsmailFawaz2018deep
  Title                    = {Deep learning for time series classification: a review}
  Author                   = {Ismail Fawaz Hassan and Forestier Germain and Weber Jonathan and Idoumghar Lhassane and Muller Pierre-Alain}
  journal                = {Data Mining and Knowledge Discovery}
  Year                     = {2019}
}
```
## Acknowledgement

We would like to thank the providers of the [UCR/UEA archive](http://timeseriesclassification.com/TSC.zip). 
We would also like to thank NVIDIA Corporation for the Quadro P6000 grant and the Mésocentre of Strasbourg for providing access to the cluster.
We would also like to thank François Petitjean and Charlotte Pelletier for the fruitful discussions their feedback and comments while writing this paper."
"Abstract:  Addressing users requests in the form of bug reports and Github issues
represents a crucial task of any successful software project. However
user-submitted issue reports tend to widely differ in their quality and
developers spend a considerable amount of time handling these reports.
Moreover an inefficient prioritization of requested changes could have a
negative impact on the developers' workloads. By collecting a dataset of around
6000 issues from the history of 323 GitHub projects we observe that
developers spend a long time (i.e. about five months on average) before
labeling an issue as a wontfix. For this reason in this paper we empirically
investigate the nature of wontfix issues by manually analyzing a sample of 800
issues of this kind extracted from heterogeneous projects. We explore the
common reasons behind a ""wontfix decision"" the main characteristics of wontfix
issues and the potential factors that could be connected with the time to close
them. Furthermore we experiment approaches for just-in-time prediction of
wontfix issues using machine learning techniques to analyze the titles and
descriptions of reported issues. Our investigation shed some light on the
wontfix issues' characteristics as well as the potential factors that may
affect the time required to make a ""wontfix decision"". Our results also
demonstrate that it is possible to predict whether an issue will be closed as a
wontfix with average values of precision recall and F-measure close to 99%
confirming the practical usefulness of the proposed approach for improving the
issue management practices on GitHub.
# Deep Learning for Time Series Classification
This is the companion repository for [our paper](https://link.springer.com/article/10.1007%2Fs10618-019-00619-1) titled ""Deep learning for time series classification: a review"" published in [Data Mining and Knowledge Discovery](https://link.springer.com/journal/10618) also available on [ArXiv](https://arxiv.org/pdf/1809.04356v3.pdf). 

![architecture resnet](https://github.com/hfawaz/dl-4-tsc/blob/master/png/resnet-archi.png)

## Data 
The data used in this project comes from two sources: 
* The [UCR/UEA archive](http://timeseriesclassification.com/TSC.zip) which contains the 85 univariate time series datasets. 
* The [MTS archive](http://www.mustafabaydogan.com/files/viewcategory/20-data-sets.html) which contains the 13 multivariate time series datasets.

## Code 
The code is divided as follows: 
* The [main.py](https://github.com/hfawaz/dl-4-tsc/blob/master/main.py) python file contains the necessary code to run an experiement. 
* The [utils](https://github.com/hfawaz/dl-4-tsc/tree/master/utils) folder contains the necessary functions to read the datasets and visualize the plots.
* The [classifiers](https://github.com/hfawaz/dl-4-tsc/tree/master/classifiers) folder contains nine python files one for each deep neural network tested in our paper. 

To run a model on one dataset you should issue the following command: 
```
python3 main.py TSC Coffee fcn _itr_8
```
which means we are launching the [fcn](https://github.com/hfawaz/dl-4-tsc/blob/master/classifiers/fcn.py) model on the univariate UCR archive for the Coffee dataset (see [constants.py](https://github.com/hfawaz/dl-4-tsc/blob/master/utils/constants.py) for a list of possible options).

## Prerequisites
All python packages needed are listed in [pip-requirements.txt](https://github.com/hfawaz/dl-4-tsc/blob/master/utils/pip-requirements.txt) file and can be installed simply using the pip command. 

* [numpy](http://www.numpy.org/)  
* [pandas](https://pandas.pydata.org/)  
* [sklearn](http://scikit-learn.org/stable/)  
* [scipy](https://www.scipy.org/)  
* [matplotlib](https://matplotlib.org/)  
* [tensorflow-gpu](https://www.tensorflow.org/)  
* [keras](https://keras.io/)  
* [h5py](http://docs.h5py.org/en/latest/build.html)
* [keras_contrib](https://www.github.com/keras-team/keras-contrib.git)

## Results
Our [results](https://github.com/hfawaz/dl-4-tsc/tree/master/results) showed that a deep residual network architecture performs best for the time series classification task. 

The following table contains the averaged accuracy over 10 runs of each implemented model on the UCR/UEA archive with the standard deviation between parentheses. 

| Datasets                       | MLP       | FCN        | ResNet     | Encoder    | MCNN       | t-LeNet   | MCDCNN     | Time-CNN  | TWIESN     | 
|--------------------------------|-----------|------------|------------|------------|------------|-----------|------------|-----------|------------| 
| 50words                        | 68.4(7.1) | 62.7(6.1)  | 74.0(1.5)  | 72.3(1.0)  | 22.0(24.3) | 12.5(0.0) | 58.9(5.3)  | 62.1(1.0) | 49.6(2.6)  | 
| Adiac                          | 39.7(1.9) | 84.4(0.7)  | 82.9(0.6)  | 48.4(2.5)  | 2.2(0.6)   | 2.0(0.0)  | 61.0(8.7)  | 37.9(2.0) | 41.6(4.5)  | 
| ArrowHead                      | 77.8(1.2) | 84.3(1.5)  | 84.5(1.2)  | 80.4(2.9)  | 33.9(4.7)  | 30.3(0.0) | 68.5(6.7)  | 72.3(2.6) | 65.9(9.4)  | 
| Beef                           | 72.0(2.8) | 69.7(4.0)  | 75.3(4.2)  | 64.3(5.0)  | 20.0(0.0)  | 20.0(0.0) | 56.3(7.8)  | 76.3(1.1) | 53.7(14.9) | 
| BeetleFly                      | 87.0(2.6) | 86.0(9.7)  | 85.0(2.4)  | 74.5(7.6)  | 50.0(0.0)  | 50.0(0.0) | 58.0(9.2)  | 89.0(3.2) | 73.0(7.9)  | 
| BirdChicken                    | 77.5(3.5) | 95.5(3.7)  | 88.5(5.3)  | 66.5(5.8)  | 50.0(0.0)  | 50.0(0.0) | 58.0(10.3) | 60.5(9.0) | 74.0(15.6) | 
| CBF                            | 87.2(0.7) | 99.4(0.1)  | 99.5(0.3)  | 94.7(1.2)  | 33.2(0.1)  | 33.2(0.1) | 82.0(20.5) | 95.7(1.0) | 89.0(4.9)  | 
| Car                            | 76.7(2.6) | 90.5(1.4)  | 92.5(1.4)  | 75.8(2.0)  | 24.0(2.7)  | 31.7(0.0) | 73.0(3.0)  | 78.2(1.2) | 78.3(4.0)  | 
| ChlorineConcentration          | 80.2(1.1) | 81.4(0.9)  | 84.4(1.0)  | 57.3(1.1)  | 53.3(0.0)  | 53.3(0.0) | 64.3(3.8)  | 60.0(0.8) | 55.3(0.3)  | 
| CinC\_ECG\_torso                 | 84.0(1.0) | 82.4(1.2)  | 82.6(2.4)  | 91.1(2.7)  | 38.1(28.0) | 25.0(0.1) | 73.6(15.2) | 74.5(4.9) | 30.0(2.9)  | 
| Coffee                         | 99.6(1.1) | 100.0(0.0) | 100.0(0.0) | 97.9(1.8)  | 51.4(3.5)  | 53.6(0.0) | 98.2(2.5)  | 99.6(1.1) | 97.1(2.8)  | 
| Computers                      | 56.3(1.6) | 82.2(1.0)  | 81.5(1.2)  | 57.4(2.2)  | 52.2(4.8)  | 50.0(0.0) | 55.9(3.3)  | 54.8(1.5) | 62.9(4.1)  | 
| Cricket\_X                      | 59.1(1.1) | 79.2(0.7)  | 79.1(0.6)  | 69.4(1.6)  | 18.9(23.8) | 7.4(0.0)  | 49.5(5.3)  | 55.2(2.9) | 62.2(2.1)  | 
| Cricket\_Y                      | 60.0(0.8) | 78.7(1.2)  | 80.3(0.8)  | 67.5(1.0)  | 18.4(22.0) | 8.5(0.0)  | 49.7(4.3)  | 57.0(2.4) | 65.6(1.3)  | 
| Cricket\_Z                      | 61.7(0.8) | 81.1(1.0)  | 81.2(1.4)  | 69.2(1.0)  | 18.3(24.4) | 6.2(0.0)  | 49.8(3.6)  | 48.8(2.8) | 62.2(2.3)  | 
| DiatomSizeReduction            | 91.0(1.4) | 31.3(3.6)  | 30.1(0.2)  | 91.3(1.8)  | 30.1(0.7)  | 30.1(0.0) | 70.3(28.9) | 95.4(0.7) | 88.0(6.6)  | 
| DistalPhalanxOutlineAgeGroup   | 65.7(1.1) | 71.0(1.3)  | 71.7(1.3)  | 73.7(1.6)  | 46.8(0.0)  | 44.6(2.3) | 74.4(2.2)  | 75.2(1.4) | 71.0(2.1)  | 
| DistalPhalanxOutlineCorrect    | 72.6(1.3) | 76.0(1.5)  | 77.1(1.0)  | 74.1(1.4)  | 58.3(0.0)  | 58.3(0.0) | 75.3(1.8)  | 75.9(2.0) | 71.3(1.0)  | 
| DistalPhalanxTW                | 61.7(1.3) | 69.0(2.1)  | 66.5(1.6)  | 68.8(1.6)  | 30.2(0.0)  | 28.3(0.7) | 67.7(1.8)  | 67.3(2.8) | 60.9(3.0)  | 
| ECG200                         | 91.6(0.7) | 88.9(1.0)  | 87.4(1.9)  | 92.3(1.1)  | 64.0(0.0)  | 64.0(0.0) | 83.3(3.9)  | 81.4(1.3) | 84.2(5.1)  | 
| ECG5000                        | 92.9(0.1) | 94.0(0.1)  | 93.4(0.2)  | 94.0(0.2)  | 61.8(10.9) | 58.4(0.0) | 93.7(0.6)  | 92.8(0.2) | 91.9(0.2)  | 
| ECGFiveDays                    | 97.0(0.5) | 98.7(0.3)  | 97.5(1.9)  | 98.2(0.7)  | 49.9(0.3)  | 49.7(0.0) | 76.2(13.4) | 88.2(1.8) | 69.8(14.1) | 
| Earthquakes                    | 71.7(1.3) | 72.7(1.7)  | 71.2(2.0)  | 74.8(0.7)  | 74.8(0.0)  | 74.8(0.0) | 74.9(0.2)  | 70.0(1.9) | 74.8(0.0)  | 
| ElectricDevices                | 59.2(1.1) | 70.2(1.2)  | 72.9(0.9)  | 67.4(1.1)  | 33.6(19.8) | 24.2(0.0) | 64.4(1.2)  | 68.1(1.0) | 60.7(0.7)  | 
| FISH                           | 84.8(0.8) | 95.8(0.6)  | 97.9(0.8)  | 86.6(0.9)  | 13.4(1.3)  | 12.6(0.0) | 75.8(3.9)  | 84.9(0.5) | 87.5(3.4)  | 
| FaceAll                        | 79.3(1.1) | 94.5(0.9)  | 83.9(2.0)  | 79.3(0.8)  | 17.0(19.5) | 8.0(0.0)  | 71.7(2.3)  | 76.8(1.1) | 65.7(2.5)  | 
| FaceFour                       | 84.0(1.4) | 92.8(0.9)  | 95.5(0.0)  | 81.5(2.6)  | 26.8(5.7)  | 29.5(0.0) | 71.2(13.5) | 90.6(1.1) | 85.5(6.2)  | 
| FacesUCR                       | 83.3(0.3) | 94.6(0.2)  | 95.5(0.4)  | 87.4(0.4)  | 15.3(2.7)  | 14.3(0.0) | 75.6(5.1)  | 86.9(0.7) | 64.4(2.0)  | 
| FordA                          | 73.0(0.4) | 90.4(0.2)  | 92.0(0.4)  | 92.3(0.3)  | 51.3(0.0)  | 51.0(0.8) | 79.5(2.6)  | 88.1(0.7) | 52.8(2.1)  | 
| FordB                          | 60.3(0.3) | 87.8(0.6)  | 91.3(0.3)  | 89.0(0.5)  | 49.8(1.2)  | 51.2(0.0) | 53.3(2.9)  | 80.6(1.5) | 50.3(1.2)  | 
| Gun\_Point                      | 92.7(1.1) | 100.0(0.0) | 99.1(0.7)  | 93.6(3.2)  | 51.3(3.9)  | 49.3(0.0) | 86.7(9.6)  | 93.2(1.9) | 96.1(2.3)  | 
| Ham                            | 69.1(1.4) | 71.8(1.4)  | 75.7(2.7)  | 72.7(1.2)  | 50.6(1.4)  | 51.4(0.0) | 73.3(4.2)  | 71.1(2.0) | 72.3(6.3)  | 
| HandOutlines                   | 91.8(0.5) | 80.6(7.9)  | 91.1(1.4)  | 89.9(2.3)  | 64.1(0.0)  | 64.1(0.0) | 90.9(0.6)  | 88.8(1.2) | 66.0(0.7)  | 
| Haptics                        | 43.3(1.4) | 48.0(2.4)  | 51.9(1.2)  | 42.7(1.6)  | 20.9(3.5)  | 20.8(0.0) | 40.4(3.3)  | 36.6(2.4) | 40.4(4.5)  | 
| Herring                        | 52.8(3.9) | 60.8(7.7)  | 61.9(3.8)  | 58.6(4.8)  | 59.4(0.0)  | 59.4(0.0) | 60.0(5.2)  | 53.9(1.7) | 59.1(6.5)  | 
| InlineSkate                    | 33.7(1.0) | 33.9(0.8)  | 37.3(0.9)  | 29.2(0.9)  | 16.7(1.6)  | 16.5(1.1) | 21.5(2.2)  | 28.7(1.2) | 33.0(6.8)  | 
| InsectWingbeatSound            | 60.7(0.4) | 39.3(0.6)  | 50.7(0.9)  | 63.3(0.6)  | 15.8(14.2) | 9.1(0.0)  | 58.3(2.6)  | 58.3(0.6) | 43.7(2.0)  | 
| ItalyPowerDemand               | 95.4(0.2) | 96.1(0.3)  | 96.3(0.4)  | 96.5(0.5)  | 50.0(0.2)  | 49.9(0.0) | 95.5(1.9)  | 95.5(0.4) | 88.0(2.2)  | 
| LargeKitchenAppliances         | 47.3(0.6) | 90.2(0.4)  | 90.0(0.5)  | 61.9(2.6)  | 41.0(16.5) | 33.3(0.0) | 43.4(2.8)  | 66.6(5.0) | 77.9(1.8)  | 
| Lighting2                      | 67.0(2.1) | 73.9(1.4)  | 77.0(1.7)  | 69.2(4.6)  | 55.7(5.2)  | 54.1(0.0) | 63.0(5.9)  | 63.6(2.5) | 70.3(4.1)  | 
| Lighting7                      | 63.0(1.7) | 82.7(2.3)  | 84.5(2.0)  | 62.5(2.3)  | 31.0(11.3) | 26.0(0.0) | 53.4(5.9)  | 65.1(3.3) | 66.4(6.6)  | 
| MALLAT                         | 91.8(0.6) | 96.7(0.9)  | 97.2(0.3)  | 87.6(2.0)  | 13.5(3.7)  | 12.3(0.1) | 90.1(5.7)  | 92.0(0.7) | 59.6(9.8)  | 
| Meat                           | 89.7(1.7) | 85.3(6.9)  | 96.8(2.5)  | 74.2(11.0) | 33.3(0.0)  | 33.3(0.0) | 70.5(8.8)  | 90.2(1.8) | 96.8(2.0)  | 
| MedicalImages                  | 72.1(0.7) | 77.9(0.4)  | 77.0(0.7)  | 73.4(1.5)  | 51.4(0.0)  | 51.4(0.0) | 64.0(1.4)  | 67.6(1.1) | 64.9(2.7)  | 
| MiddlePhalanxOutlineAgeGroup   | 53.1(1.8) | 55.3(1.8)  | 56.9(2.1)  | 57.9(2.9)  | 18.8(0.0)  | 57.1(0.0) | 58.5(3.8)  | 56.6(1.5) | 58.1(2.6)  | 
| MiddlePhalanxOutlineCorrect    | 77.0(1.1) | 80.1(1.0)  | 80.9(1.2)  | 76.1(2.3)  | 57.0(0.0)  | 57.0(0.0) | 81.1(1.6)  | 76.6(1.3) | 74.4(2.3)  | 
| MiddlePhalanxTW                | 53.4(1.6) | 51.2(1.8)  | 48.4(2.0)  | 59.2(1.0)  | 27.3(0.0)  | 28.6(0.0) | 58.1(2.4)  | 54.9(1.7) | 53.9(2.9)  | 
| MoteStrain                     | 85.8(0.9) | 93.7(0.5)  | 92.8(0.5)  | 84.0(1.0)  | 50.8(4.0)  | 53.9(0.0) | 76.5(14.4) | 88.2(0.9) | 78.5(4.2)  | 
| NonInvasiveFatalECG\_Thorax1    | 91.6(0.4) | 95.6(0.3)  | 94.5(0.3)  | 91.6(0.4)  | 16.1(29.3) | 2.9(0.0)  | 90.5(1.2)  | 86.5(0.5) | 49.4(4.2)  | 
| NonInvasiveFatalECG\_Thorax2    | 91.7(0.3) | 95.3(0.3)  | 94.6(0.3)  | 93.2(0.9)  | 16.0(29.2) | 2.9(0.0)  | 91.5(1.5)  | 89.8(0.3) | 52.5(3.2)  | 
| OSULeaf                        | 55.7(1.0) | 97.7(0.9)  | 97.9(0.8)  | 57.6(2.0)  | 24.3(12.8) | 18.2(0.0) | 37.8(4.6)  | 46.2(2.7) | 59.5(5.4)  | 
| OliveOil                       | 66.7(3.8) | 72.3(16.6) | 83.0(8.5)  | 40.0(0.0)  | 38.0(4.2)  | 38.0(4.2) | 40.0(0.0)  | 40.0(0.0) | 79.0(6.1)  | 
| PhalangesOutlinesCorrect       | 73.5(2.1) | 82.0(0.5)  | 83.9(1.2)  | 76.7(1.4)  | 61.3(0.0)  | 61.3(0.0) | 80.3(1.1)  | 77.1(4.7) | 65.4(0.4)  | 
| Phoneme                        | 9.6(0.3)  | 32.5(0.5)  | 33.4(0.7)  | 17.2(0.8)  | 13.2(4.0)  | 11.3(0.0) | 13.0(1.0)  | 9.5(0.3)  | 12.8(1.4)  | 
| Plane                          | 97.8(0.5) | 100.0(0.0) | 100.0(0.0) | 97.6(0.8)  | 13.0(4.5)  | 13.4(1.4) | 96.5(3.2)  | 96.5(1.4) | 100.0(0.0) | 
| ProximalPhalanxOutlineAgeGroup | 85.6(0.5) | 83.1(1.3)  | 85.3(0.8)  | 84.4(1.3)  | 48.8(0.0)  | 48.8(0.0) | 83.8(0.8)  | 82.8(1.6) | 84.4(0.5)  | 
| ProximalPhalanxOutlineCorrect  | 73.3(1.8) | 90.3(0.7)  | 92.1(0.6)  | 79.1(1.8)  | 68.4(0.0)  | 68.4(0.0) | 87.3(1.8)  | 81.2(2.6) | 82.1(0.9)  | 
| ProximalPhalanxTW              | 76.7(0.7) | 76.7(0.9)  | 78.0(1.7)  | 81.2(1.1)  | 35.1(0.0)  | 34.6(1.0) | 79.7(1.3)  | 78.3(1.2) | 78.1(0.7)  | 
| RefrigerationDevices           | 37.9(2.1) | 50.8(1.0)  | 52.5(2.5)  | 48.8(1.9)  | 33.3(0.0)  | 33.3(0.0) | 36.9(3.8)  | 43.9(1.0) | 50.1(1.5)  | 
| ScreenType                     | 40.3(1.0) | 62.5(1.6)  | 62.2(1.4)  | 38.3(2.2)  | 34.1(2.4)  | 33.3(0.0) | 42.7(1.8)  | 38.9(0.9) | 43.1(4.7)  | 
| ShapeletSim                    | 50.3(3.1) | 72.4(5.6)  | 77.9(15.0) | 53.0(4.7)  | 50.0(0.0)  | 50.0(0.0) | 50.7(4.1)  | 50.0(1.3) | 61.7(10.2) | 
| ShapesAll                      | 77.1(0.5) | 89.5(0.4)  | 92.1(0.4)  | 75.8(0.9)  | 13.2(24.3) | 1.7(0.0)  | 61.3(5.3)  | 61.9(0.9) | 62.9(2.6)  | 
| SmallKitchenAppliances         | 37.1(1.9) | 78.3(1.3)  | 78.6(0.8)  | 59.6(1.8)  | 36.9(11.3) | 33.3(0.0) | 48.5(3.6)  | 61.5(2.7) | 65.6(1.9)  | 
| SonyAIBORobotSurface           | 67.2(1.3) | 96.0(0.7)  | 95.8(1.3)  | 74.3(1.9)  | 44.3(4.5)  | 42.9(0.0) | 65.3(10.9) | 68.7(2.3) | 63.8(9.9)  | 
| SonyAIBORobotSurfaceII         | 83.4(0.7) | 97.9(0.5)  | 97.8(0.5)  | 83.9(1.0)  | 59.4(7.4)  | 61.7(0.0) | 77.4(6.7)  | 84.1(1.7) | 69.7(4.3)  | 
| StarLightCurves                | 94.9(0.2) | 96.1(0.9)  | 97.2(0.3)  | 95.7(0.5)  | 65.4(16.1) | 57.7(0.0) | 93.9(1.2)  | 92.6(0.2) | 85.0(0.2)  | 
| Strawberry                     | 96.1(0.5) | 97.2(0.3)  | 98.1(0.4)  | 94.6(0.9)  | 64.3(0.0)  | 64.3(0.0) | 95.6(0.6)  | 95.9(0.3) | 89.5(2.0)  | 
| SwedishLeaf                    | 85.1(0.5) | 96.9(0.5)  | 95.6(0.4)  | 93.0(1.1)  | 11.8(13.2) | 6.5(0.4)  | 84.6(3.6)  | 88.4(1.1) | 82.5(1.4)  | 
| Symbols                        | 83.2(1.0) | 95.5(1.0)  | 90.6(2.3)  | 82.1(1.9)  | 22.6(16.9) | 17.4(0.0) | 75.6(11.5) | 81.0(0.7) | 75.0(8.8)  | 
| ToeSegmentation1               | 58.3(0.9) | 96.1(0.5)  | 96.3(0.6)  | 65.9(2.6)  | 50.5(2.7)  | 52.6(0.0) | 49.0(2.5)  | 59.5(2.2) | 86.5(3.2)  | 
| ToeSegmentation2               | 74.5(1.9) | 88.0(3.3)  | 90.6(1.7)  | 79.5(2.8)  | 63.2(30.9) | 81.5(0.0) | 44.3(15.2) | 73.8(2.8) | 84.2(4.6)  | 
| Trace                          | 80.7(0.7) | 100.0(0.0) | 100.0(0.0) | 96.0(1.8)  | 35.4(27.7) | 24.0(0.0) | 86.3(5.4)  | 95.0(2.5) | 95.9(1.9)  | 
| TwoLeadECG                     | 76.2(1.3) | 100.0(0.0) | 100.0(0.0) | 86.3(2.6)  | 50.0(0.0)  | 50.0(0.0) | 76.0(16.8) | 87.2(2.1) | 85.2(11.5) | 
| Two\_Patterns                   | 94.6(0.3) | 87.1(0.3)  | 100.0(0.0) | 100.0(0.0) | 40.3(31.1) | 25.9(0.0) | 97.8(0.6)  | 99.2(0.3) | 87.1(1.1)  | 
| UWaveGestureLibraryAll         | 95.5(0.2) | 81.7(0.3)  | 86.0(0.4)  | 95.4(0.1)  | 28.9(34.7) | 12.8(0.2) | 92.9(1.1)  | 91.8(0.4) | 55.6(2.5)  | 
| Wine                           | 56.5(7.1) | 58.7(8.3)  | 74.4(8.5)  | 50.0(0.0)  | 50.0(0.0)  | 50.0(0.0) | 50.0(0.0)  | 51.7(5.1) | 75.9(9.1)  | 
| WordsSynonyms                  | 59.8(0.8) | 56.4(1.2)  | 62.2(1.5)  | 61.3(0.9)  | 28.4(13.6) | 21.9(0.0) | 46.3(6.1)  | 56.6(0.8) | 49.0(3.0)  | 
| Worms                          | 45.7(2.4) | 76.5(2.2)  | 79.1(2.5)  | 57.1(3.7)  | 42.9(0.0)  | 42.9(0.0) | 42.6(5.5)  | 38.3(2.5) | 46.6(4.5)  | 
| WormsTwoClass                  | 60.1(1.5) | 72.6(2.7)  | 74.7(3.3)  | 63.9(4.4)  | 57.1(0.0)  | 55.7(4.5) | 57.0(1.9)  | 53.8(2.6) | 57.0(2.3)  | 
| synthetic\_control              | 97.6(0.4) | 98.5(0.3)  | 99.8(0.2)  | 99.6(0.3)  | 29.8(27.8) | 16.7(0.0) | 98.3(1.2)  | 99.0(0.4) | 87.4(1.6)  | 
| uWaveGestureLibrary\_X          | 76.7(0.3) | 75.4(0.4)  | 78.0(0.4)  | 78.6(0.4)  | 18.9(21.3) | 12.5(0.4) | 71.1(1.5)  | 71.1(1.1) | 60.6(1.5)  | 
| uWaveGestureLibrary\_Y          | 69.8(0.2) | 63.9(0.6)  | 67.0(0.7)  | 69.6(0.6)  | 23.7(24.0) | 12.1(0.0) | 63.6(1.2)  | 62.6(0.7) | 52.0(2.1)  | 
| uWaveGestureLibrary\_Z          | 69.7(0.2) | 72.6(0.5)  | 75.0(0.4)  | 71.1(0.5)  | 18.0(18.4) | 12.1(0.0) | 65.0(1.8)  | 64.2(0.9) | 56.5(2.0)  | 
| wafer                          | 99.6(0.0) | 99.7(0.0)  | 99.9(0.1)  | 99.6(0.0)  | 91.3(4.4)  | 89.2(0.0) | 99.2(0.3)  | 96.1(0.1) | 91.4(0.5)  | 
| yoga                           | 85.5(0.4) | 83.9(0.7)  | 87.0(0.9)  | 82.0(0.6)  | 53.6(0.0)  | 53.6(0.0) | 76.2(3.9)  | 78.1(0.7) | 60.7(1.9)  | 
| **Average\_Rank**               | 4.611765  | 2.682353   | 1.994118   | 3.682353   | 8.017647   | 8.417647  | 5.376471   | 4.970588  | 5.247059   | 
| **Wins**                       | 4         | 18         | 41         | 10         | 0          | 0         | 3          | 4         | 1          | 

The following table contains the averaged accuracy over 10 runs of each implemented model on the MTS archive with the standard deviation between parentheses. 

| Datasets              | MLP        | FCN        | ResNet     | Encoder    | MCNN      | t-LeNet    | MCDCNN     | Time-CNN   | TWIESN     | 
|-----------------------|------------|------------|------------|------------|-----------|------------|------------|------------|------------| 
| AUSLAN                | 93.3(0.5)  | 97.5(0.4)  | 97.4(0.3)  | 93.8(0.5)  | 1.1(0.0)  | 1.1(0.0)   | 85.4(2.7)  | 72.6(3.5)  | 72.4(1.6)  | 
| ArabicDigits          | 96.9(0.2)  | 99.4(0.1)  | 99.6(0.1)  | 98.1(0.1)  | 10.0(0.0) | 10.0(0.0)  | 95.9(0.2)  | 95.8(0.3)  | 85.3(1.4)  | 
| CMUsubject16          | 60.0(16.9) | 100.0(0.0) | 99.7(1.1)  | 98.3(2.4)  | 53.1(4.4) | 51.0(5.3)  | 51.4(5.0)  | 97.6(1.7)  | 89.3(6.8)  | 
| CharacterTrajectories | 96.9(0.2)  | 99.0(0.1)  | 99.0(0.2)  | 97.1(0.2)  | 5.4(0.8)  | 6.7(0.0)   | 93.8(1.7)  | 96.0(0.8)  | 92.0(1.3)  | 
| ECG                   | 74.8(16.2) | 87.2(1.2)  | 86.7(1.3)  | 87.2(0.8)  | 67.0(0.0) | 67.0(0.0)  | 50.0(17.9) | 84.1(1.7)  | 73.7(2.3)  | 
| JapaneseVowels        | 97.6(0.2)  | 99.3(0.2)  | 99.2(0.3)  | 97.6(0.6)  | 9.2(2.5)  | 23.8(0.0)  | 94.4(1.4)  | 95.6(1.0)  | 96.5(0.7)  | 
| KickvsPunch           | 61.0(12.9) | 54.0(13.5) | 51.0(8.8)  | 61.0(9.9)  | 54.0(9.7) | 50.0(10.5) | 56.0(8.4)  | 62.0(6.3)  | 67.0(14.2) | 
| Libras                | 78.0(1.0)  | 96.4(0.7)  | 95.4(1.1)  | 78.3(0.9)  | 6.7(0.0)  | 6.7(0.0)   | 65.1(3.9)  | 63.7(3.3)  | 79.4(1.3)  | 
| NetFlow               | 55.0(26.1) | 89.1(0.4)  | 62.7(23.4) | 77.7(0.5)  | 77.9(0.0) | 72.3(17.6) | 63.0(18.2) | 89.0(0.9)  | 94.5(0.4)  | 
| UWave                 | 90.1(0.3)  | 93.4(0.3)  | 92.6(0.4)  | 90.8(0.4)  | 12.5(0.0) | 12.5(0.0)  | 84.5(1.6)  | 85.9(0.7)  | 75.4(6.3)  | 
| Wafer                 | 89.4(0.0)  | 98.2(0.5)  | 98.9(0.4)  | 98.6(0.2)  | 89.4(0.0) | 89.4(0.0)  | 65.8(38.1) | 94.8(2.1)  | 94.9(0.6)  | 
| WalkvsRun             | 70.0(15.8) | 100.0(0.0) | 100.0(0.0) | 100.0(0.0) | 75.0(0.0) | 60.0(24.2) | 45.0(25.8) | 100.0(0.0) | 94.4(9.1)  | 
| **Average\_Rank**          | 5.208333   | 2.000000   | 2.875000   | 3.041667   | 7.583333  | 8.000000   | 6.833333   | 4.625000   | 4.833333   | 
| **Wins**                  | 0          | 5          | 3          | 0          | 0         | 0          | 0          | 0          | 2          | 

These results should give an insight of deep learning for TSC therefore encouraging researchers to consider the DNNs as robust classifiers for time series data. 

## Reference

If you re-use this work please cite:

```
@article{IsmailFawaz2018deep
  Title                    = {Deep learning for time series classification: a review}
  Author                   = {Ismail Fawaz Hassan and Forestier Germain and Weber Jonathan and Idoumghar Lhassane and Muller Pierre-Alain}
  journal                = {Data Mining and Knowledge Discovery}
  Year                     = {2019}
}
```
## Acknowledgement

We would like to thank the providers of the [UCR/UEA archive](http://timeseriesclassification.com/TSC.zip). 
We would also like to thank NVIDIA Corporation for the Quadro P6000 grant and the Mésocentre of Strasbourg for providing access to the cluster.
We would also like to thank François Petitjean and Charlotte Pelletier for the fruitful discussions their feedback and comments while writing this paper."
"Abstract:  This paper addresses the challenge of dense pixel correspondence estimation
between two images. This problem is closely related to optical flow estimation
task where ConvNets (CNNs) have recently achieved significant progress. While
optical flow methods produce very accurate results for the small pixel
translation and limited appearance variation scenarios they hardly deal with
the strong geometric transformations that we consider in this work. In this
paper we propose a coarse-to-fine CNN-based framework that can leverage the
advantages of optical flow approaches and extend them to the case of large
transformations providing dense and subpixel accurate estimates. It is trained
on synthetic transformations and demonstrates very good performance to unseen
realistic data. Further we apply our method to the problem of relative camera
pose estimation and demonstrate that the model outperforms existing dense
approaches.
# DGC-Net: Dense Geometric Correspondence Network
This is a PyTorch implementation of our work [""DGC-Net: Dense Geometric Correspondence Network""](https://arxiv.org/abs/1810.08393)

**TL;DR** A CNN-based approach to obtain dense pixel correspondences between two views.

## Installation
- create and activate conda environment with Python 3.x
```
conda create -n my_fancy_env python=3.7
source activate my_fancy_env
```
- install Pytorch v1.0.0 and torchvision library
```
pip install torch torchvision
```
- install all dependencies by running the following command:
```
pip install -r requirements.txt
```

## Getting started
* ```eval.py``` demonstrates the results on the HPatches dataset
To be able to run ```eval.py``` script:
    * Download an archive with pre-trained models [click](https://drive.google.com/file/d/1p1FarlU5byWez_mQC68DZ_eRQKfF9IIf/view?usp=sharing) and extract it
to the project folder
    * Download HPatches dataset (Full image sequences). The dataset is available [here](https://github.com/hpatches/hpatches-dataset) at the end of the page
    * Run the following command:
    ```
    python eval.py --image-data-path /path/to/hpatches-geometry
    ```

* ```train.py``` is a script to train DGC-Net/DGCM-Net model from scratch. To run this script please follow the next procedure:
    * Download the [TokyoTimeMachine dataset](https://www.di.ens.fr/willow/research/netvlad/)
    * Run the command:
    ```
    python train.py --image-data-path /path/to/TokyoTimeMachine
    ```

## Performance on [HPatches](https://github.com/hpatches/hpatches-dataset) dataset
Method / HPatches ID|Viewpoint 1|Viewpoint 2|Viewpoint 3|Viewpoint 4|Viewpoint 5
:---|:---:|:---:|:---:|:---:|:---:
[PWC-Net](https://arxiv.org/abs/1709.02371)| 4.43 | 11.44 | 15.47 | 20.17 | 28.30
[GM](https://arxiv.org/abs/1703.05593) best model | 9.59 | 18.55 | 21.15 | 27.83 | 35.19
DGC-Net (paper) | **1.55** | **5.53** | **8.98** | 11.66 | 16.70
DGCM-Net (paper) | 2.97 | 6.85 | 9.95 | 12.87 | 19.13
DGC-Net (repo) | 1.74 | 5.88 | 9.07 | 12.14 | 16.50
DGCM-Net (repo) | 2.33 | 5.62 | 9.55 | **11.59** | **16.48**

Note: There is a difference in numbers presented in the original paper and obtained by the models of this repo. It might be related to the fact that both models (DGC-Net and DGCM-Net) have been trained using ```Pytorch v0.3```.

More qualitative results are presented on the [project page](https://aaltovision.github.io/dgc-net-site/)

## How to cite
If you use this software in your own research please cite our publication:

```
@inproceedings{Melekhov+Tiulpin+Sattler+Pollefeys+Rahtu+Kannala:2018
      title = {{DGC-Net}: Dense geometric correspondence network}
      author = {Melekhov Iaroslav and Tiulpin Aleksei and 
               Sattler Torsten and 
               Pollefeys Marc and 
               Rahtu Esa and Kannala Juho}
       year = {2019}
       booktitle = {Proceedings of the IEEE Winter Conference on 
                    Applications of Computer Vision (WACV)}
}
```"
"Abstract:  Revealing latent structure in data is an active field of research having
introduced exciting technologies such as variational autoencoders and
adversarial networks and is essential to push machine learning towards
unsupervised knowledge discovery. However a major challenge is the lack of
suitable benchmarks for an objective and quantitative evaluation of learned
representations. To address this issue we introduce Morpho-MNIST a framework
that aims to answer: ""to what extent has my model learned to represent specific
factors of variation in the data?"" We extend the popular MNIST dataset by
adding a morphometric analysis enabling quantitative comparison of trained
models identification of the roles of latent variables and characterisation
of sample diversity. We further propose a set of quantifiable perturbations to
assess the performance of unsupervised and supervised methods on challenging
tasks such as outlier detection and domain adaptation. Data and code are
available at this https URL.
# Morpho-MNIST

![Morpho-MNIST morphometrics and perturbations](fig1.png)

[_Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation Learning_](https://arxiv.org/abs/1809.10780)

&gt; Revealing latent structure in data is an active field of research having brought exciting new models such as variational autoencoders and generative adversarial networks and is essential to push machine learning towards unsupervised knowledge discovery. However a major challenge is the lack of suitable benchmarks for an objective and quantitative evaluation of learned representations. To address this issue we introduce Morpho-MNIST. We extend the popular MNIST dataset by adding a morphometric analysis enabling quantitative comparison of different models identification of the roles of latent variables and characterisation of sample diversity. We further propose a set of quantifiable perturbations to assess the performance of unsupervised and supervised methods on challenging tasks such as outlier detection and domain adaptation.

If you use these tools or datasets in your publications please consider citing the accompanying paper with a BibTeX entry similar to the following:

```
@unpublished{castro2018morphomnist
    author = {Castro Daniel C. and Tan Jeremy and Kainz Bernhard and Konukoglu Ender and Glocker Ben}
    title = {{Morpho-MNIST}: Quantitative Assessment and Diagnostics for Representation Learning}
    year = {2018}
    eprint = {arXiv:1809.10780}
    url = {https://arxiv.org/abs/1809.10780}
}
```

**Table of Contents**

- [Morpho-MNIST](#morpho-mnist)
    - [Datasets](#datasets)
    - [API Overview](#api-overview)
        - [`morphomnist.io`](#morphomnistio)
        - [`morphomnist.measure`](#morphomnistmeasure)
        - [`morphomnist.perturb`](#morphomnistperturb)

## Datasets

We distribute the datasets in `.zip` files containing:

- `[train|t10k]-images-idx3-ubyte.gz`: images
- `[train|t10k]-labels-idx1-ubyte.gz`: digit labels copied from original MNIST
- `[train|t10k]-morpho.csv`: morphometrics table with columns:
    - `index`: index of the corresponding digit (for convenience although rows are written in order)
    - `area` (pxÂ²) `length` (px) `thickness` (px) `slant` (rad) `width` (px) `height` (px): calculated morphometrics
- `[train|t10k]-pert-idx1-ubyte.gz` (`global` and `local` datasets): perturbation labels
    - `0`: plain; `1`: thinned; `2`: thickened; `3`: swollen; `4`: fractured.
- `README-xxxx.txt`: similar information to the above for offline reference

Here are the downloads for the datasets used in the experiments in the paper:

| Dataset  | Description               | Download                                                                                         |
| -------- | ------------------------- | ------------------------------------------------------------------------------------------------ |
| `plain`  | plain digits only         | [link](https://drive.google.com/uc?export=download&amp;id=1-E3sbKtzN8NGNefUdky2NVniW1fAa5ZG) (16 MB) |
| `global` | plain+thinning+thickening | [link](https://drive.google.com/uc?export=download&amp;id=1fFGJW0IHoBmLuD6CEKCB8jz3Y5LJ5Duk) (15 MB) |
| `local`  | plain+swelling+fractures  | [link](https://drive.google.com/uc?export=download&amp;id=1ECYmtpPvGH0AkK0JfrGfA2FpOCZK1VX2) (16 MB) |

We additionally provide the datasets affected by a single perturbation from which `local` and `global` were composed by random interleaving with `plain`:

| Dataset | Description     | Download                                                                                         |
| ------- | --------------- | ------------------------------------------------------------------------------------------------ |
| `thin`  | thinning only   | [link](https://drive.google.com/uc?export=download&amp;id=1q3Bfl1oraKZcIPLHnqkU0whnTiz-AVSP) (13 MB) |
| `thic`  | thickening only | [link](https://drive.google.com/uc?export=download&amp;id=1Uy-SmnEkwq_dptTFuoUtmO9rn2FAbNb8) (16 MB) |
| `swel`  | swelling only   | [link](https://drive.google.com/uc?export=download&amp;id=1tzcInQ5mUdDVOvF5csItbGPrbwVTBkTJ) (17 MB) |
| `frac`  | fractures only  | [link](https://drive.google.com/uc?export=download&amp;id=1Yrl_00-SFZZPyRnqjPsbAFlv6uwBZtJd) (16 MB) |

Finally we also make available the pre-computed morphometrics for the *original* MNIST images (only the `.csv` tables; the images and labels can be downloaded from [LeCun's website](http://yann.lecun.com/exdb/mnist/)):

| Dataset    | Description                  | Download                                                                                          |
| ---------- | ---------------------------- | ------------------------------------------------------------------------------------------------- |
| `original` | original MNIST morphometrics | [link](https://drive.google.com/uc?export=download&amp;id=11rWisIshN78ZJMPYN0YC6UEKX5484B-n) (3.2 MB) |

The folder with all datasets for download can be accessed [here](https://drive.google.com/drive/folders/1ZzTBfXUKa4JW0lHkUIJ1qFCCSkOqWFvL).

## API Overview

The most relevant modules for end-users are `io` `measure` and `perturb` whose API we summarise below. For further details on these and on the other modules please refer to the respective docstrings. The default arguments to all functions and constructors are the ones used in the paper and work well in practice.

### `morphomnist.io`

Utility functions to load and save MNIST data files in [IDX format](http://yann.lecun.com/exdb/mnist/). Can read and write plain or `gzip`-ed files given the `*.gz` file extension.

```python
input_images = load_idx(""input_dir/images-idx3-ubyte.gz"")
# ...
save_idx(output_images ""output_dir/images-idx3-ubyte.gz"")
```

### `morphomnist.measure`

Functions to compute morphometrics for a single MNIST image:

```python
area length thickness slant width height = measure_image(image)
```

or for a batch of images with support for parallel processing (can take up to a few minutes; displays a progress bar if [`tqdm`](https://tqdm.github.io/) is installed):

```python
with multiprocessing.Pool() as pool:
    metrics = measure_batch(images pool=pool)  # A pandas.DataFrame
```

### `morphomnist.perturb`

Contains a number of subclasses of `Perturbation` which apply a parametrisable transformation to a high-resolution binary MNIST image:

- `Perturbation` (abstract)
    - `Thinning`: Thin a digit by a specified proportion of its thickness.
    - `Thickening`: Thicken a digit by a specified proportion of its thickness.
    - `Deformation` (abstract)
        - `Swelling`: Create a local swelling at a random location along the skeleton. Coordinates within a specified radius of the centre location are warped according to a radial power transform.
    - `Fracture`: Add fractures to a digit. Fractures are added at random locations along the skeleton while avoiding stroke tips and forks and are locally perpendicular to the pen stroke.

`Perturbation` instances are callable taking as argument a `morphomnist.morpho.ImageMorphology` object constructed from the input image.

Below is a simple usage example applying a random perturbation (or none) to each of a collection of `images`:

```python
import numpy as np
from morphomnist import io morpho perturb

perturbations = (
    lambda m: m.binary_image  # No perturbation
    perturb.Thinning(amount=.7)
    perturb.Thickening(amount=1.)
    perturb.Swelling(strength=3 radius=7)
    perturb.Fracture(num_frac=3)
)

images = io.load_idx(""input_dir/images-idx3-ubyte.gz"")
perturbed_images = np.empty_like(images)
perturbation_labels = np.random.randint(len(perturbations) size=len(images))
for n in range(len(images)):
    morphology = morpho.ImageMorphology(images[n] scale=4)
    perturbation = perturbations[perturbation_labels[n]]
    perturbed_hires_image = perturbation(morphology)
    perturbed_images[n] = morphology.downscale(perturbed_hires_image)
io.save_idx(perturbed_images ""output_dir/images-idx3-ubyte.gz"")
io.save_idx(perturbation_labels ""output_dir/pert-idx1-ubyte.gz"")
```"
"Abstract:  The increasing concern with misinformation has stimulated research efforts on
automatic fact checking. The recently-released FEVER dataset introduced a
benchmark fact-verification task in which a system is asked to verify a claim
using evidential sentences from Wikipedia documents. In this paper we present
a connected system consisting of three homogeneous neural semantic matching
models that conduct document retrieval sentence selection and claim
verification jointly for fact extraction and verification. For evidence
retrieval (document retrieval and sentence selection) unlike traditional
vector space IR models in which queries and sources are matched in some
pre-designed term vector space we develop neural models to perform deep
semantic matching from raw textual input assuming no intermediate term
representation and no access to structured external knowledge bases. We also
show that Pageview frequency can also help improve the performance of evidence
retrieval results that later can be matched by using our neural semantic
matching network. For claim verification unlike previous approaches that
simply feed upstream retrieved evidence and the claim to a natural language
inference (NLI) model we further enhance the NLI model by providing it with
internal semantic relatedness scores (hence integrating it with the evidence
retrieval modules) and ontological WordNet features. Experiments on the FEVER
dataset indicate that (1) our neural semantic matching method outperforms
popular TF-IDF and encoder models by significant margins on all evidence
retrieval metrics (2) the additional relatedness score and WordNet features
improve the NLI model via better semantic awareness and (3) by formalizing all
three subtasks as a similar semantic matching problem and improving on all
three stages the complete model is able to achieve the state-of-the-art
results on the FEVER test set.
# Combine-FEVER-NSMN
This repository provides the implementation for the paper [Combining Fact Extraction and Verification with Neural Semantic Matching Networks](https://arxiv.org/abs/1811.07039) (AAAI 2019 and EMNLP-FEVER Shared Task Rank-1 System).

## Requirement
* Python 3.6
* pytorch 0.4.1
* allennlp 0.7.1
* sqlitedict
* wget
* flashtext
* pexpect
* fire
* inflection

Try to install the package as the order above.
Previous version of pytorch can be find at [legacy pytorch](https://pytorch.org/get-started/previous-versions/).

## Preparation
1. Setup the python environment and download the required package listed above.
2. Run the preparation script.
```bash
source setup.sh
bash ./scripts/prepare.sh
```
The script will download all the required data the auxiliary packages and files.

3. Tokenize the dataset and build wiki document database for easy and fast access and query.
```bash
python src/pipeline/prepare_data.py tokenization        # Tokenization
python src/pipeline/prepare_data.py build_database      # Build document database. (This might take a while)
```

After preparation the following folder should contain similar files as listed below.
```bash
data
├── fever
│   ├── license.html
│   ├── shared_task_dev.jsonl
│   ├── shared_task_test.jsonl
│   └── train.jsonl
├── fever.db
├── id_dict.jsonl
├── license.html
├── sentence_tokens.json
├── tokenized_doc_id.json
├── tokenized_fever
│   ├── shared_task_dev.jsonl
│   └── train.jsonl
├── vocab_cache
│   └── nli_basic
│       ├── labels.txt
│       ├── non_padded_namespaces.txt
│       ├── tokens.txt
│       ├── unk_count_namespaces.txt
│       └── weights
│           └── glove.840B.300d
├── wiki-pages
│   ├── wiki-001.jsonl
│   ├── ... ...
│   └── wiki-109.jsonl
└── wn_feature_p
    ├── ant_dict
    ├── em_dict
    ├── em_lemmas_dict
    ├── hyper_lvl_dict
    ├── hypernym_stems_dict
    ├── hypo_lvl_dict
    └── hyponym_stems_dict
```
```bash
dep_packages
├── DrQA
└── stanford-corenlp-full-2017-06-09
```
```bash
results
└── chaonan99
```
```bash
saved_models
├── saved_nli_m
├── nn_doc_selector
└── saved_sselector
```

## Automatic pipeline procedure.
Running the pipeline system on the dev set with the code below:
```bash
python src/pipeline/auto_pipeline.py
```
Note that this pipeline is the (SotA) model in the AAAI paper. 
For EMNLP-FEVER Shared Task version please refer to `src/nli/mesim_wn_simi_v1_3.py` and `src/pipeline/pipeline_process.py`.

## Citation
If you find this implementation helpful please consider citing:
```
@inproceedings{nie2019combining
  title={Combining Fact Extraction and Verification with Neural Semantic Matching Networks}
  author={Yixin Nie and Haonan Chen and Mohit Bansal}
  booktitle={Association for the Advancement of Artificial Intelligence ({AAAI})}
  year={2019}
}
```"
"Abstract:  For research to go in the right direction it is essential to be able to
compare and quantify performance of different algorithms focused on the same
problem. Choosing a suitable evaluation metric requires deep understanding of
the pursued task along with all of its characteristics. We argue that in the
case of applied machine learning proper evaluation metric is the basic
building block that should be in the spotlight and put under thorough
examination. Here we address tasks with class imbalance in which the class of
interest is the one with much lower number of samples. We encountered
non-insignificant amount of recent papers in which improper evaluation methods
are used borrowed mainly from the field of balanced problems. Such bad
practices may heavily bias the results in favour of inappropriate algorithms
and give false expectations of the state of the field.
# Combine-FEVER-NSMN
This repository provides the implementation for the paper [Combining Fact Extraction and Verification with Neural Semantic Matching Networks](https://arxiv.org/abs/1811.07039) (AAAI 2019 and EMNLP-FEVER Shared Task Rank-1 System).

## Requirement
* Python 3.6
* pytorch 0.4.1
* allennlp 0.7.1
* sqlitedict
* wget
* flashtext
* pexpect
* fire
* inflection

Try to install the package as the order above.
Previous version of pytorch can be find at [legacy pytorch](https://pytorch.org/get-started/previous-versions/).

## Preparation
1. Setup the python environment and download the required package listed above.
2. Run the preparation script.
```bash
source setup.sh
bash ./scripts/prepare.sh
```
The script will download all the required data the auxiliary packages and files.

3. Tokenize the dataset and build wiki document database for easy and fast access and query.
```bash
python src/pipeline/prepare_data.py tokenization        # Tokenization
python src/pipeline/prepare_data.py build_database      # Build document database. (This might take a while)
```

After preparation the following folder should contain similar files as listed below.
```bash
data
├── fever
│   ├── license.html
│   ├── shared_task_dev.jsonl
│   ├── shared_task_test.jsonl
│   └── train.jsonl
├── fever.db
├── id_dict.jsonl
├── license.html
├── sentence_tokens.json
├── tokenized_doc_id.json
├── tokenized_fever
│   ├── shared_task_dev.jsonl
│   └── train.jsonl
├── vocab_cache
│   └── nli_basic
│       ├── labels.txt
│       ├── non_padded_namespaces.txt
│       ├── tokens.txt
│       ├── unk_count_namespaces.txt
│       └── weights
│           └── glove.840B.300d
├── wiki-pages
│   ├── wiki-001.jsonl
│   ├── ... ...
│   └── wiki-109.jsonl
└── wn_feature_p
    ├── ant_dict
    ├── em_dict
    ├── em_lemmas_dict
    ├── hyper_lvl_dict
    ├── hypernym_stems_dict
    ├── hypo_lvl_dict
    └── hyponym_stems_dict
```
```bash
dep_packages
├── DrQA
└── stanford-corenlp-full-2017-06-09
```
```bash
results
└── chaonan99
```
```bash
saved_models
├── saved_nli_m
├── nn_doc_selector
└── saved_sselector
```

## Automatic pipeline procedure.
Running the pipeline system on the dev set with the code below:
```bash
python src/pipeline/auto_pipeline.py
```
Note that this pipeline is the (SotA) model in the AAAI paper. 
For EMNLP-FEVER Shared Task version please refer to `src/nli/mesim_wn_simi_v1_3.py` and `src/pipeline/pipeline_process.py`.

## Citation
If you find this implementation helpful please consider citing:
```
@inproceedings{nie2019combining
  title={Combining Fact Extraction and Verification with Neural Semantic Matching Networks}
  author={Yixin Nie and Haonan Chen and Mohit Bansal}
  booktitle={Association for the Advancement of Artificial Intelligence ({AAAI})}
  year={2019}
}
```"
"Abstract:  Many real-world vision problems suffer from inherent ambiguities. In clinical
applications for example it might not be clear from a CT scan alone which
particular region is cancer tissue. Therefore a group of graders typically
produces a set of diverse but plausible segmentations. We consider the task of
learning a distribution over segmentations given an input. To this end we
propose a generative segmentation model based on a combination of a U-Net with
a conditional variational autoencoder that is capable of efficiently producing
an unlimited number of plausible hypotheses. We show on a lung abnormalities
segmentation task and on a Cityscapes segmentation task that our model
reproduces the possible segmentation variants as well as the frequencies with
which they occur doing so significantly better than published approaches.
These models could have a high impact in real-world applications such as being
used as clinical decision-making algorithms accounting for multiple plausible
semantic segmentation hypotheses to provide possible diagnoses and recommend
further actions to resolve the present ambiguities.
# Probabilistic U-Net

Re-implementation of the model described in `A Probabilistic U-Net for Segmentation of Ambiguous Images' ([paper @ NeurIPS 2018](https://arxiv.org/abs/1806.05034)).

This was also a spotlight presentation at NeurIPS and a short video on the paper of similar content can be found [here](https://youtu.be/-cfFxQWfFrA) (4min).

The architecture of the Probabilistic U-Net is depicted below: subfigure a) shows sampling and b) the training setup: 
![](assets/architecture.png)

Below see samples conditioned on held-out validation set images from the (stochastic) CityScapes data set:
![](assets/10_image_16_sample.gif)

## Setup package in virtual environment

```
git clone https://github.com/SimonKohl/probabilistic_unet.git .
cd prob_unet/
virtualenv -p python3 venv
source venv/bin/activate
pip3 install -e .
```

## Install batch-generators for data augmentation
```
cd ..
git clone https://github.com/MIC-DKFZ/batchgenerators
cd batchgenerators
pip3 install nilearn scikit-image nibabel
pip3 install -e .
cd prob_unet
```

## Download &amp; preprocess the Cityscapes dataset

1) Create a login account on the Cityscapes website: https://www.cityscapes-dataset.com/
2) Once you've logged in download the train val and test annotations and images:
    - Annotations: [gtFine_trainvaltest.zip](https://www.cityscapes-dataset.com/file-handling/?packageID=1) (241MB)
    - Images: [leftImg8bit_trainvaltest.zip](https://www.cityscapes-dataset.com/file-handling/?packageID=3) (11GB)
3) unzip the data (unzip <name>_trainvaltest.zip) and adjust `raw_data_dir` (full path to unzipped files) and `out_dir` (full path to desired output directory) in `preprocessing_config.py`
4) bilinearly rescale the data to a resolution of 256 x 512 and save as numpy arrays by running
```
cd cityscapes
python3 preprocessing.py
cd ..
```

## Training

[skip to evaluation in case you only want to use the pretrained model.]  
modify `data_dir` and `exp_dir` in `scripts/prob_unet_config.py` then:
```
cd training
python3 train_prob_unet.py --config prob_unet_config.py
```

## Evaluation

Load your own trained model or use a pretrained model. A set of pretrained weights can be downloaded from [zenodo.org](https://zenodo.org/record/1419051#.W5utoOEzYUE) (187MB). After down-loading unpack the file via
`tar -xvzf pretrained_weights.tar.gz` e.g. in `/model`. In either case (using your own or the pretrained model) modify the `data_dir` and
`exp_dir` in `evaluation/cityscapes_eval_config.py` to match you paths.

then first write samples (defaults to 16 segmentation samples for each of the 500 validation images):
```
cd ../evaluation
python3 eval_cityscapes.py --write_samples
```
followed by their evaluation (which is multi-threaded and thus reasonably fast):
```
python3 eval_cityscapes.py --eval_samples
```
The evaluation produces a dictionary holding the results. These can be visualized by launching an ipython notbook:
```
jupyter notebook evaluation_plots.ipynb
```
The following results are obtained from the pretrained model using above notebook:
![](assets/validation_results.png) 

## Tests

The evaluation metrics are under test-coverage. Run the tests as follows:
```
cd ../tests/evaluation
python3 -m pytest eval_tests.py
```

## Deviations from original work

The code found in this repository was not used in the original paper and slight modifications apply:

- training on a single gpu (Titan Xp) instead of distributed training which is not supported in this implementation
- average-pooling rather than bilinear interpolation is used for down-sampling operations in the model
- the number of conv kernels is kept constant after the 3rd scale as opposed to strictly doubling it after each scale (for reduction of memory footprint)
- HeNormal weight initialization worked better than a orthogonal weight initialization


## How to cite this code
Please cite the original publication:
```
@article{kohl2018probabilistic
  title={A Probabilistic U-Net for Segmentation of Ambiguous Images}
  author={Kohl Simon AA and Romera-Paredes Bernardino and Meyer Clemens and De Fauw Jeffrey and Ledsam Joseph R and Maier-Hein Klaus H and Eslami SM and Rezende Danilo Jimenez and Ronneberger Olaf}
  journal={arXiv preprint arXiv:1806.05034}
  year={2018}
}
```

## License
The code is published under the [Apache License Version 2.0](LICENSE).</name>"
"Abstract:  In this brief technical report we introduce the CINIC-10 dataset as a plug-in
extended alternative for CIFAR-10. It was compiled by combining CIFAR-10 with
images selected and downsampled from the ImageNet database. We present the
approach to compiling the dataset illustrate the example images for different
classes give pixel distributions for each part of the repository and give
some standard benchmarks for well known models. Details for download usage
and compilation can be found in the associated github repository.
# CINIC-10: CINIC-10 Is Not Imagenet or CIFAR-10
Dataset: [http://dx.doi.org/10.7488/ds/2448](http://dx.doi.org/10.7488/ds/2448)

Blog: [Bayeswatch Blog: CINIC-10](http://www.bayeswatch.com/2018/10/09/CINIC/)

Paper: [Darlow L.N. Crowley E.J. Antoniou A. and A.J. Storkey (2018) CINIC-10 is not ImageNet or CIFAR-10. Report EDI-INF-ANC-1802 (arXiv:1810.03505).](https://arxiv.org/abs/1810.03505)

CINIC-10 is a drop-in replacement for CIFAR-10. We compiled it as a benchmarking datset because CIFAR-10  can be too small/too easy and ImageNet is often too large/difficult. [ImageNet32](https://arxiv.org/abs/1707.08819) and [ImageNet64](https://arxiv.org/abs/1707.08819) are smaller than ImageNet but even more difficult. CINIC-10 fills this benchmarking gap. 

## Motivation
""Recent years have seen tremendous advances in the field of deep learning ([LeCun et al. 2015](#references))""



Some derivation of the quote above may be familiar to many readers. Something similar appears at the beginning of numerous papers on deep learning. How might we assess statements like this? It is through benchmarking. AlexNet ([Krizhevsky et al. 2012](#references)) outperformed traditional computer vision methods on ImageNet ([Russakovsky et al. 2015](#references)) which was in turn outperformed by VGG nets ([Simonyan &amp; Zisserman 2015](#references)) then ResNets ([He et al. 2016](#references)) etc.

ImageNet has its flaws however. It is an unwieldy dataset. The images are large at least in neural network terms and there are over a million of them. A single training run can take several days without abundant computational resources ([Goyal et al. 2017](#references)). Perhaps for this reason CIFAR-10 and CIFAR-100 ([Krizhevsky 2009](#references)) have become the datasets of choice for many when initially benchmarking neural networks in the context of realistic images. Indeed this is where several popular architectures have demonstrated their potency ([Huang et al. 2017](#references); [Gastaldi 2017](#references)).

In CIFAR-10 each of the 10 classes has 6000 examples. The 100 classes of CIFAR-100 only have 600 examples each. This leads to a large gap in difficulty between these tasks; CIFAR-100 is arguably more difficult than even ImageNet. A dataset that provides another milestone with respect to task difficulty would be useful. ImageNet-32 ([Chrabaszcz et al. 2017](#references)) already exists as a CIFAR alternative; however this actually poses a more challenging problem than ImageNet as the down-sampled images have substantially less capacity for information. Moreover most benchmark datasets have uneven train/validation/test splits (validation being non-existent for CIFAR). Equally sized splits are desirable as they give a more principled perspective of generalisation performance.

To combat the shortcomings of existing benchmarking datasets we present CINIC-10: CINIC-10 Is Not ImageNet or CIFAR-10. It is an extension of CIFAR-10 via the addition of downsampled ImageNet images. CINIC-10 has the following desirable properties:
- It has 270000 images 4.5 times that of CIFAR.
- The images are the same size as in CIFAR meaning that CINIC-10 can be used as a drop-in
alternative to CIFAR-10. 
- It has equally sized train validation and test splits. In some experimental setups it may be that more than one training dataset is required. Nonetheless a fair assessment of generalisation performance is enabled through equal dataset split sizes. 
- The train and validation subsets can be combined to make a larger training set. 
- CINIC-10 consists of images from both CIFAR and ImageNet. The images from these are not necessarily identically distributed presenting a new challenge: distribution shift. In other words we can find out how well models trained on CIFAR images perform on ImageNet images for the same classes.

## Details

- CINIC-10 has a total of 270000 images equally split amonst three subsets: train validate and test.

- In each subset (90000 images) there are ten classes (identical to [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) classes). There are 9000 images per class per subset. Using the suggested data split (an equal three-way split) CINIC-10 has 1.8 times as many training samples than CIFAR-10. CINIC-10 is designed to be directly swappable with CIFAR-10.

- The train and validation classes can be combined to form a larger train set. In this case CINIC-10 would have 3.6 times as many training samples than CIFAR-10. A [notebook is provided](https://github.com/BayesWatch/cinic-10/blob/master/notebooks/enlarge-train-set.ipynb) to do this.

- The means and standard deviations of the (*r**g**b*) channels was calculated to be:

  ```python
  cinic_mean_RGB = [0.47889522 0.47227842 0.43047404]
  cinic_std_RGB = [0.24205776 0.23828046 0.25874835]
  ```

- CINIC-10 is saved to be [used with PyTorch data loaders](#data-loading). The following folder structure is used:

    train/

    train/airplane

    train/automobile

    train/...

    valid/

    valid/...

    test/

    test/...
    
## Construction
1. The original CIFAR-10 data was processed into image format (.png) and stored as follows:
      ``` [$set$/$class_name$/cifar-10-$origin$-$index$.png]```

      - where ```$set$``` is either train valid or test. ```$class_name$``` refers to the [CIFAR-10 classes](https://www.cs.toronto.edu/~kriz/cifar.html) (airplane automobile etc.) ```$origin$``` the set from which the image was taken (train or test) and ```$index$``` the original index of this images within the set it comes from. 
      - **NOTES**:
           - Storing in this manner enables the user to perfectly reconstruct the CIFAR-10 dataset from CINIC-10. We have provided [a notebook](https://github.com/BayesWatch/cinic-10/blob/master/notebooks/recover-cifar-and-validate.ipynb)  that demonstrates how to do this and validates that the images are identical.
           - The entirety of the original CIFAR-10 *test* set is within the abovementioned *new* *test* set. The remaining elements of this *new test* set were randomly selected from the CIFAR-10 train set. The *new* train and validation sets are a random split of the remaining elements therein.

         This is an equal split of the CIFAR-10 data: 20000 images per set; 2000 images per class within set; and an equal distribution of CIFAR-10 data among all three sets.

2. The relevant synonym sets (synsets) within the Fall 2011 release of the ImageNet Database were identified and collected. These *synset-groups* are listed in [**synsets-to-cifar-10-classes.txt**](https://github.com/BayesWatch/cinic-10/blob/master/synsets-to-cifar-10-classes.txt). The mapping from sysnsets to CINIC-10 is listed in [**imagenet-contributors.csv**](https://github.com/BayesWatch/cinic-10/blob/master/imagenet-contributors.csv) 

3. These synsets were downloaded using [Imagenet Utils](https://github.com/tzutalin/ImageNet_Utils). Note that some *.tar* downloads failed (with a 0 Byte download) even after repeated retries. This is not exceedingly detrimental as a subset of the downloaded images was taken.

4. The *.tar* files were extracted the *.JPEG* images were read using the Pillow Python library and converted to 32 by 32 pixel images with the 'Box' algorithm from the [Pillow library](https://python-pillow.org) (in the same manner as [Imagenet32x32](https://patrykchrabaszcz.github.io/Imagenet32/) for consistency).

5. The **lowest** number of CIFAR10 **class-relevant** samples from these Imagenet *synset-groups* samples was observed to be 21939 in the 'truck' class. Therefore 21000 samples were randomly selected from each *synset-group* to compile CINIC-10 by augmenting the CIFAR-10 data.

6. Finally these 21000 samples were randomly distributed (but can be recovered using the filename) within the *new* train validation and test sets storing as follows:
      ``` [$set$/$class_name$/$synset$_$number$.png]```

      - where ```$set$``` is either train valid or test. ```$class_name$``` refers to the [CIFAR-10 classes](https://www.cs.toronto.edu/~kriz/cifar.html) (airplane automobile etc.). ```$synset$``` indicates which Imagenet synset this image came from and ```$number$``` is the image number directly associated with the original *.JPEG* images. 
      - **NOTES**:
           - The image filenames themselves ```$synset$_$number$.png``` are identical to the filenames of the original *.JPEG* images from which these downsampled *.png* images were computed. 
           - This naming structure allows the user to identify exactly the origin of all images. 


## Benchmarks

Bechmarks were run on CINIC-10 in two configurations: (1) the suggested equal three-way split trained on the train subset and tested on the test subset; and (2) trained on the combined train and validation sets and tested on the test set. 

Model definitions were copied from [here](https://github.com/kuangliu/pytorch-cifar/). They were all trained for 300 epochs at an initial learning rate of 0.1 with a momentum multiplier of 0.9 weight decay with a multiplier of 0.0001 and batch size 64. The learning rate was cosine annealed to zero.

#### Results when training on the train subset

| Model              | No. Parameters |  Validation Error  |
|--------------------|----------------|--------------------|
| VGG-16             |  14.7M         | 15.25              |
| ResNet-18          |  11.2M         | 12.42              |
| ResNet-18 (preact) |  11.2M         | 12.84              |
| GoogLeNet          |   6.2M         | 11.54              |
| ResNeXt29_2x64d    |   9.2M         | 11.66              |
| Mobilenet          |   3.2M         | 19.55              |

### Results when training on the train + validation subset

For comparison with CIFAR-10 models these were trained 5 times with different seeds. The error is listed to include standard deviation over those runs:

| Model              | No. Parameters |  Validation Error  |
|--------------------|----------------|--------------------|
| VGG-16             |  14.7M         | 12.23 +/- 0.16     |
| ResNet-18          |  11.2M         |  9.73 +/- 0.05     |
| ResNet-18 (preact) |  11.2M         | 10.10 +/- 0.08     |
| GoogLeNet          |   6.2M         |  8.83 +/- 0.12     |
| ResNeXt29_2x64d    |   9.2M         |  8.55 +/- 0.15     |
| Densenet-121       |   7.0M         |  8.74 +/- 0.16     |
| Mobilenet          |   3.2M         | 18.00 +/- 0.16     |





## Usage

### Download

To download either [use this link](https://datashare.is.ed.ac.uk/handle/10283/3192) or the following wget command:

```bash
wget https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz
```



### Data loading

The simplest way to use CINIC-10 is with a [PyTorch](https://pytorch.org/) data loader as follows:

``` python
import torchvision
import torchvision.transforms as transforms

cinic_directory = '/path/to/cinic/directory'
cinic_mean = [0.47889522 0.47227842 0.43047404]
cinic_std = [0.24205776 0.23828046 0.25874835]
cinic_train = torch.utils.data.DataLoader(
    torchvision.datasets.ImageFolder(cinic_directory + '/train'
    	transform=transforms.Compose([transforms.ToTensor()
        transforms.Normalize(mean=cinic_meanstd=cinic_std)]))
    batch_size=128 shuffle=True)
```

### Classification
The suggested dataset can be used *as is* in a standard classification set-up. Further the train and validation subsets can be combined ([using symbolic links into a new data folder](https://github.com/BayesWatch/cinic-10/blob/master/notebooks/enlarge-train-set.ipynb))  to more closely match the data split choice of CIFAR-10 (one large train set and one smaller test set).

### Distribution shift

Since CINIC-10 is constructed from two different sources it is not a guarantee that the constituent elements are drawn from the same distribution. This is one of the motivations for an equal split of the CIFAR-10 data between each subset of CINIC-10. This property can however be leveraged to understand how well models cope with samples drawn from similar but not identical distributions. A [notebook](https://github.com/BayesWatch/cinic-10/blob/master/notebooks/imagenet-extraction.ipynb) is provided to extract the imagenet samples from CINIC-10 and [another](https://github.com/BayesWatch/cinic-10/blob/master/notebooks/cifar-extraction.ipynb) to extract the CIFAR-10 samples. 

## Samples
Below are samples randomly selected from CINIC-10 and from CIFAR-10 for comparison. It is clear that CINIC-10 is a more noisy dataset because the *Imagenet constituent samples were not vetted*.


### Airplane

##### CIFAR-10

![CIFAR airplane](images/cifar-airplane.png)
##### CINIC-10
![CINIC airplane](images/cinic-airplane.png)

---


### Automobile

##### CIFAR-10

![CIFAR automobile](images/cifar-automobile.png)
##### CINIC-10
![CINIC automobile](images/cinic-automobile.png)

---

### Bird

##### CIFAR-10

![CIFAR bird](images/cifar-bird.png)
##### CINIC-10
![CINIC bird](images/cinic-bird.png)

---

### Cat

##### CIFAR-10

![CIFAR cat](images/cifar-cat.png)
##### CINIC-10
![CINIC cat](images/cinic-cat.png)

---

### Deer

##### CIFAR-10

![CIFAR deer](images/cifar-deer.png)
##### CINIC-10
![CINIC deer](images/cinic-deer.png)

---

### Dog

##### CIFAR-10

![CIFAR dog](images/cifar-dog.png)
##### CINIC-10
![CINIC dog](images/cinic-dog.png)

---

### Frog

##### CIFAR-10

![CIFAR frog](images/cifar-frog.png)
##### CINIC-10
![CINIC frog](images/cinic-frog.png)

---
### Horse

##### CIFAR-10

![CIFAR horse](images/cifar-horse.png)
##### CINIC-10
![CINIC horse](images/cinic-horse.png)

---

### Ship

##### CIFAR-10

![CIFAR ship](images/cifar-ship.png)
##### CINIC-10
![CINIC ship](images/cinic-ship.png)

---

### Truck

##### CIFAR-10

![CIFAR truck](images/cifar-truck.png)
##### CINIC-10
![CINIC truck](images/cinic-truck.png)

---

## Pixel intensity distribution

The figure below compares the intensity distributions of Imagenet contributors and CIFAR-10. The dashed lines are the means.

![Distributions](images/histogram-cinic.svg)


## References

Patryk Chrabaszcz Ilya Loshchilov and Hutter Frank. A downsampled variant of ImageNet as an alternative
to the CIFAR datasets. arXiv preprint arXiv:1707.08819 2017.

Xavier Gastaldi. Shake-shake regularization. arXiv preprint arXiv:1705.07485 2017.

P. Goyal P. Dollár R. Girshick P. Noordhuis L. Wesolowski A. Kyrola A. Tulloch Y. Jia and K. He. Accurate large minibatch SGD: Training ImageNet in 1 hour. arXiv preprint arXiv:1706.02677 2017.

Kaiming He Xiangyu Zhang Shaoqing Ren and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2016.

G. Huang Z. Liu L. van der Maaten and K. Q. Weinberger. Densely connected convolutional networks.
2017.

Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis Toronto University
2009.

Alex Krizhevsky Ilya Sutskever and Geoffrey E. Hinton. ImageNet classification with deep convolutional
neural networks. In Advances in Neural Information Processing Systems 2012.

Yann LeCun Yoshua Bengio and Geoffrey Hinton. Deep learning. Nature 521(7553):436–444 2015.

Olga Russakovsky Jia Deng Hao Su Jonathan Krause Sanjeev Satheesh Sean Ma Zhiheng Huang Andrej Karpathy Aditya Khosla Michael Bernstein Alexander C. Berg and Li Fei-Fei. ImageNet large scale visual recognition challenge. International Journal of Computer Vision (IJCV) 115(3):211–252 2015. doi: 10.1007/s11263-015-0816-y.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations 2015."
"Abstract:  This paper describes an Open Source Software (OSS) project: PythonRobotics.
This is a collection of robotics algorithms implemented in the Python
programming language. The focus of the project is on autonomous navigation and
the goal is for beginners in robotics to understand the basic ideas behind each
algorithm. In this project the algorithms which are practical and widely used
in both academia and industry are selected. Each sample code is written in
Python3 and only depends on some standard modules for readability and ease of
use. It includes intuitive animations to understand the behavior of the
simulation.
<img align=""right"" src=""https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true"" width=""300""/>

# PythonRobotics
[![Build Status](https://travis-ci.org/AtsushiSakai/PythonRobotics.svg?branch=master)](https://travis-ci.org/AtsushiSakai/PythonRobotics)
[![Documentation Status](https://readthedocs.org/projects/pythonrobotics/badge/?version=latest)](https://pythonrobotics.readthedocs.io/en/latest/?badge=latest)
[![Build status](https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true)](https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics)
[![Coverage Status](https://coveralls.io/repos/github/AtsushiSakai/PythonRobotics/badge.svg?branch=master)](https://coveralls.io/github/AtsushiSakai/PythonRobotics?branch=master)
[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/AtsushiSakai/PythonRobotics.svg?logo=lgtm&amp;logoWidth=18)](https://lgtm.com/projects/g/AtsushiSakai/PythonRobotics/context:python)
[![CodeFactor](https://www.codefactor.io/repository/github/atsushisakai/pythonrobotics/badge/master)](https://www.codefactor.io/repository/github/atsushisakai/pythonrobotics/overview/master)
[![tokei](https://tokei.rs/b1/github/AtsushiSakai/PythonRobotics)](https://github.com/AtsushiSakai/PythonRobotics)
[![Say Thanks!](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/AtsushiSakai)

Python codes for robotics algorithm.



# Table of Contents
   * [What is this?](#what-is-this)
   * [Requirements](#requirements)
   * [Documentation](#documentation)
   * [How to use](#how-to-use)
   * [Localization](#localization)
      * [Extended Kalman Filter localization](#extended-kalman-filter-localization)
      * [Particle filter localization](#particle-filter-localization)
      * [Histogram filter localization](#histogram-filter-localization)
   * [Mapping](#mapping)
      * [Gaussian grid map](#gaussian-grid-map)
      * [Ray casting grid map](#ray-casting-grid-map)
      * [k-means object clustering](#k-means-object-clustering)
      * [Rectangle fitting](#rectangle-fitting)
   * [SLAM](#slam)
      * [Iterative Closest Point (ICP) Matching](#iterative-closest-point-icp-matching)
      * [FastSLAM 1.0](#fastslam-10)
      * [Graph based SLAM](#graph-based-slam)
   * [Path Planning](#path-planning)
      * [Dynamic Window Approach](#dynamic-window-approach)
      * [Grid based search](#grid-based-search)
         * [Dijkstra algorithm](#dijkstra-algorithm)
         * [A* algorithm](#a-algorithm)
         * [Potential Field algorithm](#potential-field-algorithm)
      * [State Lattice Planning](#state-lattice-planning)
         * [Biased polar sampling](#biased-polar-sampling)
         * [Lane sampling](#lane-sampling)
      * [Probabilistic Road-Map (PRM) planning](#probabilistic-road-map-prm-planning)
      * [Rapidly-Exploring Random Trees (RRT)](#rapidly-exploring-random-trees-rrt)
         * [RRT*](#rrt)
         * [RRT* with reeds-sheep path](#rrt-with-reeds-sheep-path)
         * [LQR-RRT*](#lqr-rrt)
      * [Quintic polynomials planning](#quintic-polynomials-planning)
      * [Reeds Shepp planning](#reeds-shepp-planning)
      * [LQR based path planning](#lqr-based-path-planning)
      * [Optimal Trajectory in a Frenet Frame](#optimal-trajectory-in-a-frenet-frame)
   * [Path Tracking](#path-tracking)
      * [move to a pose control](#move-to-a-pose-control)
      * [Stanley control](#stanley-control)
      * [Rear wheel feedback control](#rear-wheel-feedback-control)
      * [Linear–quadratic regulator (LQR) speed and steering control](#linearquadratic-regulator-lqr-speed-and-steering-control)
      * [Model predictive speed and steering control](#model-predictive-speed-and-steering-control)
      * [Nonlinear Model predictive control with C-GMRES](#nonlinear-model-predictive-control-with-c-gmres)
   * [Arm Navigation](#arm-navigation)
      * [N joint arm to point control](#n-joint-arm-to-point-control)
      * [Arm navigation with obstacle avoidance](#arm-navigation-with-obstacle-avoidance)
   * [Aerial Navigation](#aerial-navigation)
      * [drone 3d trajectory following](#drone-3d-trajectory-following)
      * [rocket powered landing](#rocket-powered-landing)
   * [Bipedal](#bipedal)
      * [bipedal planner with inverted pendulum](#bipedal-planner-with-inverted-pendulum)
   * [License](#license)
   * [Use-case](#use-case)
   * [Contribution](#contribution)
   * [Support](#support)
   * [Authors](#authors)

# What is this?

This is a Python code collection of robotics algorithms especially for autonomous navigation.

Features:

1. Easy to read for understanding each algorithm's basic idea.

2. Widely used and practical algorithms are selected.

3. Minimum dependency.

See this paper for more details:

- [\[1808\.10703\] PythonRobotics: a Python code collection of robotics algorithms](https://arxiv.org/abs/1808.10703) ([BibTeX](https://github.com/AtsushiSakai/PythonRoboticsPaper/blob/master/python_robotics.bib))


# Requirements

- Python 3.6.x (2.7 is not supported)

- numpy

- scipy

- matplotlib

- pandas

- [cvxpy](http://www.cvxpy.org/en/latest/) 

# Documentation

This README only shows some examples of this project. 

If you are interested in other examples or mathematical backgrounds of each algorithm 

You can check the full documentation online: [https://pythonrobotics.readthedocs.io/](https://pythonrobotics.readthedocs.io/)

All animation gifs are stored here: [AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs)

# How to use

1. Clone this repo.

&gt; git clone https://github.com/AtsushiSakai/PythonRobotics.git

&gt; cd PythonRobotics/


2. Install the required libraries. You can use environment.yml with conda command.

&gt; conda env create -f environment.yml


3. Execute python script in each directory.

4. Add star to this repo if you like it :smiley:. 

# Localization

## Extended Kalman Filter localization

<img src=""https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif"" width=""640""/>

Documentation: [Notebook](https://github.com/AtsushiSakai/PythonRobotics/blob/master/Localization/extended_kalman_filter/extended_kalman_filter_localization.ipynb)

## Particle filter localization

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif)

This is a sensor fusion localization with Particle Filter(PF).

The blue line is true trajectory the black line is dead reckoning trajectory

and the red line is estimated trajectory with PF.

It is assumed that the robot can measure a distance from landmarks (RFID).

This measurements are used for PF localization.

Ref:

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)


## Histogram filter localization

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif)

This is a 2D localization example with Histogram filter.

The red cross is true position black points are RFID positions.

The blue grid shows a position probability of histogram filter.  

In this simulation xy are unknown yaw is known.

The filter integrates speed input and range observations from RFID for localization.

Initial position is not needed.

Ref:

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)

# Mapping

## Gaussian grid map

This is a 2D Gaussian grid mapping example.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif)

## Ray casting grid map

This is a 2D ray casting grid mapping example.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif)

## k-means object clustering

This is a 2D object clustering with k-means algorithm.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif)

## Rectangle fitting

This is a 2D rectangle fitting for vehicle detection.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif)


# SLAM

Simultaneous Localization and Mapping(SLAM) examples

## Iterative Closest Point (ICP) Matching

This is a 2D ICP matching example with singular value decomposition.

It can calculate a rotation matrix and a translation vector between points to points.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif)

Ref:

- [Introduction to Mobile Robotics: Iterative Closest Point Algorithm](https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf)


## FastSLAM 1.0

This is a feature based SLAM example using FastSLAM 1.0.

The blue line is ground truth the black line is dead reckoning the red line is the estimated trajectory with FastSLAM.

The red points are particles of FastSLAM.

Black points are landmarks blue crosses are estimated landmark positions by FastSLAM.


![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif)


Ref:

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)

- [SLAM simulations by Tim Bailey](http://www-personal.acfr.usyd.edu.au/tbailey/software/slam_simulations.htm)


## Graph based SLAM

This is a graph based SLAM example.

The blue line is ground truth.

The black line is dead reckoning.

The red line is the estimated trajectory with Graph based SLAM.

The black stars are landmarks for graph edge generation.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/GraphBasedSLAM/animation.gif)

Ref:

- [A Tutorial on Graph-Based SLAM](http://www2.informatik.uni-freiburg.de/~stachnis/pdf/grisetti10titsmag.pdf)


# Path Planning

## Dynamic Window Approach

This is a 2D navigation sample code with Dynamic Window Approach.

- [The Dynamic Window Approach to Collision Avoidance](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf)

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif)


## Grid based search

### Dijkstra algorithm

This is a 2D grid based shortest path planning with Dijkstra's algorithm.

![PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif)

In the animation cyan points are searched nodes.

### A\* algorithm

This is a 2D grid based shortest path planning with A star algorithm.

![PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif)

In the animation cyan points are searched nodes.

Its heuristic is 2D Euclid distance.

### Potential Field algorithm

This is a 2D grid based path planning with Potential Field algorithm.

![PotentialField](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif)

In the animation the blue heat map shows potential value on each grid.

Ref:

- [Robotic Motion Planning:Potential Functions](https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf)

## State Lattice Planning

This script is a path planning code with state lattice planning.

This code uses the model predictive trajectory generator to solve boundary problem.

Ref: 

- [Optimal rough terrain trajectory generation for wheeled mobile robots](http://journals.sagepub.com/doi/pdf/10.1177/0278364906075328)

- [State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments](http://www.frc.ri.cmu.edu/~alonzo/pubs/papers/JFR_08_SS_Sampling.pdf)


### Biased polar sampling

![PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif)


### Lane sampling

![PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif)

## Probabilistic Road-Map (PRM) planning 

![PRM](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif)

This PRM planner uses Dijkstra method for graph search.

In the animation blue points are sampled points

Cyan crosses means searched points with Dijkstra method

The red line is the final path of PRM.

Ref:

- [Probabilistic roadmap \- Wikipedia](https://en.wikipedia.org/wiki/Probabilistic_roadmap)

　　

## Rapidly-Exploring Random Trees (RRT)

### RRT\*

![PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif)

This is a path planning code with RRT\*

Black circles are obstacles green line is a searched tree red crosses are start and goal positions.

Ref:

- [Incremental Sampling-based Algorithms for Optimal Motion Planning](https://arxiv.org/abs/1005.0416)

- [Sampling-based Algorithms for Optimal Motion Planning](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.419.5503&amp;rep=rep1&amp;type=pdf)

### RRT\* with reeds-sheep path

![Robotics/animation.gif at master · AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif))

Path planning for a car robot with RRT\* and reeds sheep path planner.

### LQR-RRT\*

This is a path planning simulation with LQR-RRT\*.

A double integrator motion model is used for LQR local planner.

![LQRRRT](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif)

Ref:

- [LQR\-RRT\*: Optimal Sampling\-Based Motion Planning with Automatically Derived Extension Heuristics](http://lis.csail.mit.edu/pubs/perez-icra12.pdf)

- [MahanFathi/LQR\-RRTstar: LQR\-RRT\* method is used for random motion planning of a simple pendulum in its phase plot](https://github.com/MahanFathi/LQR-RRTstar)


## Quintic polynomials planning

Motion planning with quintic polynomials.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif)

It can calculate 2D path velocity and acceleration profile based on quintic polynomials.

Ref:

- [Local Path Planning And Motion Control For Agv In Positioning](http://ieeexplore.ieee.org/document/637936/)

## Reeds Shepp planning

A sample code with Reeds Shepp path planning.

![RSPlanning](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true)

Ref:

- [15.3.2 Reeds\-Shepp Curves](http://planning.cs.uiuc.edu/node822.html) 

- [optimal paths for a car that goes both forwards and backwards](https://pdfs.semanticscholar.org/932e/c495b1d0018fd59dee12a0bf74434fac7af4.pdf)

- [ghliu/pyReedsShepp: Implementation of Reeds Shepp curve\.](https://github.com/ghliu/pyReedsShepp)


## LQR based path planning

A sample code using LQR based path planning for double integrator model.

![RSPlanning](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true)


## Optimal Trajectory in a Frenet Frame 

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif)

This is optimal trajectory generation in a Frenet Frame.

The cyan line is the target course and black crosses are obstacles.

The red line is predicted path.

Ref:

- [Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame](https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf)

- [Optimal trajectory generation for dynamic street scenarios in a Frenet Frame](https://www.youtube.com/watch?v=Cj6tAQe7UCY)


# Path Tracking

## move to a pose control

This is a simulation of moving to a pose control

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/move_to_pose/animation.gif)

Ref:

- [P. I. Corke ""Robotics Vision and Control"" \| SpringerLink p102](https://link.springer.com/book/10.1007/978-3-642-20144-8)


## Stanley control

Path tracking simulation with Stanley steering control and PID speed control.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif)

Ref:

- [Stanley: The robot that won the DARPA grand challenge](http://robots.stanford.edu/papers/thrun.stanley05.pdf)

- [Automatic Steering Methods for Autonomous Automobile Path Tracking](https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf)



## Rear wheel feedback control

Path tracking simulation with rear wheel feedback steering control and PID speed control.

![PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif)

Ref:

- [A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles](https://arxiv.org/abs/1604.07446)


## Linear–quadratic regulator (LQR) speed and steering control

Path tracking simulation with LQR speed and steering control.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif)

Ref:

- [Towards fully autonomous driving: Systems and algorithms \- IEEE Conference Publication](http://ieeexplore.ieee.org/document/5940562/)


## Model predictive speed and steering control

Path tracking simulation with iterative linear model predictive speed and steering control.

<img src=""https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif"" width=""640""/>

Ref:

- [notebook](https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathTracking/model_predictive_speed_and_steer_control/Model_predictive_speed_and_steering_control.ipynb)

## Nonlinear Model predictive control with C-GMRES

A motion planning and path tracking simulation with NMPC of C-GMRES 

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif)

Ref:

- [notebook](https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathTracking/cgmres_nmpc/cgmres_nmpc.ipynb)


# Arm Navigation

## N joint arm to point control

N joint arm to a point control simulation.

This is a interactive simulation.

You can set the goal position of the end effector with left-click on the ploting area. 

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif)

In this simulation N = 10 however you can change it.

## Arm navigation with obstacle avoidance 

Arm navigation with obstacle avoidance simulation.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif)


# Aerial Navigation

## drone 3d trajectory following 

This is a 3d trajectory following simulation for a quadrotor.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif)

## rocket powered landing

This is a 3d trajectory generation simulation for a rocket powered landing.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif)

Ref:

- [notebook](https://github.com/AtsushiSakai/PythonRobotics/blob/master/AerialNavigation/rocket_powered_landing/rocket_powered_landing.ipynb)

# Bipedal

## bipedal planner with inverted pendulum

This is a bipedal planner for modifying footsteps with inverted pendulum.

You can set the footsteps and the planner will modify those automatically.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif)

# License 

MIT

# Use-case

See: [users\_comments](https://github.com/AtsushiSakai/PythonRobotics/blob/master/users_comments.md)

# Contribution

A small PR like bug fix is welcome.

If your PR is merged multiple times I will add your account to the author list.

# Support

If you or your company would like to support this project please consider:

- [Become a backer or sponsor on Patreon](https://www.patreon.com/myenigma)

- [One-time donation via PayPal](https://www.paypal.me/myenigmapay/)

You can add your name or your company logo in README if you are a patron.

E-mail consultant is also available.

 　

Your comment using [![Say Thanks!](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/AtsushiSakai) is also welcome. 

This is a list: [Users comments](https://github.com/AtsushiSakai/PythonRobotics/blob/master/users_comments.md)

# Authors

- [Atsushi Sakai](https://github.com/AtsushiSakai/) ([@Atsushi_twi](https://twitter.com/Atsushi_twi))

- [Daniel Ingram](https://github.com/daniel-s-ingram)

- [Joe Dinius](https://github.com/jwdinius)

- [Karan Chawla](https://github.com/karanchawla)

- [Antonin RAFFIN](https://github.com/araffin)

- [Alexis Paques](https://github.com/AlexisTM)"
"Abstract:  Dialogue state tracking which estimates user goals and requests given the
dialogue context is an essential part of task-oriented dialogue systems. In
this paper we propose the Global-Locally Self-Attentive Dialogue State Tracker
(GLAD) which learns representations of the user utterance and previous system
actions with global-local modules. Our model uses global modules to share
parameters between estimators for different types (called slots) of dialogue
states and uses local modules to learn slot-specific features. We show that
this significantly improves tracking of rare states and achieves
state-of-the-art performance on the WoZ and DSTC2 state tracking tasks. GLAD
obtains 88.1% joint goal accuracy and 97.1% request accuracy on WoZ
outperforming prior work by 3.7% and 5.5%. On DSTC2 our model obtains 74.5%
joint goal accuracy and 97.5% request accuracy outperforming prior work by
1.1% and 1.0%.
# Global-Locally Self-Attentive Dialogue State Tracker

This repository contains an implementation of the [Global-Locally Self-Attentive Dialogue State Tracker (GLAD)](https://arxiv.org/abs/1805.09655).
If you use this in your work please cite the following

```
@inproceedings{ zhong2018global
  title={ Global-Locally Self-Attentive Encoder for Dialogue State Tracking }
  author={ Zhong Victor and Xiong Caiming and Socher Richard }
  booktitle={ ACL }
  year={ 2018 }
}
```


# Install dependencies

Using Docker

```
docker build -t glad:0.4 .
docker run --name embeddings -d vzhong/embeddings:0.0.5  # get the embeddings
env NV_GPU=0 nvidia-docker run --name glad -d -t --net host --volumes-from embeddings glad:0.4
```

If you do not want to build the Docker image then run the following (you still need to have the CoreNLP server).

```
pip install -r requirements.txt
```

# Download and annotate data

This project uses Stanford CoreNLP to annotate the dataset.
In particular we use the [Stanford NLP Stanza python interface](https://github.com/stanfordnlp/stanza).
To run the server do

```
docker run --name corenlp -d -p 9000:9000 vzhong/corenlp-server
```

The first time you preprocess the data we will [download word embeddings and character embeddings and put them into a SQLite database](https://github.com/vzhong/embeddings) which will be slow.
Subsequent runs will be much faster.

```
docker exec glad python preprocess_data.py
```

The raw data will be stored in `data/woz/raw` of the container.
The annotation results will be stored in `data/woz/ann` of the container.

If you do not want to build the Docker image then run

```
python preprocess_data.py
```


# Train model

You can checkout the training options via `python train.py -h`.
By default `train.py` will save checkpoints to `exp/glad/default`.

```
docker exec glad python train.py --gpu 0
```

You can attach to the container via `docker exec glad -it bin/bash` to look at what's inside or `docker cp glad /opt/glad/exp exp` to copy out the experiment results.

If you do not want to build the Docker image then run

```
python train.py --gpu 0
```


# Evaluation

You can evaluate the model using

```
docker exec glad python evaluate.py --gpu 0 --split test exp/glad/default
```

You can also dump a predictions file by specifying the `--fout` flag.
In this case the output will be a list of lists.
Each `i`th sublist is the set of predicted slot-value pairs for the `i`th turn.
Please see `evaluate.py` to see how to match up the turn predictions with the dialogues.

If you do not want to build the Docker image then run

```
python evaluate.py --gpu 0 --split test exp/glad/default
```


# Contribution

Pull requests are welcome!
If you have any questions please create an issue or contact the corresponding author at `victor <at> victorzhong <dot> com`.
</dot></at>"
"Abstract:  Recent results at the Large Hadron Collider (LHC) have pointed to enhanced
physics capabilities through the improvement of the real-time event processing
techniques. Machine learning methods are ubiquitous and have proven to be very
powerful in LHC physics and particle physics as a whole. However exploration
of the use of such techniques in low-latency low-power FPGA hardware has only
just begun. FPGA-based trigger and data acquisition (DAQ) systems have
extremely low sub-microsecond latency requirements that are unique to particle
physics. We present a case study for neural network inference in FPGAs focusing
on a classifier for jet substructure which would enable among many other
physics scenarios searches for new dark sector particles and novel
measurements of the Higgs boson. While we focus on a specific example the
lessons are far-reaching. We develop a package based on High-Level Synthesis
(HLS) called hls4ml to build machine learning models in FPGAs. The use of HLS
increases accessibility across a broad user community and allows for a drastic
decrease in firmware development time. We map out FPGA resource usage and
latency versus neural network hyperparameters to identify the problems in
particle physics that would benefit from performing neural network inference
with FPGAs. For our example jet substructure model we fit well within the
available resources of modern FPGAs with a latency on the scale of 100 ns.
# Global-Locally Self-Attentive Dialogue State Tracker

This repository contains an implementation of the [Global-Locally Self-Attentive Dialogue State Tracker (GLAD)](https://arxiv.org/abs/1805.09655).
If you use this in your work please cite the following

```
@inproceedings{ zhong2018global
  title={ Global-Locally Self-Attentive Encoder for Dialogue State Tracking }
  author={ Zhong Victor and Xiong Caiming and Socher Richard }
  booktitle={ ACL }
  year={ 2018 }
}
```


# Install dependencies

Using Docker

```
docker build -t glad:0.4 .
docker run --name embeddings -d vzhong/embeddings:0.0.5  # get the embeddings
env NV_GPU=0 nvidia-docker run --name glad -d -t --net host --volumes-from embeddings glad:0.4
```

If you do not want to build the Docker image then run the following (you still need to have the CoreNLP server).

```
pip install -r requirements.txt
```

# Download and annotate data

This project uses Stanford CoreNLP to annotate the dataset.
In particular we use the [Stanford NLP Stanza python interface](https://github.com/stanfordnlp/stanza).
To run the server do

```
docker run --name corenlp -d -p 9000:9000 vzhong/corenlp-server
```

The first time you preprocess the data we will [download word embeddings and character embeddings and put them into a SQLite database](https://github.com/vzhong/embeddings) which will be slow.
Subsequent runs will be much faster.

```
docker exec glad python preprocess_data.py
```

The raw data will be stored in `data/woz/raw` of the container.
The annotation results will be stored in `data/woz/ann` of the container.

If you do not want to build the Docker image then run

```
python preprocess_data.py
```


# Train model

You can checkout the training options via `python train.py -h`.
By default `train.py` will save checkpoints to `exp/glad/default`.

```
docker exec glad python train.py --gpu 0
```

You can attach to the container via `docker exec glad -it bin/bash` to look at what's inside or `docker cp glad /opt/glad/exp exp` to copy out the experiment results.

If you do not want to build the Docker image then run

```
python train.py --gpu 0
```


# Evaluation

You can evaluate the model using

```
docker exec glad python evaluate.py --gpu 0 --split test exp/glad/default
```

You can also dump a predictions file by specifying the `--fout` flag.
In this case the output will be a list of lists.
Each `i`th sublist is the set of predicted slot-value pairs for the `i`th turn.
Please see `evaluate.py` to see how to match up the turn predictions with the dialogues.

If you do not want to build the Docker image then run

```
python evaluate.py --gpu 0 --split test exp/glad/default
```


# Contribution

Pull requests are welcome!
If you have any questions please create an issue or contact the corresponding author at `victor <at> victorzhong <dot> com`.
</dot></at>"
"Abstract:  Highlight detection models are typically trained to identify cues that make
visual content appealing or interesting for the general public with the
objective of reducing a video to such moments. However the ""interestingness""
of a video segment or image is subjective. Thus such highlight models provide
results of limited relevance for the individual user. On the other hand
training one model per user is inefficient and requires large amounts of
personal information which is typically not available. To overcome these
limitations we present a global ranking model which conditions on each
particular user's interests. Rather than training one model per user our model
is personalized via its inputs which allows it to effectively adapt its
predictions given only a few user-specific examples. To train this model we
create a large-scale dataset of users and the GIFs they created giving us an
accurate indication of their interests. Our experiments show that using the
user history substantially improves the prediction accuracy. On our test set of
850 videos our model improves the recall by 8% with respect to generic
highlight detectors. Furthermore our method proves more precise than the
user-agnostic baselines even with just one person-specific example.
# PHD<sup>2</sup>: Personalized Highlight Detection Dataset

This repository contains PHD<sup>2</sup> a dataset with personalized highlight information.
As such it allows to train highlight detection models that use
information about the user when making predictions.
The dataset contains information from more than __13'000 users__. 

## Motivation
Highlight detection algorithms automatically reduce a
video to its most interesting moments. Even though this
notion of *interestingness is subjective* existing works only
learn *generic highlight models* which limits their potential
performance. To achieve better performance models need to rely on
information about the user for which it should extract highlights.

Previously such data was unavailable in the research community
which prevented the research on personalized highlight detection models.
Thus at gifs.com we created a large-scale dataset of users
and the GIFs they created which provides an accurate indication
of their interests.
On this data models can be trained to make personalized
and thus more accurate predictions.
Indeed as we show in our [paper](https://arxiv.org/abs/1804.06604) [1] making models personalized improves the performance over generic highlight detection
models such as [2] by a significant margin.

We are happy to publicly release this dataset to the research community
and hope that it will spur research on personalized
highlight detection.

## Dataset Description
The dataset contains information on what video segments a specific user considers a highlight.
Having this kind of data allows for strong personalization
models as specific examples of what a user is interested
in help models obtain a fine-grained understanding
of that specific user.

The data consists of YouTube videos from which gifs.com
users manually extracted their highlights by creating GIFs
from a segment of the full video.
Thus the dataset is similar to that of [1] with two major differences.
1. Each selection is associated with a user which is what allows
personalization.
2. [1] used visual matching to find the position in the video
from which a GIF was selected. Instead we directly use the timestamps
which we have internally available.
Thus the ground truth is free from any alignment errors.

__Training set__
The training set contains highlights from 12'972 users. 
You can find it [here](training.csv).

__Test set__
The test set contains highlights from 850 users. 
You can find it [here](testing.csv).

### Dataset example
![example users in our dataset](./dataset_example.png)

### More information
For more information on the dataset see our paper [1].

## Using the dateset
### Data format
The data is stored in csv files.
They have the following fields:

```
youtubeId       the source youtube video
start           start of the highlight selection (in seconds)
duration        duration of the selection (in seconds)
user_id         the ID of that user
video_duration  the full video duration
is_last         is this video the last one the user selected highlights from? (boolean)
```

Most fields should be self-explanatory.
The `is_last` field indicates if that particular video 
was the last the user extracted highlights from.
This information is needed for predicting the highlights
for a user given his previously created GIFs.

To make this more clear consider this user (id 12999) from the test set:

```csv
youtubeIdstartdurationuser_idvideo_durationis_last
dhuI9TO1rY8204.397.0812999314.0True
dhuI9TO1rY8206.295.2912999314.0True
Z-aTqtRBTIM0.07.66129998.0False
An7ylfOhlOs82.997.5512999133.0False
EFweWyBUFk4238.957.1312999561.0False
2GSS6rSn5Gk384.056.91129991027.0False
WSEsOJQJ9900.02.08129993.0False
H9tPfQNXFy8128.6078.7502112999143.0False
```

There is one video (2 rows) which has `is_last==True`.
Thus a highlight detection model that uses information about
the user would predict the users highlight selection on this video:
 ```csv
youtubeIdstartdurationuser_idvideo_durationis_last
dhuI9TO1rY8204.397.0812999314.0True
dhuI9TO1rY8206.295.2912999314.0True
```

by using the information on the previous video segments
the user selected:
```csv
youtubeIdstartdurationuser_idvideo_durationis_last
Z-aTqtRBTIM0.07.66129998.0False
An7ylfOhlOs82.997.5512999133.0False
EFweWyBUFk4238.957.1312999561.0False
2GSS6rSn5Gk384.056.91129991027.0False
WSEsOJQJ9900.02.08129993.0False
H9tPfQNXFy8128.6078.7502112999143.0False
```
 
### Downloading the videos
For using the dataset you need to first get the videos from YouTube.
We recommend using [pafy](http://pythonhosted.org/Pafy/) or
[youtube-dl](https://rg3.github.io/youtube-dl/) for this.

### Evaluation metrics
We encourage using mAP nMSD and Recall as the performance metrics.
For reference our model obtains `mAP=16.68%` `nMSD=40.26%` and `Recall@5=30.71%` while the public model of [1] obtains
`mAP=15.69%` `nMSD=42.59%` and `Recall@5=27.28%` on the 850 test videos which have `is_last==True`.
You can find the implementation on mAP and nMSD [here](https://github.com/gyglim/video2gif_dataset).

## Citation
If you use this dataset please cite our paper:

```
@article{gifs2017highlights
   author = {Gygli Michael and García del Molino Ana }
   title = {{PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation}}   
   journal = {arXiv preprint arXiv:1804.06604}
   year = {2018}
   type = {Journal Article}
}
```

## References
[1] PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation. A. García del Molino M. Gygli. ACM Multimedia 2018. https://arxiv.org/abs/1804.06604

[2] Video2GIF: Automatic Generation of Animated GIFs from Video.  M. Gygli Y. Song L. Cao. CVPR 2016
https://arxiv.org/abs/1605.04850"
"Abstract:  As machine learning becomes widely used for automated decisions attackers
have strong incentives to manipulate the results and models generated by
machine learning algorithms. In this paper we perform the first systematic
study of poisoning attacks and their countermeasures for linear regression
models. In poisoning attacks attackers deliberately influence the training
data to manipulate the results of a predictive model. We propose a
theoretically-grounded optimization framework specifically designed for linear
regression and demonstrate its effectiveness on a range of datasets and models.
We also introduce a fast statistical attack that requires limited knowledge of
the training process. Finally we design a new principled defense method that
is highly resilient against all poisoning attacks. We provide formal guarantees
about its convergence and an upper bound on the effect of poisoning attacks
when the defense is deployed. We evaluate extensively our attacks and defenses
on three realistic datasets from health care loan assessment and real estate
domains.
# manip-ml
Code for the IEEE S&amp;P 2018 paper 'Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning'"
"Abstract:  Simulation studies are computer experiments that involve creating data by
pseudorandom sampling. The key strength of simulation studies is the ability to
understand the behaviour of statistical methods because some 'truth' (usually
some parameter/s of interest) is known from the process of generating the data.
This allows us to consider properties of methods such as bias. While widely
used simulation studies are often poorly designed analysed and reported. This
tutorial outlines the rationale for using simulation studies and offers
guidance for design execution analysis reporting and presentation. In
particular this tutorial provides: a structured approach for planning and
reporting simulation studies which involves defining aims data-generating
mechanisms estimands methods and performance measures ('ADEMP'); coherent
terminology for simulation studies; guidance on coding simulation studies; a
critical discussion of key performance measures and their estimation; guidance
on structuring tabular and graphical presentation of results; and new graphical
presentations. With a view to describing recent practice we review 100
articles taken from Volume 34 of Statistics in Medicine that included at least
one simulation study and identify areas for improvement.
# Stata code for running a simple simulation study
The files contained in this repository are provided for for readers of [Morris White and Crowther's tutorial paper](https://arxiv.org/abs/1712.03198) to run the example simulation study.

## Running the `.do` files
For those running the files (rather than just reading them) note that `simstudy.do` should be run first since this is the core of the simulation. It produces the data files `estimates.dta` `estimateslabels.dta` (a cleanly labelled version of estimates) and `states.dta`. However these data files are also provided here.

## Stata versions
Relatively recent versions of Stata are needed:   
* `simstudy.do` requires version 14 (Stata's random number generator changed from using KISS to Mersenne twister and the form of `c(rngstate)` also changed to be more complex; This file shows how to handle the resulting &gt;5000 character strings defining the random number generator state).
* `ansim-simsum.do` requires version 14 (due to the `ci proportions` command).
* `ansim-pointplots.do` requires version 15 (the graphs use the translucency features introduced at version 15)
* `ansim-zipplot.do` requires version 15 (the graphs use the translucency features introduced at version 15)
The guts of these files would work in older versions (down to 11.2) and could be adapted by users.

## User-written packages
To run `simstudy.do` the user-written package `survsim` is required. This can be installed with:   
`. ssc install survsim`   
See: [Crowther MJ and Lambert PC. Simulating complex survival data. The Stata Journal 2012;12(4):674-687.](http://www.stata-journal.com/article.html?article=st0275)   

Similarly to run `ansim-simsum.do` submit:   
`. ssc install simsum`   
See: [White IR. simsum: Analyses of simulation studies including Monte Carlo error. The Stata Journal 2010;10(3):369-385](http://www.stata-journal.com/article.html?article=st0200)   

Note that the graphs presented in the [tutorial](https://arxiv.org/abs/1712.03198) used the MRC graph scheme which can be downloaded using:
`. ssc install scheme-mrc`
and invoked with
`. set scheme mrc`

## Bugs issues and improvements
Please do let us know of any issues you discover in these files and we will endeavor to acknowledge you here.

## To-do
**Produce R scripts to perform the same simulation study**   
Background: In Stata saving estimates and random number states are what trips people up. This is straightforward to do in R. Many of the simulation studies we reviewed in [https://arxiv.org/abs/1712.03198](https://arxiv.org/abs/1712.03198) were coded in R and very few in Stata. We therefore judged that the Stata community were more likely to want these files and did not expect demand from R users. However having now received several requests from R users we are working on this."
"Abstract:  Feature selection as a data preprocessing strategy has been proven to be
effective and efficient in preparing data (especially high-dimensional data)
for various data mining and machine learning problems. The objectives of
feature selection include: building simpler and more comprehensible models
improving data mining performance and preparing clean understandable data.
The recent proliferation of big data has presented some substantial challenges
and opportunities to feature selection. In this survey we provide a
comprehensive and structured overview of recent advances in feature selection
research. Motivated by current challenges and opportunities in the era of big
data we revisit feature selection research from a data perspective and review
representative feature selection algorithms for conventional data structured
data heterogeneous data and streaming data. Methodologically to emphasize the
differences and similarities of most existing feature selection algorithms for
conventional data we categorize them into four main groups: similarity based
information theoretical based sparse learning based and statistical based
methods. To facilitate and promote the research in this community we also
present an open-source feature selection repository that consists of most of
the popular feature selection algorithms
(\url{this http URL}). Also we use it as an example to show
how to evaluate feature selection algorithms. At the end of the survey we
present a discussion about some open problems and challenges that require more
attention in future research.
scikit-feature
===============================
Feature selection repository scikit-feature in Python (DMML Lab@ASU). 

scikit-feature is an open-source feature selection repository in Python developed by Data Mining and Machine Learning Lab at Arizona State University. It is built upon one widely used machine learning package scikit-learn and two scientific computing packages Numpy and Scipy. scikit-feature contains around 40 popular feature selection algorithms including traditional feature selection algorithms and some structural and streaming feature selection algorithms. 

It serves as a platform for facilitating feature selection application research and comparative study. It is designed to share widely used feature selection algorithms developed in the feature selection research and offer convenience for researchers and practitioners to perform empirical evaluation in developing new feature selection algorithms.

## Installing scikit-feature
### Prerequisites:
Python 2.7 *and Python 3*

NumPy

SciPy

Scikit-learn

### Steps:
After you download scikit-feature-1.0.0.zip from the project website (http://featureselection.asu.edu/) unzip the file.

For Linux users you can install the repository by the following command:

    python setup.py install

For Windows users you can also install the repository by the following command:

    setup.py install

## Project website
Instructions of using this repository can be found in our project webpage at http://featureselection.asu.edu/

## Citation

If you find scikit-feature feature selection reposoitory useful in your research please consider citing the following paper::

    @article{li2016feature
       title={Feature Selection: A Data Perspective}
       author={Li Jundong and Cheng Kewei and Wang Suhang and Morstatter Fred and Trevino Robert P and Tang Jiliang and Liu Huan}
       journal={arXiv preprint arXiv:1601.07996}
       year={2016}
    }
    
## Contact
Jundong Li
E-mail: jundong.li@asu.edu

Kewei Cheng
E-mail: kcheng18@asu.edu"
"Abstract:  Recently a number of existing blockchain systems have witnessed major bugs
and vulnerabilities within smart contracts. Although the literature features a
number of proposals for securing smart contracts these proposals mostly focus
on proving the correctness or absence of a certain type of vulnerability within
a contract but cannot protect deployed (legacy) contracts from being
exploited. In this paper we address this problem in the context of re-entrancy
exploits and propose a novel smart contract security technology dubbed Sereum
(Secure Ethereum) which protects existing deployed contracts against
re-entrancy attacks in a backwards compatible way based on run-time monitoring
and validation. Sereum does neither require any modification nor any semantic
knowledge of existing contracts. By means of implementation and evaluation
using the Ethereum blockchain we show that Sereum covers the actual execution
flow of a smart contract to accurately detect and prevent attacks with a false
positive rate as small as 0.06% and with negligible run-time overhead. As a
by-product we develop three advanced re-entrancy attacks to demonstrate the
limitations of existing offline vulnerability analysis tools.
# Re-Entrancy Attack Patterns

These attack patterns were discovered during evaluation of `Sereum` a runtime
monitoring solution for re-entrancy attacks which utilizes taint tracking and
dynamic write locks to detect and prevent re-entrancy attacks. For more
information please refer to our paper *""Sereum: Protecting Existing Smart
Contracts Against Re-Entrancy Attacks""* ([arxiv preprint](https://arxiv.org/abs/1812.05934)).

For every type of attack pattern this repository contains a small example
implementation of a vulnerable contract and an attack. The source code of the
vulnerable and attacker contracts are contained in the `*.sol` files. We also
provide a `*_setup.js` file for every example which deploys the contracts on a
dev blockchain and exploits the vulnerability in the example contract. The
scripts assume they're run in the geth dev mode blockchain (`geth --dev`).

### Cross-function re-entrancy

**Example:** `./cross-function.sol`

The Token contract in the example is vulnerable to a re-entrancy attack
starting with the `withdrawAll` function. However the attacker cannot
re-enter the `withdrawAll`. Instead the attacker has to re-enter the contract
at the `exchangeAndWithdrawToken` to exploit the bug and drain the vulnerable
contract from ether.

### Delegated re-entrancy

**Example:** `./delegated.sol`

The `Bank` contract utilizes a library called via `delegatecall` for
performing the ether sending. This obfuscates the re-entrancy vulnerability in
the `withdraw` function. Any static analysis tool will not be able to detect
this vulnerability when analyzing only the `Bank` contract and not the
combination of the contract and its libraries.

### Create-based re-entrancy

**Example:** `./create-based.sol`

In this example multiple contracts interact with each other. The `Bank`
contract utilizes the `CREATE` instruction (i.e. `new` in solidity) to create
new subcontracts. Contract creation immediately triggers the execution of the
constructor of the newly created contract. This constructor can perform
external calls to the unknown. This can lead to re-entrancy scenarios where
the attacker re-enters a contract during execution of a sub-contracts
constructor. For static analysis tools to catch these kinds of problems they
must (1) also analyze combination of contracts and (2) consider the `CREATE`
instruction as an external call similar to the `CALL` instruction.


## Tested Tools

The following table lists the tools and versions we tested. If the tool detects
the test-case we mark it with ""Yes"" otherwise ""No"". Mythril and Securify use
a very conservative policy that marks every state update after an external
call. This would prevent all re-entrancy vulnerabilities but also results in a
high number of false positives. For example for create-based re-entrancy
vulnerabilities it is highly likely that the creater of the contract will
want to modify the state (e.g. registering the address of the newly created
contract).

| Tool          | Version     | Simple                 | Cross-Function         | Delegated | Create-based | 
| ------------- | ----------- | ---------------------- | ---------------------- | --------- | ------------ |
| Oyente        | 0.2.7       | Yes                    | No                     | No<sup>1</sup> | No      |
| Mythril       | v0.19.9     | Partial (conservative) | Partial (conservative) | No        | Partial (conservative) |
| Securify      | 2018-08-01  | Partial (conservative) | Partial (conservative) | No        | No           |
| Manticore<sup>2</sup> | 0.2.2 | Yes                  | Yes<sup>3</sup>        | No        | No           |
| ECFChecker    | geth1.8port | Yes                    | Yes<sup>4</sup>        | Yes       | No           |
| Sereum        |             | Yes                    | Yes                    | Yes       | Yes          |


* <sup>1</sup> Oyente detects a re-entrancy in the Library contract. However 
  the library contract itself is arguably not vulnerable to re-entrancy.
* <sup>2</sup> We evaluate the detector enabled with `--detect-reentrancy-advanced`.
  The other detector `--detect-reentrancy` uses a similar policy to Mythril and
  Securify.
* <sup>3</sup> However other tests (e.g. `manual-lock.sol`) show that
  Manticore is sometimes not as accurate and reports re-entrancy attacks even
  though they're not really possible.
* <sup>4</sup> However we crafted a different example for a cross-function
  re-entrancy attack that is not detected by ECFChecker. See the next section 
  for details.


## Testcase: manual lock

The file `manual-lock.sol` contains several versions of the same contract. These
contracts can be used to investigate the quality of re-entrancy detection tools.
This file contains three functionally equivalent contracts:

* `VulnBankNoLock` is vulnerable to simple same function re-entrancy.
* `VulnBankBuggyLock` is vulnerable to cross-function re-entrancy due to a 
  incomplete locking mechanism.
* `VulnBankSecureLock` is not vulnerable due to the locking mechanism. However
  the locking mechanism can result in a false positive.

Furthermore there are two types of attacks implemented against all of these
contracts.

* `MallorySameFunction` implements simple same-function re-entrancy
* `MalloryCrossFunction` implements a cross-function re-entrancy attack

Static analysis tools have a hard time correctly analysing the contracts. Oyente
detects only the simple re-entrancy vulnerability and does not report the
cross-function re-entrancy. Manticore on the other hand detects a re-entrancy
bug in both the BuggyLock and SecureLock version resulting in a false positive.

| Tool \ Testcase | NoLock   | BuggyLock | SecureLock | 
| --------------- | -------- | --------- | ---------- |
| Oyente          | Yes      | No        | No         |
| Manticore       | Yes      | Yes       | Yes        |
| Mythril         | Yes      | Yes       | Yes        |
| Expected        | Yes      | Yes       | No         |

For the dynamic analysis tools we use several combinations of vulnerable
contracts and attack contracts. We verify whether the tool detects an attack
against the same-function and cross-function re-entrancy attack.

| Testcase \ Tool            | ECFChecker | Sereum    | Expected |
| ---------------------------| ---------- | --------- | -------- |
| NoLock + SameFunction      | Yes        | Yes       | Yes      |
| NoLock + CrossFunction     | No         | Yes       | Yes      |
| BuggyLock + SameFunction   | No         | Yes       | No       |
| BuggyLock + CrossFunction  | No         | Yes       | Yes      |
| SecureLock + SameFunction  | No         | Yes       | No       |
| SecureLock + CrossFunction | No         | Yes       | No       |

The reason Sereum reports all contracts is that the locking mechanism itself
does exhibit exactly the same pattern as an re-entrancy attack. So Sereum
reports an re-entrancy attack on the lock variables because Sereum cannot know
the semantics of the lock variables.


## Unconditional Re-Entrancy

**Example:** `./unconditional.sol`

Typically a re-entrancy attack will try to subvert a business logic check of an
application. Every check (`if` `require` `assert` etc.) is implemented as a
conditional jump (`JUMPI`) on the EVM level. While certainly unlikely it is
possible to write a contract which does not perform any check on anything
before sending ether. In this example the functionality transfers all the ether
a user has invested. This example is exploitable only with a re-entrancy
vulnerability. Currently this example is not detected by Sereum since we
assume that this is a rather unlikely case. We plan to detect this kind of
Vulnerabilities in a future versions of Sereum.

| Tool            | Detected | 
| --------------- | --- |
| Oyente          | Yes |
| Manticore       | Yes |
| Mythril         | Yes |
| ECFChecker      | Yes |
| Sereum          | No  |

Another very simple example is the following contract which is deployed on the Ethereum blockchain at [0xb7c5c5aa4d42967efe906e1b66cb8df9cebf04f7](https://etherscan.io/address/0xb7c5c5aa4d42967efe906e1b66cb8df9cebf04f7#code).

## Citing in Academic Work

If you want to refer to these attack patterns in academic work please cite the
following paper:

```bibtex
@inproceedings{sereum-ndss19
  title     = ""Sereum: Protecting Existing Smart Contracts Against Re-Entrancy Attacks""
  booktitle = ""Proceedings of the Network and Distributed System Security Symposium ({NDSS'19})""
  author    = ""Rodler Michael and Li Wenting and Karame Ghassan and Davi Lucas""
  year      =  2019
}
```"
"Abstract:  DarkCapPy is a Python 3/Jupyter package for calculating rates associated with
dark matter capture in the Earth annihilation into light mediators and the
subsequent observable decay of the light mediators near the surface of the
Earth. The package includes a calculation of the Sommerfeld enhancement at the
center of the Earth and the timescale for capture--annihilation equilibrium.
The code is open source and can be modified for other compact astronomical
objects and mediator spins.
# Re-Entrancy Attack Patterns

These attack patterns were discovered during evaluation of `Sereum` a runtime
monitoring solution for re-entrancy attacks which utilizes taint tracking and
dynamic write locks to detect and prevent re-entrancy attacks. For more
information please refer to our paper *""Sereum: Protecting Existing Smart
Contracts Against Re-Entrancy Attacks""* ([arxiv preprint](https://arxiv.org/abs/1812.05934)).

For every type of attack pattern this repository contains a small example
implementation of a vulnerable contract and an attack. The source code of the
vulnerable and attacker contracts are contained in the `*.sol` files. We also
provide a `*_setup.js` file for every example which deploys the contracts on a
dev blockchain and exploits the vulnerability in the example contract. The
scripts assume they're run in the geth dev mode blockchain (`geth --dev`).

### Cross-function re-entrancy

**Example:** `./cross-function.sol`

The Token contract in the example is vulnerable to a re-entrancy attack
starting with the `withdrawAll` function. However the attacker cannot
re-enter the `withdrawAll`. Instead the attacker has to re-enter the contract
at the `exchangeAndWithdrawToken` to exploit the bug and drain the vulnerable
contract from ether.

### Delegated re-entrancy

**Example:** `./delegated.sol`

The `Bank` contract utilizes a library called via `delegatecall` for
performing the ether sending. This obfuscates the re-entrancy vulnerability in
the `withdraw` function. Any static analysis tool will not be able to detect
this vulnerability when analyzing only the `Bank` contract and not the
combination of the contract and its libraries.

### Create-based re-entrancy

**Example:** `./create-based.sol`

In this example multiple contracts interact with each other. The `Bank`
contract utilizes the `CREATE` instruction (i.e. `new` in solidity) to create
new subcontracts. Contract creation immediately triggers the execution of the
constructor of the newly created contract. This constructor can perform
external calls to the unknown. This can lead to re-entrancy scenarios where
the attacker re-enters a contract during execution of a sub-contracts
constructor. For static analysis tools to catch these kinds of problems they
must (1) also analyze combination of contracts and (2) consider the `CREATE`
instruction as an external call similar to the `CALL` instruction.


## Tested Tools

The following table lists the tools and versions we tested. If the tool detects
the test-case we mark it with ""Yes"" otherwise ""No"". Mythril and Securify use
a very conservative policy that marks every state update after an external
call. This would prevent all re-entrancy vulnerabilities but also results in a
high number of false positives. For example for create-based re-entrancy
vulnerabilities it is highly likely that the creater of the contract will
want to modify the state (e.g. registering the address of the newly created
contract).

| Tool          | Version     | Simple                 | Cross-Function         | Delegated | Create-based | 
| ------------- | ----------- | ---------------------- | ---------------------- | --------- | ------------ |
| Oyente        | 0.2.7       | Yes                    | No                     | No<sup>1</sup> | No      |
| Mythril       | v0.19.9     | Partial (conservative) | Partial (conservative) | No        | Partial (conservative) |
| Securify      | 2018-08-01  | Partial (conservative) | Partial (conservative) | No        | No           |
| Manticore<sup>2</sup> | 0.2.2 | Yes                  | Yes<sup>3</sup>        | No        | No           |
| ECFChecker    | geth1.8port | Yes                    | Yes<sup>4</sup>        | Yes       | No           |
| Sereum        |             | Yes                    | Yes                    | Yes       | Yes          |


* <sup>1</sup> Oyente detects a re-entrancy in the Library contract. However 
  the library contract itself is arguably not vulnerable to re-entrancy.
* <sup>2</sup> We evaluate the detector enabled with `--detect-reentrancy-advanced`.
  The other detector `--detect-reentrancy` uses a similar policy to Mythril and
  Securify.
* <sup>3</sup> However other tests (e.g. `manual-lock.sol`) show that
  Manticore is sometimes not as accurate and reports re-entrancy attacks even
  though they're not really possible.
* <sup>4</sup> However we crafted a different example for a cross-function
  re-entrancy attack that is not detected by ECFChecker. See the next section 
  for details.


## Testcase: manual lock

The file `manual-lock.sol` contains several versions of the same contract. These
contracts can be used to investigate the quality of re-entrancy detection tools.
This file contains three functionally equivalent contracts:

* `VulnBankNoLock` is vulnerable to simple same function re-entrancy.
* `VulnBankBuggyLock` is vulnerable to cross-function re-entrancy due to a 
  incomplete locking mechanism.
* `VulnBankSecureLock` is not vulnerable due to the locking mechanism. However
  the locking mechanism can result in a false positive.

Furthermore there are two types of attacks implemented against all of these
contracts.

* `MallorySameFunction` implements simple same-function re-entrancy
* `MalloryCrossFunction` implements a cross-function re-entrancy attack

Static analysis tools have a hard time correctly analysing the contracts. Oyente
detects only the simple re-entrancy vulnerability and does not report the
cross-function re-entrancy. Manticore on the other hand detects a re-entrancy
bug in both the BuggyLock and SecureLock version resulting in a false positive.

| Tool \ Testcase | NoLock   | BuggyLock | SecureLock | 
| --------------- | -------- | --------- | ---------- |
| Oyente          | Yes      | No        | No         |
| Manticore       | Yes      | Yes       | Yes        |
| Mythril         | Yes      | Yes       | Yes        |
| Expected        | Yes      | Yes       | No         |

For the dynamic analysis tools we use several combinations of vulnerable
contracts and attack contracts. We verify whether the tool detects an attack
against the same-function and cross-function re-entrancy attack.

| Testcase \ Tool            | ECFChecker | Sereum    | Expected |
| ---------------------------| ---------- | --------- | -------- |
| NoLock + SameFunction      | Yes        | Yes       | Yes      |
| NoLock + CrossFunction     | No         | Yes       | Yes      |
| BuggyLock + SameFunction   | No         | Yes       | No       |
| BuggyLock + CrossFunction  | No         | Yes       | Yes      |
| SecureLock + SameFunction  | No         | Yes       | No       |
| SecureLock + CrossFunction | No         | Yes       | No       |

The reason Sereum reports all contracts is that the locking mechanism itself
does exhibit exactly the same pattern as an re-entrancy attack. So Sereum
reports an re-entrancy attack on the lock variables because Sereum cannot know
the semantics of the lock variables.


## Unconditional Re-Entrancy

**Example:** `./unconditional.sol`

Typically a re-entrancy attack will try to subvert a business logic check of an
application. Every check (`if` `require` `assert` etc.) is implemented as a
conditional jump (`JUMPI`) on the EVM level. While certainly unlikely it is
possible to write a contract which does not perform any check on anything
before sending ether. In this example the functionality transfers all the ether
a user has invested. This example is exploitable only with a re-entrancy
vulnerability. Currently this example is not detected by Sereum since we
assume that this is a rather unlikely case. We plan to detect this kind of
Vulnerabilities in a future versions of Sereum.

| Tool            | Detected | 
| --------------- | --- |
| Oyente          | Yes |
| Manticore       | Yes |
| Mythril         | Yes |
| ECFChecker      | Yes |
| Sereum          | No  |

Another very simple example is the following contract which is deployed on the Ethereum blockchain at [0xb7c5c5aa4d42967efe906e1b66cb8df9cebf04f7](https://etherscan.io/address/0xb7c5c5aa4d42967efe906e1b66cb8df9cebf04f7#code).

## Citing in Academic Work

If you want to refer to these attack patterns in academic work please cite the
following paper:

```bibtex
@inproceedings{sereum-ndss19
  title     = ""Sereum: Protecting Existing Smart Contracts Against Re-Entrancy Attacks""
  booktitle = ""Proceedings of the Network and Distributed System Security Symposium ({NDSS'19})""
  author    = ""Rodler Michael and Li Wenting and Karame Ghassan and Davi Lucas""
  year      =  2019
}
```"
"Abstract:  TensorFlow is a machine learning system that operates at large scale and in
heterogeneous environments. TensorFlow uses dataflow graphs to represent
computation shared state and the operations that mutate that state. It maps
the nodes of a dataflow graph across many machines in a cluster and within a
machine across multiple computational devices including multicore CPUs
general-purpose GPUs and custom designed ASICs known as Tensor Processing
Units (TPUs). This architecture gives flexibility to the application developer:
whereas in previous ""parameter server"" designs the management of shared state
is built into the system TensorFlow enables developers to experiment with
novel optimizations and training algorithms. TensorFlow supports a variety of
applications with particularly strong support for training and inference on
deep neural networks. Several Google services use TensorFlow in production we
have released it as an open-source project and it has become widely used for
machine learning research. In this paper we describe the TensorFlow dataflow
model in contrast to existing systems and demonstrate the compelling
performance that TensorFlow achieves for several real-world applications.
# Re-Entrancy Attack Patterns

These attack patterns were discovered during evaluation of `Sereum` a runtime
monitoring solution for re-entrancy attacks which utilizes taint tracking and
dynamic write locks to detect and prevent re-entrancy attacks. For more
information please refer to our paper *""Sereum: Protecting Existing Smart
Contracts Against Re-Entrancy Attacks""* ([arxiv preprint](https://arxiv.org/abs/1812.05934)).

For every type of attack pattern this repository contains a small example
implementation of a vulnerable contract and an attack. The source code of the
vulnerable and attacker contracts are contained in the `*.sol` files. We also
provide a `*_setup.js` file for every example which deploys the contracts on a
dev blockchain and exploits the vulnerability in the example contract. The
scripts assume they're run in the geth dev mode blockchain (`geth --dev`).

### Cross-function re-entrancy

**Example:** `./cross-function.sol`

The Token contract in the example is vulnerable to a re-entrancy attack
starting with the `withdrawAll` function. However the attacker cannot
re-enter the `withdrawAll`. Instead the attacker has to re-enter the contract
at the `exchangeAndWithdrawToken` to exploit the bug and drain the vulnerable
contract from ether.

### Delegated re-entrancy

**Example:** `./delegated.sol`

The `Bank` contract utilizes a library called via `delegatecall` for
performing the ether sending. This obfuscates the re-entrancy vulnerability in
the `withdraw` function. Any static analysis tool will not be able to detect
this vulnerability when analyzing only the `Bank` contract and not the
combination of the contract and its libraries.

### Create-based re-entrancy

**Example:** `./create-based.sol`

In this example multiple contracts interact with each other. The `Bank`
contract utilizes the `CREATE` instruction (i.e. `new` in solidity) to create
new subcontracts. Contract creation immediately triggers the execution of the
constructor of the newly created contract. This constructor can perform
external calls to the unknown. This can lead to re-entrancy scenarios where
the attacker re-enters a contract during execution of a sub-contracts
constructor. For static analysis tools to catch these kinds of problems they
must (1) also analyze combination of contracts and (2) consider the `CREATE`
instruction as an external call similar to the `CALL` instruction.


## Tested Tools

The following table lists the tools and versions we tested. If the tool detects
the test-case we mark it with ""Yes"" otherwise ""No"". Mythril and Securify use
a very conservative policy that marks every state update after an external
call. This would prevent all re-entrancy vulnerabilities but also results in a
high number of false positives. For example for create-based re-entrancy
vulnerabilities it is highly likely that the creater of the contract will
want to modify the state (e.g. registering the address of the newly created
contract).

| Tool          | Version     | Simple                 | Cross-Function         | Delegated | Create-based | 
| ------------- | ----------- | ---------------------- | ---------------------- | --------- | ------------ |
| Oyente        | 0.2.7       | Yes                    | No                     | No<sup>1</sup> | No      |
| Mythril       | v0.19.9     | Partial (conservative) | Partial (conservative) | No        | Partial (conservative) |
| Securify      | 2018-08-01  | Partial (conservative) | Partial (conservative) | No        | No           |
| Manticore<sup>2</sup> | 0.2.2 | Yes                  | Yes<sup>3</sup>        | No        | No           |
| ECFChecker    | geth1.8port | Yes                    | Yes<sup>4</sup>        | Yes       | No           |
| Sereum        |             | Yes                    | Yes                    | Yes       | Yes          |


* <sup>1</sup> Oyente detects a re-entrancy in the Library contract. However 
  the library contract itself is arguably not vulnerable to re-entrancy.
* <sup>2</sup> We evaluate the detector enabled with `--detect-reentrancy-advanced`.
  The other detector `--detect-reentrancy` uses a similar policy to Mythril and
  Securify.
* <sup>3</sup> However other tests (e.g. `manual-lock.sol`) show that
  Manticore is sometimes not as accurate and reports re-entrancy attacks even
  though they're not really possible.
* <sup>4</sup> However we crafted a different example for a cross-function
  re-entrancy attack that is not detected by ECFChecker. See the next section 
  for details.


## Testcase: manual lock

The file `manual-lock.sol` contains several versions of the same contract. These
contracts can be used to investigate the quality of re-entrancy detection tools.
This file contains three functionally equivalent contracts:

* `VulnBankNoLock` is vulnerable to simple same function re-entrancy.
* `VulnBankBuggyLock` is vulnerable to cross-function re-entrancy due to a 
  incomplete locking mechanism.
* `VulnBankSecureLock` is not vulnerable due to the locking mechanism. However
  the locking mechanism can result in a false positive.

Furthermore there are two types of attacks implemented against all of these
contracts.

* `MallorySameFunction` implements simple same-function re-entrancy
* `MalloryCrossFunction` implements a cross-function re-entrancy attack

Static analysis tools have a hard time correctly analysing the contracts. Oyente
detects only the simple re-entrancy vulnerability and does not report the
cross-function re-entrancy. Manticore on the other hand detects a re-entrancy
bug in both the BuggyLock and SecureLock version resulting in a false positive.

| Tool \ Testcase | NoLock   | BuggyLock | SecureLock | 
| --------------- | -------- | --------- | ---------- |
| Oyente          | Yes      | No        | No         |
| Manticore       | Yes      | Yes       | Yes        |
| Mythril         | Yes      | Yes       | Yes        |
| Expected        | Yes      | Yes       | No         |

For the dynamic analysis tools we use several combinations of vulnerable
contracts and attack contracts. We verify whether the tool detects an attack
against the same-function and cross-function re-entrancy attack.

| Testcase \ Tool            | ECFChecker | Sereum    | Expected |
| ---------------------------| ---------- | --------- | -------- |
| NoLock + SameFunction      | Yes        | Yes       | Yes      |
| NoLock + CrossFunction     | No         | Yes       | Yes      |
| BuggyLock + SameFunction   | No         | Yes       | No       |
| BuggyLock + CrossFunction  | No         | Yes       | Yes      |
| SecureLock + SameFunction  | No         | Yes       | No       |
| SecureLock + CrossFunction | No         | Yes       | No       |

The reason Sereum reports all contracts is that the locking mechanism itself
does exhibit exactly the same pattern as an re-entrancy attack. So Sereum
reports an re-entrancy attack on the lock variables because Sereum cannot know
the semantics of the lock variables.


## Unconditional Re-Entrancy

**Example:** `./unconditional.sol`

Typically a re-entrancy attack will try to subvert a business logic check of an
application. Every check (`if` `require` `assert` etc.) is implemented as a
conditional jump (`JUMPI`) on the EVM level. While certainly unlikely it is
possible to write a contract which does not perform any check on anything
before sending ether. In this example the functionality transfers all the ether
a user has invested. This example is exploitable only with a re-entrancy
vulnerability. Currently this example is not detected by Sereum since we
assume that this is a rather unlikely case. We plan to detect this kind of
Vulnerabilities in a future versions of Sereum.

| Tool            | Detected | 
| --------------- | --- |
| Oyente          | Yes |
| Manticore       | Yes |
| Mythril         | Yes |
| ECFChecker      | Yes |
| Sereum          | No  |

Another very simple example is the following contract which is deployed on the Ethereum blockchain at [0xb7c5c5aa4d42967efe906e1b66cb8df9cebf04f7](https://etherscan.io/address/0xb7c5c5aa4d42967efe906e1b66cb8df9cebf04f7#code).

## Citing in Academic Work

If you want to refer to these attack patterns in academic work please cite the
following paper:

```bibtex
@inproceedings{sereum-ndss19
  title     = ""Sereum: Protecting Existing Smart Contracts Against Re-Entrancy Attacks""
  booktitle = ""Proceedings of the Network and Distributed System Security Symposium ({NDSS'19})""
  author    = ""Rodler Michael and Li Wenting and Karame Ghassan and Davi Lucas""
  year      =  2019
}
```"
"Abstract:  The population of fast radio bursts (FRBs) will continue to diverge into two
groups depending on their method of discovery: those which can be localised
and those which cannot. Events potentially less useful for astronomical and
cosmological purposes due to limited localisation will accumulate with the
advent of new facilities and continued efforts by e.g. the SUPERB
collaboration which may require afterglows or multi-wavelength counterparts
for sub-arcsecond localisation. It is important to exploit these sources to
their maximum scientific potential. We perform analysis of FRB dispersion
measures (DMs) considering different theoretical FRB progenitors with view to
place more rigorous constraints on FRB redshifts in particular for large
statistical samples via their DMs. We review FRB DM components and build
redshift-scalable probability distributions corresponding to different
progenitor scenarios. We combine these components into a framework for
obtaining FRB DM probabilities given their redshifts. Taking into account
different possibilities for the evolution of progenitors across cosmic time we
invert this model thus deriving redshift constraints. Effects of varying FRB
progenitor models are illustrated. While as expected host galaxy DM
contributions become decreasingly important with increasing redshift for
AGN-like progenitor scenarios they could remain significant out to redshift 3.
Constraints are placed on redshifts of catalogued FRBs with various models and
increasingly realistic models may be employed as general understanding of FRBs
improves. For localised FRBs we highlight future prospects for disentangling
host and intergalactic medium DM components using their respective redshift
scaling. We identify a use for large samples of unlocalised FRBs resulting from
upcoming flux-limited surveys such as with CHIME in mapping out the Milky Way
contribution to the DM.
# Re-Entrancy Attack Patterns

These attack patterns were discovered during evaluation of `Sereum` a runtime
monitoring solution for re-entrancy attacks which utilizes taint tracking and
dynamic write locks to detect and prevent re-entrancy attacks. For more
information please refer to our paper *""Sereum: Protecting Existing Smart
Contracts Against Re-Entrancy Attacks""* ([arxiv preprint](https://arxiv.org/abs/1812.05934)).

For every type of attack pattern this repository contains a small example
implementation of a vulnerable contract and an attack. The source code of the
vulnerable and attacker contracts are contained in the `*.sol` files. We also
provide a `*_setup.js` file for every example which deploys the contracts on a
dev blockchain and exploits the vulnerability in the example contract. The
scripts assume they're run in the geth dev mode blockchain (`geth --dev`).

### Cross-function re-entrancy

**Example:** `./cross-function.sol`

The Token contract in the example is vulnerable to a re-entrancy attack
starting with the `withdrawAll` function. However the attacker cannot
re-enter the `withdrawAll`. Instead the attacker has to re-enter the contract
at the `exchangeAndWithdrawToken` to exploit the bug and drain the vulnerable
contract from ether.

### Delegated re-entrancy

**Example:** `./delegated.sol`

The `Bank` contract utilizes a library called via `delegatecall` for
performing the ether sending. This obfuscates the re-entrancy vulnerability in
the `withdraw` function. Any static analysis tool will not be able to detect
this vulnerability when analyzing only the `Bank` contract and not the
combination of the contract and its libraries.

### Create-based re-entrancy

**Example:** `./create-based.sol`

In this example multiple contracts interact with each other. The `Bank`
contract utilizes the `CREATE` instruction (i.e. `new` in solidity) to create
new subcontracts. Contract creation immediately triggers the execution of the
constructor of the newly created contract. This constructor can perform
external calls to the unknown. This can lead to re-entrancy scenarios where
the attacker re-enters a contract during execution of a sub-contracts
constructor. For static analysis tools to catch these kinds of problems they
must (1) also analyze combination of contracts and (2) consider the `CREATE`
instruction as an external call similar to the `CALL` instruction.


## Tested Tools

The following table lists the tools and versions we tested. If the tool detects
the test-case we mark it with ""Yes"" otherwise ""No"". Mythril and Securify use
a very conservative policy that marks every state update after an external
call. This would prevent all re-entrancy vulnerabilities but also results in a
high number of false positives. For example for create-based re-entrancy
vulnerabilities it is highly likely that the creater of the contract will
want to modify the state (e.g. registering the address of the newly created
contract).

| Tool          | Version     | Simple                 | Cross-Function         | Delegated | Create-based | 
| ------------- | ----------- | ---------------------- | ---------------------- | --------- | ------------ |
| Oyente        | 0.2.7       | Yes                    | No                     | No<sup>1</sup> | No      |
| Mythril       | v0.19.9     | Partial (conservative) | Partial (conservative) | No        | Partial (conservative) |
| Securify      | 2018-08-01  | Partial (conservative) | Partial (conservative) | No        | No           |
| Manticore<sup>2</sup> | 0.2.2 | Yes                  | Yes<sup>3</sup>        | No        | No           |
| ECFChecker    | geth1.8port | Yes                    | Yes<sup>4</sup>        | Yes       | No           |
| Sereum        |             | Yes                    | Yes                    | Yes       | Yes          |


* <sup>1</sup> Oyente detects a re-entrancy in the Library contract. However 
  the library contract itself is arguably not vulnerable to re-entrancy.
* <sup>2</sup> We evaluate the detector enabled with `--detect-reentrancy-advanced`.
  The other detector `--detect-reentrancy` uses a similar policy to Mythril and
  Securify.
* <sup>3</sup> However other tests (e.g. `manual-lock.sol`) show that
  Manticore is sometimes not as accurate and reports re-entrancy attacks even
  though they're not really possible.
* <sup>4</sup> However we crafted a different example for a cross-function
  re-entrancy attack that is not detected by ECFChecker. See the next section 
  for details.


## Testcase: manual lock

The file `manual-lock.sol` contains several versions of the same contract. These
contracts can be used to investigate the quality of re-entrancy detection tools.
This file contains three functionally equivalent contracts:

* `VulnBankNoLock` is vulnerable to simple same function re-entrancy.
* `VulnBankBuggyLock` is vulnerable to cross-function re-entrancy due to a 
  incomplete locking mechanism.
* `VulnBankSecureLock` is not vulnerable due to the locking mechanism. However
  the locking mechanism can result in a false positive.

Furthermore there are two types of attacks implemented against all of these
contracts.

* `MallorySameFunction` implements simple same-function re-entrancy
* `MalloryCrossFunction` implements a cross-function re-entrancy attack

Static analysis tools have a hard time correctly analysing the contracts. Oyente
detects only the simple re-entrancy vulnerability and does not report the
cross-function re-entrancy. Manticore on the other hand detects a re-entrancy
bug in both the BuggyLock and SecureLock version resulting in a false positive.

| Tool \ Testcase | NoLock   | BuggyLock | SecureLock | 
| --------------- | -------- | --------- | ---------- |
| Oyente          | Yes      | No        | No         |
| Manticore       | Yes      | Yes       | Yes        |
| Mythril         | Yes      | Yes       | Yes        |
| Expected        | Yes      | Yes       | No         |

For the dynamic analysis tools we use several combinations of vulnerable
contracts and attack contracts. We verify whether the tool detects an attack
against the same-function and cross-function re-entrancy attack.

| Testcase \ Tool            | ECFChecker | Sereum    | Expected |
| ---------------------------| ---------- | --------- | -------- |
| NoLock + SameFunction      | Yes        | Yes       | Yes      |
| NoLock + CrossFunction     | No         | Yes       | Yes      |
| BuggyLock + SameFunction   | No         | Yes       | No       |
| BuggyLock + CrossFunction  | No         | Yes       | Yes      |
| SecureLock + SameFunction  | No         | Yes       | No       |
| SecureLock + CrossFunction | No         | Yes       | No       |

The reason Sereum reports all contracts is that the locking mechanism itself
does exhibit exactly the same pattern as an re-entrancy attack. So Sereum
reports an re-entrancy attack on the lock variables because Sereum cannot know
the semantics of the lock variables.


## Unconditional Re-Entrancy

**Example:** `./unconditional.sol`

Typically a re-entrancy attack will try to subvert a business logic check of an
application. Every check (`if` `require` `assert` etc.) is implemented as a
conditional jump (`JUMPI`) on the EVM level. While certainly unlikely it is
possible to write a contract which does not perform any check on anything
before sending ether. In this example the functionality transfers all the ether
a user has invested. This example is exploitable only with a re-entrancy
vulnerability. Currently this example is not detected by Sereum since we
assume that this is a rather unlikely case. We plan to detect this kind of
Vulnerabilities in a future versions of Sereum.

| Tool            | Detected | 
| --------------- | --- |
| Oyente          | Yes |
| Manticore       | Yes |
| Mythril         | Yes |
| ECFChecker      | Yes |
| Sereum          | No  |

Another very simple example is the following contract which is deployed on the Ethereum blockchain at [0xb7c5c5aa4d42967efe906e1b66cb8df9cebf04f7](https://etherscan.io/address/0xb7c5c5aa4d42967efe906e1b66cb8df9cebf04f7#code).

## Citing in Academic Work

If you want to refer to these attack patterns in academic work please cite the
following paper:

```bibtex
@inproceedings{sereum-ndss19
  title     = ""Sereum: Protecting Existing Smart Contracts Against Re-Entrancy Attacks""
  booktitle = ""Proceedings of the Network and Distributed System Security Symposium ({NDSS'19})""
  author    = ""Rodler Michael and Li Wenting and Karame Ghassan and Davi Lucas""
  year      =  2019
}
```"
"Abstract:  Sample-induced image-degradation remains an intricate wave-optical problem in
light-sheet microscopy. Here we present biobeam an open-source software
package that enables to simulate operational light-sheet microscopes by
combining data from 10^5 - 10^6 multiplexed and GPU-accelerated
point-spread-function calculations. The wave-optical nature of these
simulations leads to the faithful reproduction of spatially varying
aberrations diffraction artifacts geometric image distortions adaptive
optics and emergent wave optical phenomena and renders image-formation in
light-sheet microscopy computationally tractable.
# Re-Entrancy Attack Patterns

These attack patterns were discovered during evaluation of `Sereum` a runtime
monitoring solution for re-entrancy attacks which utilizes taint tracking and
dynamic write locks to detect and prevent re-entrancy attacks. For more
information please refer to our paper *""Sereum: Protecting Existing Smart
Contracts Against Re-Entrancy Attacks""* ([arxiv preprint](https://arxiv.org/abs/1812.05934)).

For every type of attack pattern this repository contains a small example
implementation of a vulnerable contract and an attack. The source code of the
vulnerable and attacker contracts are contained in the `*.sol` files. We also
provide a `*_setup.js` file for every example which deploys the contracts on a
dev blockchain and exploits the vulnerability in the example contract. The
scripts assume they're run in the geth dev mode blockchain (`geth --dev`).

### Cross-function re-entrancy

**Example:** `./cross-function.sol`

The Token contract in the example is vulnerable to a re-entrancy attack
starting with the `withdrawAll` function. However the attacker cannot
re-enter the `withdrawAll`. Instead the attacker has to re-enter the contract
at the `exchangeAndWithdrawToken` to exploit the bug and drain the vulnerable
contract from ether.

### Delegated re-entrancy

**Example:** `./delegated.sol`

The `Bank` contract utilizes a library called via `delegatecall` for
performing the ether sending. This obfuscates the re-entrancy vulnerability in
the `withdraw` function. Any static analysis tool will not be able to detect
this vulnerability when analyzing only the `Bank` contract and not the
combination of the contract and its libraries.

### Create-based re-entrancy

**Example:** `./create-based.sol`

In this example multiple contracts interact with each other. The `Bank`
contract utilizes the `CREATE` instruction (i.e. `new` in solidity) to create
new subcontracts. Contract creation immediately triggers the execution of the
constructor of the newly created contract. This constructor can perform
external calls to the unknown. This can lead to re-entrancy scenarios where
the attacker re-enters a contract during execution of a sub-contracts
constructor. For static analysis tools to catch these kinds of problems they
must (1) also analyze combination of contracts and (2) consider the `CREATE`
instruction as an external call similar to the `CALL` instruction.


## Tested Tools

The following table lists the tools and versions we tested. If the tool detects
the test-case we mark it with ""Yes"" otherwise ""No"". Mythril and Securify use
a very conservative policy that marks every state update after an external
call. This would prevent all re-entrancy vulnerabilities but also results in a
high number of false positives. For example for create-based re-entrancy
vulnerabilities it is highly likely that the creater of the contract will
want to modify the state (e.g. registering the address of the newly created
contract).

| Tool          | Version     | Simple                 | Cross-Function         | Delegated | Create-based | 
| ------------- | ----------- | ---------------------- | ---------------------- | --------- | ------------ |
| Oyente        | 0.2.7       | Yes                    | No                     | No<sup>1</sup> | No      |
| Mythril       | v0.19.9     | Partial (conservative) | Partial (conservative) | No        | Partial (conservative) |
| Securify      | 2018-08-01  | Partial (conservative) | Partial (conservative) | No        | No           |
| Manticore<sup>2</sup> | 0.2.2 | Yes                  | Yes<sup>3</sup>        | No        | No           |
| ECFChecker    | geth1.8port | Yes                    | Yes<sup>4</sup>        | Yes       | No           |
| Sereum        |             | Yes                    | Yes                    | Yes       | Yes          |


* <sup>1</sup> Oyente detects a re-entrancy in the Library contract. However 
  the library contract itself is arguably not vulnerable to re-entrancy.
* <sup>2</sup> We evaluate the detector enabled with `--detect-reentrancy-advanced`.
  The other detector `--detect-reentrancy` uses a similar policy to Mythril and
  Securify.
* <sup>3</sup> However other tests (e.g. `manual-lock.sol`) show that
  Manticore is sometimes not as accurate and reports re-entrancy attacks even
  though they're not really possible.
* <sup>4</sup> However we crafted a different example for a cross-function
  re-entrancy attack that is not detected by ECFChecker. See the next section 
  for details.


## Testcase: manual lock

The file `manual-lock.sol` contains several versions of the same contract. These
contracts can be used to investigate the quality of re-entrancy detection tools.
This file contains three functionally equivalent contracts:

* `VulnBankNoLock` is vulnerable to simple same function re-entrancy.
* `VulnBankBuggyLock` is vulnerable to cross-function re-entrancy due to a 
  incomplete locking mechanism.
* `VulnBankSecureLock` is not vulnerable due to the locking mechanism. However
  the locking mechanism can result in a false positive.

Furthermore there are two types of attacks implemented against all of these
contracts.

* `MallorySameFunction` implements simple same-function re-entrancy
* `MalloryCrossFunction` implements a cross-function re-entrancy attack

Static analysis tools have a hard time correctly analysing the contracts. Oyente
detects only the simple re-entrancy vulnerability and does not report the
cross-function re-entrancy. Manticore on the other hand detects a re-entrancy
bug in both the BuggyLock and SecureLock version resulting in a false positive.

| Tool \ Testcase | NoLock   | BuggyLock | SecureLock | 
| --------------- | -------- | --------- | ---------- |
| Oyente          | Yes      | No        | No         |
| Manticore       | Yes      | Yes       | Yes        |
| Mythril         | Yes      | Yes       | Yes        |
| Expected        | Yes      | Yes       | No         |

For the dynamic analysis tools we use several combinations of vulnerable
contracts and attack contracts. We verify whether the tool detects an attack
against the same-function and cross-function re-entrancy attack.

| Testcase \ Tool            | ECFChecker | Sereum    | Expected |
| ---------------------------| ---------- | --------- | -------- |
| NoLock + SameFunction      | Yes        | Yes       | Yes      |
| NoLock + CrossFunction     | No         | Yes       | Yes      |
| BuggyLock + SameFunction   | No         | Yes       | No       |
| BuggyLock + CrossFunction  | No         | Yes       | Yes      |
| SecureLock + SameFunction  | No         | Yes       | No       |
| SecureLock + CrossFunction | No         | Yes       | No       |

The reason Sereum reports all contracts is that the locking mechanism itself
does exhibit exactly the same pattern as an re-entrancy attack. So Sereum
reports an re-entrancy attack on the lock variables because Sereum cannot know
the semantics of the lock variables.


## Unconditional Re-Entrancy

**Example:** `./unconditional.sol`

Typically a re-entrancy attack will try to subvert a business logic check of an
application. Every check (`if` `require` `assert` etc.) is implemented as a
conditional jump (`JUMPI`) on the EVM level. While certainly unlikely it is
possible to write a contract which does not perform any check on anything
before sending ether. In this example the functionality transfers all the ether
a user has invested. This example is exploitable only with a re-entrancy
vulnerability. Currently this example is not detected by Sereum since we
assume that this is a rather unlikely case. We plan to detect this kind of
Vulnerabilities in a future versions of Sereum.

| Tool            | Detected | 
| --------------- | --- |
| Oyente          | Yes |
| Manticore       | Yes |
| Mythril         | Yes |
| ECFChecker      | Yes |
| Sereum          | No  |

Another very simple example is the following contract which is deployed on the Ethereum blockchain at [0xb7c5c5aa4d42967efe906e1b66cb8df9cebf04f7](https://etherscan.io/address/0xb7c5c5aa4d42967efe906e1b66cb8df9cebf04f7#code).

## Citing in Academic Work

If you want to refer to these attack patterns in academic work please cite the
following paper:

```bibtex
@inproceedings{sereum-ndss19
  title     = ""Sereum: Protecting Existing Smart Contracts Against Re-Entrancy Attacks""
  booktitle = ""Proceedings of the Network and Distributed System Security Symposium ({NDSS'19})""
  author    = ""Rodler Michael and Li Wenting and Karame Ghassan and Davi Lucas""
  year      =  2019
}
```"
"Abstract:  This paper introduces a deep-learning approach to photographic style transfer
that handles a large variety of image content while faithfully transferring the
reference style. Our approach builds upon the recent work on painterly transfer
that separates style from the content of an image by considering different
layers of a neural network. However as is this approach is not suitable for
photorealistic style transfer. Even when both the input and reference images
are photographs the output still exhibits distortions reminiscent of a
painting. Our contribution is to constrain the transformation from the input to
the output to be locally affine in colorspace and to express this constraint
as a custom fully differentiable energy term. We show that this approach
successfully suppresses distortion and yields satisfying photorealistic style
transfers in a broad variety of scenarios including transfer of the time of
day weather season and artistic edits.
# deep-photo-styletransfer
Code and data for paper ""[Deep Photo Style Transfer](https://arxiv.org/abs/1703.07511)""

## Disclaimer 
**This software is published for academic and non-commercial use only.**

## Setup
This code is based on torch. It has been tested on Ubuntu 14.04 LTS.

Dependencies:
* [Torch](https://github.com/torch/torch7) (with [matio-ffi](https://github.com/soumith/matio-ffi.torch) and [loadcaffe](https://github.com/szagoruyko/loadcaffe))
* [Matlab](https://www.mathworks.com/) or [Octave](https://www.gnu.org/software/octave/)

CUDA backend:
* [CUDA](https://developer.nvidia.com/cuda-downloads)
* [cudnn](https://developer.nvidia.com/cudnn)

Download VGG-19:
```
sh models/download_models.sh
```

Compile ``cuda_utils.cu`` (Adjust ``PREFIX`` and ``NVCC_PREFIX`` in ``makefile`` for your machine):
```
make clean &amp;&amp; make
```

## Usage
### Quick start
To generate all results (in ``examples/``) using the provided scripts simply run
```
run('gen_laplacian/gen_laplacian.m')
```
in Matlab or Octave and then
```
python gen_all.py
```
in Python. The final output will be in ``examples/final_results/``.

### Basic usage
1. Given input and style images with semantic segmentation masks put them in ``examples/`` respectively. They will have the following filename form: ``examples/input/in<id>.png`` ``examples/style/tar<id>.png`` and ``examples/segmentation/in<id>.png`` ``examples/segmentation/tar<id>.png``;
2. Compute the matting Laplacian matrix using ``gen_laplacian/gen_laplacian.m`` in Matlab. The output matrix will have the following filename form: ``gen_laplacian/Input_Laplacian_3x3_1e-7_CSR<id>.mat``; 

**Note: Please make sure that the content image resolution is consistent for Matting Laplacian computation in Matlab and style transfer in Torch otherwise the result won't be correct.**

3. Run the following script to generate segmented intermediate result:
```
th neuralstyle_seg.lua -content_image <input/> -style_image <style></style></id></id></id></id></id>"
"Abstract:  We introduce a framework for the modeling of sequential data capturing
pathways of varying lengths observed in a network. Such data are important
e.g. when studying click streams in information networks travel patterns in
transportation systems information cascades in social networks biological
pathways or time-stamped social interactions. While it is common to apply graph
analytics and network analysis to such data recent works have shown that
temporal correlations can invalidate the results of such methods. This raises a
fundamental question: when is a network abstraction of sequential data
justified? Addressing this open question we propose a framework which combines
Markov chains of multiple higher orders into a multi-layer graphical model
that captures temporal correlations in pathways at multiple length scales
simultaneously. We develop a model selection technique to infer the optimal
number of layers of such a model and show that it outperforms previously used
Markov order detection techniques. An application to eight real-world data sets
on pathways and temporal networks shows that it allows to infer graphical
models which capture both topological and temporal characteristics of such
data. Our work highlights fallacies of network abstractions and provides a
principled answer to the open question when they are justified. Generalizing
network representations to multi-order graphical models it opens perspectives
for new data mining and knowledge discovery algorithms.
# deep-photo-styletransfer
Code and data for paper ""[Deep Photo Style Transfer](https://arxiv.org/abs/1703.07511)""

## Disclaimer 
**This software is published for academic and non-commercial use only.**

## Setup
This code is based on torch. It has been tested on Ubuntu 14.04 LTS.

Dependencies:
* [Torch](https://github.com/torch/torch7) (with [matio-ffi](https://github.com/soumith/matio-ffi.torch) and [loadcaffe](https://github.com/szagoruyko/loadcaffe))
* [Matlab](https://www.mathworks.com/) or [Octave](https://www.gnu.org/software/octave/)

CUDA backend:
* [CUDA](https://developer.nvidia.com/cuda-downloads)
* [cudnn](https://developer.nvidia.com/cudnn)

Download VGG-19:
```
sh models/download_models.sh
```

Compile ``cuda_utils.cu`` (Adjust ``PREFIX`` and ``NVCC_PREFIX`` in ``makefile`` for your machine):
```
make clean &amp;&amp; make
```

## Usage
### Quick start
To generate all results (in ``examples/``) using the provided scripts simply run
```
run('gen_laplacian/gen_laplacian.m')
```
in Matlab or Octave and then
```
python gen_all.py
```
in Python. The final output will be in ``examples/final_results/``.

### Basic usage
1. Given input and style images with semantic segmentation masks put them in ``examples/`` respectively. They will have the following filename form: ``examples/input/in<id>.png`` ``examples/style/tar<id>.png`` and ``examples/segmentation/in<id>.png`` ``examples/segmentation/tar<id>.png``;
2. Compute the matting Laplacian matrix using ``gen_laplacian/gen_laplacian.m`` in Matlab. The output matrix will have the following filename form: ``gen_laplacian/Input_Laplacian_3x3_1e-7_CSR<id>.mat``; 

**Note: Please make sure that the content image resolution is consistent for Matting Laplacian computation in Matlab and style transfer in Torch otherwise the result won't be correct.**

3. Run the following script to generate segmented intermediate result:
```
th neuralstyle_seg.lua -content_image <input/> -style_image <style></style></id></id></id></id></id>"
"Abstract:  Bilinear models provide rich representations compared with linear models.
They have been applied in various visual tasks such as object recognition
segmentation and visual question-answering to get state-of-the-art
performances taking advantage of the expanded representations. However
bilinear representations tend to be high-dimensional limiting the
applicability to computationally complex tasks. We propose low-rank bilinear
pooling using Hadamard product for an efficient attention mechanism of
multimodal learning. We show that our model outperforms compact bilinear
pooling in visual question-answering tasks with the state-of-the-art results on
the VQA dataset having a better parsimonious property.
# Hadamard Product for Low-rank Bilinear Pooling

Multimodal Low-rank Bilinear Attention Networks (MLB) have an efficient attention mechanism by low-rank bilinear pooling for visual question-answering tasks. MLB achieves a new state-of-the-art performance having a better parsimonious property than previous methods.

This current code can get **65.07** on Open-Ended and **68.89** on Multiple-Choice on **test-standard** split for the [VQA dataset](http://visualqa.org). For an ensemble model **66.89** and **70.29** resepectively.

### Dependencies

* [rnn](https://github.com/Element-Research/rnn)

You can install the dependencies:

```bash
luarocks install rnn
```

### Training

Please follow the instruction from [VQA_LSTM_CNN](https://github.com/VT-vision-lab/VQA_LSTM_CNN/blob/master/readme.md) for preprocessing. `--split 2` option allows to use train+val set to train and test-dev or test-standard set to evaluate. Set `--num_ans` to `2000` to reproduce the result.

For question features you need to use this:

* [skip-thoughts](https://github.com/ryankiros/skip-thoughts)
* [DPPnet](https://github.com/HyeonwooNoh/DPPnet) (see 003_skipthoughts_porting)
* `make_lookuptable.lua`

for image features

```
$ th prepro_res.lua -input_json data_train-val_test-dev_2k/data_prepro.json -image_root path_to_image_root -cnn_model path to cnn_model
```

The pretrained ResNet-152 model and related scripts can be found in [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua).

```
$ th train.lua
``` 

With the default parameter this will take around 2.6 days on a sinlge NVIDIA Titan X GPU and will generate the model under `model/`. For the result of the paper use `-seconds` option for `answer sampling` in Section 5. `seconds.json` file can be optained using `prepro_seconds.lua` or from [here](https://drive.google.com/drive/folders/0B-75nmZV6j-JLUJRZlJPVjVBZ2c?usp=sharing) (updated as default).

### Evaluation

```
$ th eval.lua
```

### References

If you use this code as part of any published research we'd really appreciate it if you could cite the following paper:

```
@inproceedings{Kim2017
author = {Kim Jin-Hwa and On Kyoung Woon and Lim Woosang and Kim Jeonghee and Ha Jung-Woo and Zhang Byoung-Tak}
booktitle = {The 5th International Conference on Learning Representations}
title = {{Hadamard Product for Low-rank Bilinear Pooling}}
year = {2017}
}
```

This code uses Torch7 `rnn` package and its `TrimZero` module for question embeddings. Notice that following papers:

```
@article{Leonard2015a
author = {L{\'{e}}onard Nicholas and Waghmare Sagar and Wang Yang and Kim Jin-Hwa}
journal = {arXiv preprint arXiv:1511.07889}
title = {{rnn : Recurrent Library for Torch}}
year = {2015}
}
@inproceedings{Kim2016a
author = {Kim Jin-Hwa and Kim Jeonghee and Ha Jung-Woo and Zhang Byoung-Tak}
booktitle = {Proceedings of KIIS Spring Conference}
isbn = {2093-4025}
number = {1}
pages = {165--166}
title = {{TrimZero: A Torch Recurrent Module for Efficient Natural Language Processing}}
volume = {26}
year = {2016}
}
```

### License

BSD 3-Clause License
  
### Patent (Pending)

METHOD AND SYSTEM FOR PROCESSING DATA USING ELEMENT-WISE MULTIPLICATION AND MULTIMODAL RESIDUAL LEARNING FOR VISUAL QUESTION-ANSWERING"
"Abstract:  CleverHans is a software library that provides standardized reference
implementations of adversarial example construction techniques and adversarial
training. The library may be used to develop more robust machine learning
models and to provide standardized benchmarks of models' performance in the
adversarial setting. Benchmarks constructed without a standardized
implementation of adversarial example construction are not comparable to each
other because a good result may indicate a robust model or it may merely
indicate a weak implementation of the adversarial example construction
procedure.
This technical report is structured as follows. Section 1 provides an
overview of adversarial examples in machine learning and of the CleverHans
software. Section 2 presents the core functionalities of the library: namely
the attacks based on adversarial examples and defenses to improve the
robustness of machine learning models to these attacks. Section 3 describes how
to report benchmark results using the library. Section 4 describes the
versioning system.
# CleverHans (latest release: v3.0.1)

<img alt=""cleverhans logo"" src=""https://github.com/tensorflow/cleverhans/blob/master/assets/logo.png?raw=true""/>

[![Build Status](https://travis-ci.org/tensorflow/cleverhans.svg?branch=master)](https://travis-ci.org/tensorflow/cleverhans)
[![Documentation Status](https://readthedocs.org/projects/cleverhans/badge/?version=latest)](https://cleverhans.readthedocs.io/en/latest/?badge=latest)

This repository contains the source code for CleverHans a Python library to
benchmark machine learning systems' vulnerability to
[adversarial examples](http://karpathy.github.io/2015/03/30/breaking-convnets/).
You can learn more about such vulnerabilities on the accompanying [blog](http://cleverhans.io).

The CleverHans library is under continual development always welcoming
[contributions](https://github.com/tensorflow/cleverhans#contributing)
of the latest attacks and defenses.
In particular we always welcome help towards resolving the [issues](https://github.com/tensorflow/cleverhans/issues)
currently open.

## Major updates coming to CleverHans

CleverHans will soon support 3 frameworks: JAX PyTorch and TF2.  The package
itself will focus on its initial principle: reference implementation of attacks
against machine learning models to help with benchmarking models against
adversarial examples. This repository will also contain two folders:
`tutorials/` for scripts demonstrating the features of CleverHans and
`defenses/` for scripts that contain authoritative implementations of defenses
in one of the 3 supported frameworks. The structure of the future repository
will look like this:

```
cleverhans/
  jax/
    attacks/
      ...
  tf2/
    attacks/
      ...
  torch/
    attacks/
      ...
defenses/
  jax/
    ...
  tf2/
    ...
  torch/
    ...
tutorials/
  jax/
    ...
  tf2/
    ...
  torch/
    ...
```

In the meanwhile all of these folders can be found in the correspond `future/`
subdirectory (e.g. `cleverhans/future/jax/attacks` or `defenses/future/jax/`).

A public milestone has been created to track the changes that are to be
implemented before the library version is incremented to v4. 

## Setting up CleverHans

### Dependencies

This library uses [TensorFlow](https://www.tensorflow.org/) to accelerate graph
computations performed by many machine learning models.
Therefore installing TensorFlow is a pre-requisite.

You can find instructions
[here](https://www.tensorflow.org/install/).
For better performance it is also recommended to install TensorFlow
with GPU support (detailed instructions on how to do this are available
in the TensorFlow installation documentation).

Installing TensorFlow will
take care of all other dependencies like `numpy` and `scipy`.

### Installation

Once dependencies have been taken care of you can install CleverHans using
`pip` or by cloning this Github repository.

#### `pip` installation

If you are installing CleverHans using `pip` run the following command
after installing TensorFlow:

```
pip install cleverhans
```

This will install the last version uploaded to
[Pypi](https://pypi.org/project/cleverhans).
If you'd instead like to install the bleeding edge version use:

```
pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans
```

#### Installation for development

If you want to make an editable installation of CleverHans so that you can
develop the library and contribute changes back first fork the repository
on GitHub and then clone your fork into a directory of your choice:

```
git clone https://github.com/tensorflow/cleverhans
```

You can then install the local package in ""editable"" mode in order to add it to
your `PYTHONPATH`:

```
cd cleverhans
pip install -e .
```

### Currently supported setups

Although CleverHans is likely to work on many other machine configurations we
currently [test it](https://travis-ci.org/tensorflow/cleverhans) it with Python
3.5 and TensorFlow {1.8 1.12} on Ubuntu 14.04.5 LTS (Trusty Tahr).
Support for Python 2.7 is deprecated.
CleverHans 3.0.1 supports Python 2.7 and the master branch is likely to
continue to work in Python 2.7 for some time but we no longer run the tests
in Python 2.7 and we do not plan to fix bugs affecting only Python 2.7 after
2019-07-04.
Support for TensorFlow prior to 1.12 is deprecated.
Backwards compatibility wrappers for these versions may be removed after 2019-07-07
and we will not fix bugs for those versions after that date.
Support for TensorFlow 1.7 and earlier is already deprecated: we do not fix
bugs for those versions and any remaining wrapper code for those versions
may be removed without further notice.

## Getting support

If you have a request for support please ask a question
on [StackOverflow](https://stackoverflow.com/questions/tagged/cleverhans)
rather than opening an issue in the GitHub tracker. The GitHub
issue tracker should *only* be used to report bugs or make feature requests.

## Contributing

Contributions are welcomed! To speed the code review process we ask that:
* New efforts and features be coordinated
on the mailing list for CleverHans development: [cleverhans-dev@googlegroups.com](https://groups.google.com/forum/#!forum/cleverhans-dev).
* When making code contributions to CleverHans you follow the
`PEP8 with two spaces` coding style (the same as the one used
by TensorFlow) in your pull requests.
In most cases this can be done by running `autopep8 -i --indent-size 2 <file>`
on the files you have edited.
You can check your code by running `nosestests cleverhans/devtools/tests/test_format.py` or check an individual file by running `pylint <file>` from inside the cleverhans repository root directory.
* When making your first pull request you [sign the Google CLA](https://cla.developers.google.com/clas)
* We do not accept pull requests that add git submodules because of [the
  problems that arise when maintaining git
  submodules](https://medium.com/@porteneuve/mastering-git-submodules-34c65e940407)

Bug fixes can be initiated through Github pull requests.

## Scripts: `scripts` directory

The `scripts` directory contains command line utilities.
In many cases you can use these to run CleverHans functionality on your
saved models without needing to write any of your own Python code.

You may want to set your `.bashrc` / `.bash_profile` file to add the
CleverHans `scripts` directory to your `PATH` environment variable
so that these scripts will be conveniently executable from any directory.

## Tutorials: `cleverhans_tutorials` directory

To help you get started with the functionalities provided by this library the
`cleverhans_tutorials/` folder comes with the following tutorials:
* **MNIST with FGSM** ([code](cleverhans_tutorials/mnist_tutorial_tf.py)): this
tutorial covers how to train a MNIST model using TensorFlow
craft adversarial examples using the [fast gradient sign method](https://arxiv.org/abs/1412.6572)
and make the model more robust to adversarial examples using adversarial training.
* **MNIST with FGSM using Keras** ([code](cleverhans_tutorials/mnist_tutorial_keras_tf.py)): this
tutorial covers how to define a MNIST model with Keras and train it using TensorFlow
craft adversarial examples using the [fast gradient sign method](https://arxiv.org/abs/1412.6572)
and make the model more robust to adversarial
examples using adversarial training.
* **MNIST with JSMA** ([code](cleverhans_tutorials/mnist_tutorial_jsma.py)): this second
tutorial covers how to define a MNIST model with Keras and train it using TensorFlow and
craft adversarial examples using the [Jacobian-based saliency map approach](https://arxiv.org/abs/1511.07528).
* **MNIST using a black-box attack** ([code](cleverhans_tutorials/mnist_blackbox.py)):
this tutorial implements the black-box
attack described in this [paper](https://arxiv.org/abs/1602.02697).
The adversary train a substitute model: a copy that imitates the black-box
model by observing the labels that the black-box model assigns to inputs chosen
carefully by the adversary. The adversary then uses the substitute
model’s gradients to find adversarial examples that are misclassified by the
black-box model as well.

NOTE: the tutorials are maintained carefully in the sense that we use
continuous integration to make sure they continue working. They are not
considered part of the API and they can change at any time without warning.
You should not write 3rd party code that imports the tutorials and expect
that the interface will not break. Only the main library is subject to
our six month interface deprecation warning rule.

NOTE: please write to cleverhans-dev@googlegroups.com before writing a new
tutorial. Because each new tutorial involves a large amount of duplicated
code relative to the existing tutorials and because every line of code
requires ongoing testing and maintenance indefinitely we generally prefer
not to add new tutorials. Each tutorial should showcase an extremely different
way of using the library. Just calling a different attack model or dataset
is not enough to justify maintaining a parallel tutorial.

## Examples : `examples` directory

The `examples/` folder contains additional scripts to showcase different uses
of the CleverHans library or get you started competing in different adversarial
example contests. We do not offer nearly as much ongoing maintenance or support
for this directory as the rest of the library and if code in here gets broken
we may just delete it without warning.

## List of attacks

You can find a full list attacks along with their function signatures at [cleverhans.readthedocs.io](http://cleverhans.readthedocs.io/)

## Reporting benchmarks

When reporting benchmarks please:
* Use a versioned release of CleverHans. You can find a list of released versions [here](https://github.com/tensorflow/cleverhans/releases).
* Either use the latest version or if comparing to an earlier publication use the same version as the earlier publication.
* Report which attack method was used.
* Report any configuration variables used to determine the behavior of the attack.

For example you might report ""We benchmarked the robustness of our method to
adversarial attack using v3.0.1 of CleverHans. On a test set modified by the
`FastGradientMethod` with a max-norm `eps` of 0.3 we obtained a test set accuracy of 71.3%.""

## Citing this work

If you use CleverHans for academic research you are highly encouraged
(though not required) to cite the following [paper](https://arxiv.org/abs/1610.00768):

```
@article{papernot2018cleverhans
  title={Technical Report on the CleverHans v2.1.0 Adversarial Examples Library}
  author={Nicolas Papernot and Fartash Faghri and Nicholas Carlini and
  Ian Goodfellow and Reuben Feinman and Alexey Kurakin and Cihang Xie and
  Yash Sharma and Tom Brown and Aurko Roy and Alexander Matyasko and
  Vahid Behzadan and Karen Hambardzumyan and Zhishuai Zhang and
  Yi-Lin Juang and Zhi Li and Ryan Sheatsley and Abhibhav Garg and
  Jonathan Uesato and Willi Gierke and Yinpeng Dong and David Berthelot and
  Paul Hendricks and Jonas Rauber and Rujun Long}
  journal={arXiv preprint arXiv:1610.00768}
  year={2018}
}
```

## About the name

The name CleverHans is a reference to a presentation by Bob Sturm titled
“Clever Hans Clever Algorithms: Are Your Machine Learnings Learning What You
Think?"" and the corresponding publication [""A Simple Method to Determine if a
Music Information Retrieval System is a
'Horse'.""](http://ieeexplore.ieee.org/document/6847693/) Clever Hans was a
horse that appeared to have learned to answer arithmetic questions but had in
fact only learned to read social cues that enabled him to give the correct
answer. In controlled settings where he could not see people's faces or receive
other feedback he was unable to answer the same questions. The story of Clever
Hans is a metaphor for machine learning systems that may achieve very high
accuracy on a test set drawn from the same distribution as the training data
but that do not actually understand the underlying task and perform poorly on
other inputs.

## Authors

This library is managed and maintained by Ian Goodfellow (Google Brain) and
Nicolas Papernot (Google Brain).

The following authors contributed 100 lines or more (ordered according to the GitHub contributors page):
* Ian Goodfellow (Google Brain)
* Nicolas Papernot (Google Brain)
* Nicholas Carlini (Google Brain)
* Fartash Faghri (University of Toronto)
* Tzu-Wei Sung (National Taiwan University)
* Alexey Kurakin (Google Brain)
* Reuben Feinman (New York University)
* Phani Krishna (Video Analytics Lab)
* David Berthelot (Google Brain)
* Tom Brown (Google Brain)
* Cihang Xie (Johns Hopkins)
* Yash Sharma (The Cooper Union)
* Aashish Kumar (HARMAN X)
* Aurko Roy (Google Brain)
* Alexander Matyasko (Nanyang Technological University)
* Anshuman Suri (Microsoft)
* Yen-Chen Lin (MIT)
* Vahid Behzadan (Kansas State)
* Jonathan Uesato (DeepMind)
* Haojie Yuan (University of Science &amp; Technology of China)
* Zhishuai Zhang (Johns Hopkins)
* Karen Hambardzumyan (YerevaNN)
* Jianbo Chen (UC Berkeley)
* Catherine Olsson (Google Brain)
* Aidan Gomez (University of Oxford)
* Zhi Li (University of Toronto)
* Yi-Lin Juang (NTUEE)
* Pratyush Sahay (formerly HARMAN X)
* Abhibhav Garg (IIT Delhi)
* Aditi Raghunathan (Stanford University)
* Yang Song (Stanford University)
* Riccardo Volpi (Italian Institute of Technology)
* Angus Galloway (University of Guelph)
* Yinpeng Dong (Tsinghua University)
* Willi Gierke (Hasso Plattner Institute)
* Bruno López
* Jonas Rauber (IMPRS)
* Paul Hendricks (NVIDIA)
* Ryan Sheatsley (Pennsylvania State University)
* Rujun Long (0101.AI)
* Bogdan Kulynych (EPFL)
* Erfan Noury (UMBC)
* Robert Wagner (Case Western Reserve University)

## Copyright

Copyright 2019 - Google Inc. OpenAI and Pennsylvania State University.
</file></file>"
"Abstract:  As a new way of training generative models Generative Adversarial Nets (GAN)
that uses a discriminative model to guide the training of the generative model
has enjoyed considerable success in generating real-valued data. However it
has limitations when the goal is for generating sequences of discrete tokens. A
major reason lies in that the discrete outputs from the generative model make
it difficult to pass the gradient update from the discriminative model to the
generative model. Also the discriminative model can only assess a complete
sequence while for a partially generated sequence it is non-trivial to
balance its current score and the future one once the entire sequence has been
generated. In this paper we propose a sequence generation framework called
SeqGAN to solve the problems. Modeling the data generator as a stochastic
policy in reinforcement learning (RL) SeqGAN bypasses the generator
differentiation problem by directly performing gradient policy update. The RL
reward signal comes from the GAN discriminator judged on a complete sequence
and is passed back to the intermediate state-action steps using Monte Carlo
search. Extensive experiments on synthetic data and real-world tasks
demonstrate significant improvements over strong baselines.
# SeqGAN

## Requirements: 
* **Tensorflow r1.0.1**
* Python 2.7
* CUDA 7.5+ (For GPU)

## Introduction
Apply Generative Adversarial Nets to generating sequences of discrete tokens.

![](https://github.com/LantaoYu/SeqGAN/blob/master/figures/seqgan.png)

The illustration of SeqGAN. Left: D is trained over the real data and the generated data by G. Right: G is trained by policy gradient where the final reward signal is provided by D and is passed back to the intermediate action value via Monte Carlo search.  

The research paper [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](http://arxiv.org/abs/1609.05473) has been accepted at the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17).

We provide example codes to repeat the synthetic data experiments with oracle evaluation mechanisms.
To run the experiment with default parameters:
```
$ python sequence_gan.py
```
You can change the all the parameters in `sequence_gan.py`.

The experiment has two stages. In the first stage use the positive data provided by the oracle model and Maximum Likelihood Estimation to perform supervise learning. In the second stage use adversarial training to improve the generator.

After running the experiments you could get the negative log-likelihodd performance saved in `save/experiment-log.txt` like:
```
pre-training...
epoch:	0	nll:	10.1716
epoch:	5	nll:	9.42939
epoch:	10	nll:	9.2388
epoch:	15	nll:	9.11899
epoch:	20	nll:	9.13099
epoch:	25	nll:	9.14474
epoch:	30	nll:	9.12539
epoch:	35	nll:	9.13982
epoch:	40	nll:	9.135
epoch:	45	nll:	9.13081
epoch:	50	nll:	9.10678
epoch:	55	nll:	9.10694
epoch:	60	nll:	9.10349
epoch:	65	nll:	9.10403
epoch:	70	nll:	9.07613
epoch:	75	nll:	9.091
epoch:	80	nll:	9.08909
epoch:	85	nll:	9.0807
epoch:	90	nll:	9.08434
epoch:	95	nll:	9.08936
epoch:	100	nll:	9.07443
epoch:	105	nll:	9.08305
epoch:	110	nll:	9.06973
epoch:	115	nll:	9.07058
adversarial training...
epoch:	0	nll:	9.08457
epoch:	5	nll:	9.04511
epoch:	10	nll:	9.03079
epoch:	15	nll:	8.99239
epoch:	20	nll:	8.96401
epoch:	25	nll:	8.93864
epoch:	30	nll:	8.91642
epoch:	35	nll:	8.87761
epoch:	40	nll:	8.88582
epoch:	45	nll:	8.8592
epoch:	50	nll:	8.83388
epoch:	55	nll:	8.81342
epoch:	60	nll:	8.80247
epoch:	65	nll:	8.77778
epoch:	70	nll:	8.7567
epoch:	75	nll:	8.73002
epoch:	80	nll:	8.72488
epoch:	85	nll:	8.72233
epoch:	90	nll:	8.71473
epoch:	95	nll:	8.71163
epoch:	100	nll:	8.70113
epoch:	105	nll:	8.69879
epoch:	110	nll:	8.69208
epoch:	115	nll:	8.69291
epoch:	120	nll:	8.68371
epoch:	125	nll:	8.689
epoch:	130	nll:	8.68989
epoch:	135	nll:	8.68269
epoch:	140	nll:	8.68647
epoch:	145	nll:	8.68066
epoch:	150	nll:	8.6832
```

Note: this code is based on the [previous work by ofirnachum](https://github.com/ofirnachum/sequence_gan). Many thanks to [ofirnachum](https://github.com/ofirnachum)."
"Abstract:  Software popularity is a valuable information to modern open source
developers who constantly want to know if their systems are attracting new
users if new releases are gaining acceptance or if they are meeting user's
expectations. In this paper we describe a study on the popularity of software
systems hosted at GitHub which is the world's largest collection of open
source software. GitHub provides an explicit way for users to manifest their
satisfaction with a hosted repository: the stargazers button. In our study we
reveal the main factors that impact the number of stars of GitHub projects
including programming language and application domain. We also study the impact
of new features on project popularity. Finally we identify four main patterns
of popularity growth which are derived after clustering the time series
representing the number of stars of 2279 popular GitHub repositories. We hope
our results provide valuable insights to developers and maintainers which can
help them on building and evolving systems in a competitive software market.
# SeqGAN

## Requirements: 
* **Tensorflow r1.0.1**
* Python 2.7
* CUDA 7.5+ (For GPU)

## Introduction
Apply Generative Adversarial Nets to generating sequences of discrete tokens.

![](https://github.com/LantaoYu/SeqGAN/blob/master/figures/seqgan.png)

The illustration of SeqGAN. Left: D is trained over the real data and the generated data by G. Right: G is trained by policy gradient where the final reward signal is provided by D and is passed back to the intermediate action value via Monte Carlo search.  

The research paper [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](http://arxiv.org/abs/1609.05473) has been accepted at the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17).

We provide example codes to repeat the synthetic data experiments with oracle evaluation mechanisms.
To run the experiment with default parameters:
```
$ python sequence_gan.py
```
You can change the all the parameters in `sequence_gan.py`.

The experiment has two stages. In the first stage use the positive data provided by the oracle model and Maximum Likelihood Estimation to perform supervise learning. In the second stage use adversarial training to improve the generator.

After running the experiments you could get the negative log-likelihodd performance saved in `save/experiment-log.txt` like:
```
pre-training...
epoch:	0	nll:	10.1716
epoch:	5	nll:	9.42939
epoch:	10	nll:	9.2388
epoch:	15	nll:	9.11899
epoch:	20	nll:	9.13099
epoch:	25	nll:	9.14474
epoch:	30	nll:	9.12539
epoch:	35	nll:	9.13982
epoch:	40	nll:	9.135
epoch:	45	nll:	9.13081
epoch:	50	nll:	9.10678
epoch:	55	nll:	9.10694
epoch:	60	nll:	9.10349
epoch:	65	nll:	9.10403
epoch:	70	nll:	9.07613
epoch:	75	nll:	9.091
epoch:	80	nll:	9.08909
epoch:	85	nll:	9.0807
epoch:	90	nll:	9.08434
epoch:	95	nll:	9.08936
epoch:	100	nll:	9.07443
epoch:	105	nll:	9.08305
epoch:	110	nll:	9.06973
epoch:	115	nll:	9.07058
adversarial training...
epoch:	0	nll:	9.08457
epoch:	5	nll:	9.04511
epoch:	10	nll:	9.03079
epoch:	15	nll:	8.99239
epoch:	20	nll:	8.96401
epoch:	25	nll:	8.93864
epoch:	30	nll:	8.91642
epoch:	35	nll:	8.87761
epoch:	40	nll:	8.88582
epoch:	45	nll:	8.8592
epoch:	50	nll:	8.83388
epoch:	55	nll:	8.81342
epoch:	60	nll:	8.80247
epoch:	65	nll:	8.77778
epoch:	70	nll:	8.7567
epoch:	75	nll:	8.73002
epoch:	80	nll:	8.72488
epoch:	85	nll:	8.72233
epoch:	90	nll:	8.71473
epoch:	95	nll:	8.71163
epoch:	100	nll:	8.70113
epoch:	105	nll:	8.69879
epoch:	110	nll:	8.69208
epoch:	115	nll:	8.69291
epoch:	120	nll:	8.68371
epoch:	125	nll:	8.689
epoch:	130	nll:	8.68989
epoch:	135	nll:	8.68269
epoch:	140	nll:	8.68647
epoch:	145	nll:	8.68066
epoch:	150	nll:	8.6832
```

Note: this code is based on the [previous work by ofirnachum](https://github.com/ofirnachum/sequence_gan). Many thanks to [ofirnachum](https://github.com/ofirnachum)."
"Abstract:  An experimental approach to studying the properties of word embeddings is
proposed. Controlled experiments achieved through modifications of the
training corpus permit the demonstration of direct relations between word
properties and word vector direction and length. The approach is demonstrated
using the word2vec CBOW model with experiments that independently vary word
frequency and word co-occurrence noise. The experiments reveal that word vector
length depends more or less linearly on both word frequency and the level of
noise in the co-occurrence distribution of the word. The coefficients of
linearity depend upon the word. The special point in feature space defined by
the (artificial) word with pure noise in its co-occurrence distribution is
found to be small but non-zero.
# word2vec-norm-experiments"
"Abstract:  State-of-the-art object detection networks depend on region proposal
algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN
have reduced the running time of these detection networks exposing region
proposal computation as a bottleneck. In this work we introduce a Region
Proposal Network (RPN) that shares full-image convolutional features with the
detection network thus enabling nearly cost-free region proposals. An RPN is a
fully convolutional network that simultaneously predicts object bounds and
objectness scores at each position. The RPN is trained end-to-end to generate
high-quality region proposals which are used by Fast R-CNN for detection. We
further merge RPN and Fast R-CNN into a single network by sharing their
convolutional features---using the recently popular terminology of neural
networks with 'attention' mechanisms the RPN component tells the unified
network where to look. For the very deep VGG-16 model our detection system has
a frame rate of 5fps (including all steps) on a GPU while achieving
state-of-the-art object detection accuracy on PASCAL VOC 2007 2012 and MS
COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015
competitions Faster R-CNN and RPN are the foundations of the 1st-place winning
entries in several tracks. Code has been made publicly available.
## This repo has been deprecated. Please see [Detectron](https://github.com/facebookresearch/Detectron) which includes an implementation of [Mask R-CNN](https://arxiv.org/abs/1703.06870).

# *Faster* R-CNN: Towards Real-Time Object Detection with Region Proposal Networks

By Shaoqing Ren Kaiming He Ross Girshick Jian Sun at Microsoft Research

### Introduction

**Faster** R-CNN is an object detection framework based on deep convolutional networks which includes a Region Proposal Network (RPN) and an Object Detection Network. Both networks are trained for sharing convolutional layers for fast testing. 

Faster R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1506.01497).

This repo contains a MATLAB re-implementation of Fast R-CNN. Details about Fast R-CNN are in: [rbgirshick/fast-rcnn](https://github.com/rbgirshick/fast-rcnn).

This code has been tested on Windows 7/8 64-bit Windows Server 2012 R2 and Linux and on MATLAB 2014a.

Python version is available at [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn).

### License

Faster R-CNN is released under the MIT License (refer to the LICENSE file for details).

### Citing Faster R-CNN

If you find Faster R-CNN useful in your research please consider citing:

    @article{ren15fasterrcnn
        Author = {Shaoqing Ren Kaiming He Ross Girshick Jian Sun}
        Title = {{Faster R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks}
        Journal = {arXiv preprint arXiv:1506.01497}
        Year = {2015}
    }

### Main Results
|                           | training data                          | test data            | mAP   | time/img |
| ------------------------- |:--------------------------------------:|:--------------------:|:-----:|:-----: |
| Faster RCNN VGG-16       | VOC 2007 trainval                      | VOC 2007 test        | 69.9% | 198ms |
| Faster RCNN VGG-16       | VOC 2007 trainval + 2012 trainval      | VOC 2007 test        | 73.2% | 198ms |
| Faster RCNN VGG-16       | VOC 2012 trainval                      | VOC 2012 test        | 67.0% | 198ms |
| Faster RCNN VGG-16       | VOC 2007 trainval&amp;test + 2012 trainval | VOC 2012 test        | 70.4% | 198ms |

**Note**: The mAP results are subject to random variations. We have run 5 times independently for ZF net and the mAPs are 59.9 (as in the paper) 60.4 59.5 60.1 and 59.5 with a mean of 59.88 and std 0.39.


### Contents
0. [Requirements: software](#requirements-software)
0. [Requirements: hardware](#requirements-hardware)
0. [Preparation for Testing](#preparation-for-testing)
0. [Testing Demo](#testing-demo)
0. [Preparation for Training](#preparation-for-training)
0. [Training](#training)
0. [Resources](#resources)


### Requirements: software

0. `Caffe` build for Faster R-CNN (included in this repository see `external/caffe`)
    - If you are using Windows you may download a compiled mex file by running `fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m`
    - If you are using Linux or you want to compile for Windows please follow the [instructions](https://github.com/ShaoqingRen/caffe/tree/faster-R-CNN) on our Caffe branch.
0.	MATLAB
 
    
### Requirements: hardware

GPU: Titan Titan Black Titan X K20 K40 K80.

0. Region Proposal Network (RPN)
    - 2GB GPU memory for ZF net
    - 5GB GPU memory for VGG-16 net
0. Object Detection Network (Fast R-CNN)
    - 3GB GPU memory for ZF net
    - 8GB GPU memory for VGG-16 net


### Preparation for Testing:
0.	Run `fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m` to download a compiled Caffe mex (for Windows only).
0.	Run `faster_rcnn_build.m`
0.	Run `startup.m`


### Testing Demo:
0.	Run `fetch_data/fetch_faster_rcnn_final_model.m` to download our trained models.
0.	Run `experiments/script_faster_rcnn_demo.m` to test a single demo image.
    - You will see the timing information as below. We get the following running time on K40 @ 875 MHz and Intel Xeon CPU E5-2650 v2 @ 2.60GHz for the demo images with VGG-16:
	```Shell
	001763.jpg (500x375): time 0.201s (resize+conv+proposal: 0.150s nms+regionwise: 0.052s)
	004545.jpg (500x375): time 0.201s (resize+conv+proposal: 0.151s nms+regionwise: 0.050s)
	000542.jpg (500x375): time 0.192s (resize+conv+proposal: 0.151s nms+regionwise: 0.041s)
	000456.jpg (500x375): time 0.202s (resize+conv+proposal: 0.152s nms+regionwise: 0.050s)
	001150.jpg (500x375): time 0.194s (resize+conv+proposal: 0.151s nms+regionwise: 0.043s)
	mean time: 0.198s
	```
	and with ZF net:
	```Shell
	001763.jpg (500x375): time 0.061s (resize+conv+proposal: 0.032s nms+regionwise: 0.029s)
	004545.jpg (500x375): time 0.063s (resize+conv+proposal: 0.034s nms+regionwise: 0.029s)
	000542.jpg (500x375): time 0.052s (resize+conv+proposal: 0.034s nms+regionwise: 0.018s)
	000456.jpg (500x375): time 0.062s (resize+conv+proposal: 0.034s nms+regionwise: 0.028s)
	001150.jpg (500x375): time 0.058s (resize+conv+proposal: 0.034s nms+regionwise: 0.023s)
	mean time: 0.059s
	```
    - The visual results might be different from those in the paper due to numerical variations.	
    - Running time on other GPUs

    |       GPU / mean time        |        VGG-16        |        ZF     |     
    | :--------------------------: |:--------------------:|:------------: |
    |  K40                         |        198ms         |       59ms    |
    |  Titan Black                 |        174ms         |       56ms    |
    | Titan X                      |        151ms         |       59ms    |

### Preparation for Training:
0.	Run `fetch_data/fetch_model_ZF.m` to download an ImageNet-pre-trained ZF net.
0.	Run `fetch_data/fetch_model_VGG16.m` to download an ImageNet-pre-trained VGG-16 net.
0.	Download VOC 2007 and 2012 data to ./datasets


### Training:
0. Run `experiments/script_faster_rcnn_VOC2007_ZF.m` to train a model with ZF net. It runs four steps as follows:
    - Train RPN with conv layers tuned; compute RPN results on the train/test sets.
    - Train Fast R-CNN with conv layers tuned using step-1 RPN proposals; evaluate detection mAP.
    - Train RPN with conv layers fixed; compute RPN results on the train/test sets. 
    - Train Fast R-CNN with conv layers fixed using step-3 RPN proposals; evaluate detection mAP.
    - **Note**: the entire training time is ~12 hours on K40.
0. Run `experiments/script_faster_rcnn_VOC2007_VGG16.m` to train a model with VGG net.
    - **Note**: the entire training time is ~2 days on K40.
0. Check other scripts in `./experiments` for more settings.

### Resources

**Note**: This documentation may contain links to third party websites which are provided for your convenience only. Such third party websites are not under Microsoft’s control. Microsoft does not endorse or make any representation guarantee or assurance regarding any third party website content service or product. Third party websites may be subject to the third party’s terms conditions and privacy statements.

0. Experiment logs: [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!110&amp;authkey=!ACpgYZR2MmfklwI&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/wu841r7zmebjp6r/faster_rcnn_logs.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1ntJ3dLv)
0. Regions proposals of our trained RPN:
    - ZF net trained on VOC 07 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!115&amp;authkey=!AJJMrFJHKLXIg5c&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1pKGBDyz)
    - ZF net trained on VOC 07/12 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!117&amp;authkey=!AJiy5F6Cum1iosI&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1jGAgkZW)
    - VGG net trained on VOC 07 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!116&amp;authkey=!AH4Zi_KAaun7MhQ&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1qWHv4JU)
    - VGG net trained on VOC 07/12 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!118&amp;authkey=!AB_lKk3dbGyr1-I&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1c0fQpqg)
    - **Note**: the proposals are in the format of [left top right bottom confidence]

If the automatic ""fetch_data"" fails you may manually download resouces from:

0. Pre-complied caffe mex:
    - Windows-based mex complied with VS2013 and Cuda6.5: [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!111&amp;authkey=!AFVWFGTbViiX5tg&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/m6sg347tiaqpcwy/caffe_mex.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1i3m0i0H)
0. ImageNet-pretrained networks:
    - Zeiler &amp; Fergus (ZF) net [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!113&amp;authkey=!AIzdm0sD_SmhUQ4&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/sw58b2froihzwyf/model_ZF.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1o6zipPS)
    - VGG-16 net [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!114&amp;authkey=!AE8uV9B07dREbhM&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/z5rrji25uskha73/model_VGG16.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1mgzSnI4)
0. Final RPN+FastRCNN models: [OneDrive](https://onedrive.live.com/download?resid=D7AF52BADBA8A4BC!114&amp;authkey=!AERHoxZ-iAx_j34&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/jswrnkaln47clg2/faster_rcnn_final_model.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1hsFKmeK)"
"Abstract:  We report the discovery of 29 promising (and 59 total) new lens candidates
from the CFHT Legacy Survey (CFHTLS) based on about 11 million classifications
performed by citizen scientists as part of the first Space Warps lens search.
The goal of the blind lens search was to identify lens candidates missed by
robots (the RingFinder on galaxy scales and ArcFinder on group/cluster scales)
which had been previously used to mine the CFHTLS for lenses. We compare some
properties of the samples detected by these algorithms to the Space Warps
sample and find them to be broadly similar. The image separation distribution
calculated from the Space Warps sample shows that previous constraints on the
average density profile of lens galaxies are robust. SpaceWarps recovers about
65% of known lenses while the new candidates show a richer variety compared to
those found by the two robots. This detection rate could be increased to 80% by
only using classifications performed by expert volunteers (albeit at the cost
of a lower purity) indicating that the training and performance calibration of
the citizen scientists is very important for the success of Space Warps. In
this work we present the SIMCT pipeline used for generating in situ a sample
of realistic simulated lensed images. This training sample along with the
false positives identified during the search has a legacy value for testing
future lens finding algorithms. We make the pipeline and the training set
publicly available.
## This repo has been deprecated. Please see [Detectron](https://github.com/facebookresearch/Detectron) which includes an implementation of [Mask R-CNN](https://arxiv.org/abs/1703.06870).

# *Faster* R-CNN: Towards Real-Time Object Detection with Region Proposal Networks

By Shaoqing Ren Kaiming He Ross Girshick Jian Sun at Microsoft Research

### Introduction

**Faster** R-CNN is an object detection framework based on deep convolutional networks which includes a Region Proposal Network (RPN) and an Object Detection Network. Both networks are trained for sharing convolutional layers for fast testing. 

Faster R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1506.01497).

This repo contains a MATLAB re-implementation of Fast R-CNN. Details about Fast R-CNN are in: [rbgirshick/fast-rcnn](https://github.com/rbgirshick/fast-rcnn).

This code has been tested on Windows 7/8 64-bit Windows Server 2012 R2 and Linux and on MATLAB 2014a.

Python version is available at [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn).

### License

Faster R-CNN is released under the MIT License (refer to the LICENSE file for details).

### Citing Faster R-CNN

If you find Faster R-CNN useful in your research please consider citing:

    @article{ren15fasterrcnn
        Author = {Shaoqing Ren Kaiming He Ross Girshick Jian Sun}
        Title = {{Faster R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks}
        Journal = {arXiv preprint arXiv:1506.01497}
        Year = {2015}
    }

### Main Results
|                           | training data                          | test data            | mAP   | time/img |
| ------------------------- |:--------------------------------------:|:--------------------:|:-----:|:-----: |
| Faster RCNN VGG-16       | VOC 2007 trainval                      | VOC 2007 test        | 69.9% | 198ms |
| Faster RCNN VGG-16       | VOC 2007 trainval + 2012 trainval      | VOC 2007 test        | 73.2% | 198ms |
| Faster RCNN VGG-16       | VOC 2012 trainval                      | VOC 2012 test        | 67.0% | 198ms |
| Faster RCNN VGG-16       | VOC 2007 trainval&amp;test + 2012 trainval | VOC 2012 test        | 70.4% | 198ms |

**Note**: The mAP results are subject to random variations. We have run 5 times independently for ZF net and the mAPs are 59.9 (as in the paper) 60.4 59.5 60.1 and 59.5 with a mean of 59.88 and std 0.39.


### Contents
0. [Requirements: software](#requirements-software)
0. [Requirements: hardware](#requirements-hardware)
0. [Preparation for Testing](#preparation-for-testing)
0. [Testing Demo](#testing-demo)
0. [Preparation for Training](#preparation-for-training)
0. [Training](#training)
0. [Resources](#resources)


### Requirements: software

0. `Caffe` build for Faster R-CNN (included in this repository see `external/caffe`)
    - If you are using Windows you may download a compiled mex file by running `fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m`
    - If you are using Linux or you want to compile for Windows please follow the [instructions](https://github.com/ShaoqingRen/caffe/tree/faster-R-CNN) on our Caffe branch.
0.	MATLAB
 
    
### Requirements: hardware

GPU: Titan Titan Black Titan X K20 K40 K80.

0. Region Proposal Network (RPN)
    - 2GB GPU memory for ZF net
    - 5GB GPU memory for VGG-16 net
0. Object Detection Network (Fast R-CNN)
    - 3GB GPU memory for ZF net
    - 8GB GPU memory for VGG-16 net


### Preparation for Testing:
0.	Run `fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m` to download a compiled Caffe mex (for Windows only).
0.	Run `faster_rcnn_build.m`
0.	Run `startup.m`


### Testing Demo:
0.	Run `fetch_data/fetch_faster_rcnn_final_model.m` to download our trained models.
0.	Run `experiments/script_faster_rcnn_demo.m` to test a single demo image.
    - You will see the timing information as below. We get the following running time on K40 @ 875 MHz and Intel Xeon CPU E5-2650 v2 @ 2.60GHz for the demo images with VGG-16:
	```Shell
	001763.jpg (500x375): time 0.201s (resize+conv+proposal: 0.150s nms+regionwise: 0.052s)
	004545.jpg (500x375): time 0.201s (resize+conv+proposal: 0.151s nms+regionwise: 0.050s)
	000542.jpg (500x375): time 0.192s (resize+conv+proposal: 0.151s nms+regionwise: 0.041s)
	000456.jpg (500x375): time 0.202s (resize+conv+proposal: 0.152s nms+regionwise: 0.050s)
	001150.jpg (500x375): time 0.194s (resize+conv+proposal: 0.151s nms+regionwise: 0.043s)
	mean time: 0.198s
	```
	and with ZF net:
	```Shell
	001763.jpg (500x375): time 0.061s (resize+conv+proposal: 0.032s nms+regionwise: 0.029s)
	004545.jpg (500x375): time 0.063s (resize+conv+proposal: 0.034s nms+regionwise: 0.029s)
	000542.jpg (500x375): time 0.052s (resize+conv+proposal: 0.034s nms+regionwise: 0.018s)
	000456.jpg (500x375): time 0.062s (resize+conv+proposal: 0.034s nms+regionwise: 0.028s)
	001150.jpg (500x375): time 0.058s (resize+conv+proposal: 0.034s nms+regionwise: 0.023s)
	mean time: 0.059s
	```
    - The visual results might be different from those in the paper due to numerical variations.	
    - Running time on other GPUs

    |       GPU / mean time        |        VGG-16        |        ZF     |     
    | :--------------------------: |:--------------------:|:------------: |
    |  K40                         |        198ms         |       59ms    |
    |  Titan Black                 |        174ms         |       56ms    |
    | Titan X                      |        151ms         |       59ms    |

### Preparation for Training:
0.	Run `fetch_data/fetch_model_ZF.m` to download an ImageNet-pre-trained ZF net.
0.	Run `fetch_data/fetch_model_VGG16.m` to download an ImageNet-pre-trained VGG-16 net.
0.	Download VOC 2007 and 2012 data to ./datasets


### Training:
0. Run `experiments/script_faster_rcnn_VOC2007_ZF.m` to train a model with ZF net. It runs four steps as follows:
    - Train RPN with conv layers tuned; compute RPN results on the train/test sets.
    - Train Fast R-CNN with conv layers tuned using step-1 RPN proposals; evaluate detection mAP.
    - Train RPN with conv layers fixed; compute RPN results on the train/test sets. 
    - Train Fast R-CNN with conv layers fixed using step-3 RPN proposals; evaluate detection mAP.
    - **Note**: the entire training time is ~12 hours on K40.
0. Run `experiments/script_faster_rcnn_VOC2007_VGG16.m` to train a model with VGG net.
    - **Note**: the entire training time is ~2 days on K40.
0. Check other scripts in `./experiments` for more settings.

### Resources

**Note**: This documentation may contain links to third party websites which are provided for your convenience only. Such third party websites are not under Microsoft’s control. Microsoft does not endorse or make any representation guarantee or assurance regarding any third party website content service or product. Third party websites may be subject to the third party’s terms conditions and privacy statements.

0. Experiment logs: [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!110&amp;authkey=!ACpgYZR2MmfklwI&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/wu841r7zmebjp6r/faster_rcnn_logs.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1ntJ3dLv)
0. Regions proposals of our trained RPN:
    - ZF net trained on VOC 07 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!115&amp;authkey=!AJJMrFJHKLXIg5c&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1pKGBDyz)
    - ZF net trained on VOC 07/12 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!117&amp;authkey=!AJiy5F6Cum1iosI&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1jGAgkZW)
    - VGG net trained on VOC 07 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!116&amp;authkey=!AH4Zi_KAaun7MhQ&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1qWHv4JU)
    - VGG net trained on VOC 07/12 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!118&amp;authkey=!AB_lKk3dbGyr1-I&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1c0fQpqg)
    - **Note**: the proposals are in the format of [left top right bottom confidence]

If the automatic ""fetch_data"" fails you may manually download resouces from:

0. Pre-complied caffe mex:
    - Windows-based mex complied with VS2013 and Cuda6.5: [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!111&amp;authkey=!AFVWFGTbViiX5tg&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/m6sg347tiaqpcwy/caffe_mex.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1i3m0i0H)
0. ImageNet-pretrained networks:
    - Zeiler &amp; Fergus (ZF) net [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!113&amp;authkey=!AIzdm0sD_SmhUQ4&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/sw58b2froihzwyf/model_ZF.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1o6zipPS)
    - VGG-16 net [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!114&amp;authkey=!AE8uV9B07dREbhM&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/z5rrji25uskha73/model_VGG16.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1mgzSnI4)
0. Final RPN+FastRCNN models: [OneDrive](https://onedrive.live.com/download?resid=D7AF52BADBA8A4BC!114&amp;authkey=!AERHoxZ-iAx_j34&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/jswrnkaln47clg2/faster_rcnn_final_model.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1hsFKmeK)"
"Abstract:  No true extrasolar Earth analog is known. Hundreds of planets have been found
around Sun-like stars that are either Earth-sized but on shorter periods or
else on year-long orbits but somewhat larger. Under strong assumptions
exoplanet catalogs have been used to make an extrapolated estimate of the rate
at which Sun-like stars host Earth analogs. These studies are complicated by
the fact that every catalog is censored by non-trivial selection effects and
detection efficiencies and every property (period radius etc.) is measured
noisily. Here we present a general hierarchical probabilistic framework for
making justified inferences about the population of exoplanets taking into
account survey completeness and for the first time observational
uncertainties. We are able to make fewer assumptions about the distribution
than previous studies; we only require that the occurrence rate density be a
smooth function of period and radius (employing a Gaussian process). By
applying our method to synthetic catalogs we demonstrate that it produces more
accurate estimates of the whole population than standard procedures based on
weighting by inverse detection efficiency. We apply the method to an existing
catalog of small planet candidates around G dwarf stars (Petigura et al. 2013).
We confirm a previous result that the radius distribution changes slope near
Earth's radius. We find that the rate density of Earth analogs is about 0.02
(per star per natural logarithmic bin in period and radius) with large
uncertainty. This number is much smaller than previous estimates made with the
same data but stronger assumptions.
## This repo has been deprecated. Please see [Detectron](https://github.com/facebookresearch/Detectron) which includes an implementation of [Mask R-CNN](https://arxiv.org/abs/1703.06870).

# *Faster* R-CNN: Towards Real-Time Object Detection with Region Proposal Networks

By Shaoqing Ren Kaiming He Ross Girshick Jian Sun at Microsoft Research

### Introduction

**Faster** R-CNN is an object detection framework based on deep convolutional networks which includes a Region Proposal Network (RPN) and an Object Detection Network. Both networks are trained for sharing convolutional layers for fast testing. 

Faster R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1506.01497).

This repo contains a MATLAB re-implementation of Fast R-CNN. Details about Fast R-CNN are in: [rbgirshick/fast-rcnn](https://github.com/rbgirshick/fast-rcnn).

This code has been tested on Windows 7/8 64-bit Windows Server 2012 R2 and Linux and on MATLAB 2014a.

Python version is available at [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn).

### License

Faster R-CNN is released under the MIT License (refer to the LICENSE file for details).

### Citing Faster R-CNN

If you find Faster R-CNN useful in your research please consider citing:

    @article{ren15fasterrcnn
        Author = {Shaoqing Ren Kaiming He Ross Girshick Jian Sun}
        Title = {{Faster R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks}
        Journal = {arXiv preprint arXiv:1506.01497}
        Year = {2015}
    }

### Main Results
|                           | training data                          | test data            | mAP   | time/img |
| ------------------------- |:--------------------------------------:|:--------------------:|:-----:|:-----: |
| Faster RCNN VGG-16       | VOC 2007 trainval                      | VOC 2007 test        | 69.9% | 198ms |
| Faster RCNN VGG-16       | VOC 2007 trainval + 2012 trainval      | VOC 2007 test        | 73.2% | 198ms |
| Faster RCNN VGG-16       | VOC 2012 trainval                      | VOC 2012 test        | 67.0% | 198ms |
| Faster RCNN VGG-16       | VOC 2007 trainval&amp;test + 2012 trainval | VOC 2012 test        | 70.4% | 198ms |

**Note**: The mAP results are subject to random variations. We have run 5 times independently for ZF net and the mAPs are 59.9 (as in the paper) 60.4 59.5 60.1 and 59.5 with a mean of 59.88 and std 0.39.


### Contents
0. [Requirements: software](#requirements-software)
0. [Requirements: hardware](#requirements-hardware)
0. [Preparation for Testing](#preparation-for-testing)
0. [Testing Demo](#testing-demo)
0. [Preparation for Training](#preparation-for-training)
0. [Training](#training)
0. [Resources](#resources)


### Requirements: software

0. `Caffe` build for Faster R-CNN (included in this repository see `external/caffe`)
    - If you are using Windows you may download a compiled mex file by running `fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m`
    - If you are using Linux or you want to compile for Windows please follow the [instructions](https://github.com/ShaoqingRen/caffe/tree/faster-R-CNN) on our Caffe branch.
0.	MATLAB
 
    
### Requirements: hardware

GPU: Titan Titan Black Titan X K20 K40 K80.

0. Region Proposal Network (RPN)
    - 2GB GPU memory for ZF net
    - 5GB GPU memory for VGG-16 net
0. Object Detection Network (Fast R-CNN)
    - 3GB GPU memory for ZF net
    - 8GB GPU memory for VGG-16 net


### Preparation for Testing:
0.	Run `fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m` to download a compiled Caffe mex (for Windows only).
0.	Run `faster_rcnn_build.m`
0.	Run `startup.m`


### Testing Demo:
0.	Run `fetch_data/fetch_faster_rcnn_final_model.m` to download our trained models.
0.	Run `experiments/script_faster_rcnn_demo.m` to test a single demo image.
    - You will see the timing information as below. We get the following running time on K40 @ 875 MHz and Intel Xeon CPU E5-2650 v2 @ 2.60GHz for the demo images with VGG-16:
	```Shell
	001763.jpg (500x375): time 0.201s (resize+conv+proposal: 0.150s nms+regionwise: 0.052s)
	004545.jpg (500x375): time 0.201s (resize+conv+proposal: 0.151s nms+regionwise: 0.050s)
	000542.jpg (500x375): time 0.192s (resize+conv+proposal: 0.151s nms+regionwise: 0.041s)
	000456.jpg (500x375): time 0.202s (resize+conv+proposal: 0.152s nms+regionwise: 0.050s)
	001150.jpg (500x375): time 0.194s (resize+conv+proposal: 0.151s nms+regionwise: 0.043s)
	mean time: 0.198s
	```
	and with ZF net:
	```Shell
	001763.jpg (500x375): time 0.061s (resize+conv+proposal: 0.032s nms+regionwise: 0.029s)
	004545.jpg (500x375): time 0.063s (resize+conv+proposal: 0.034s nms+regionwise: 0.029s)
	000542.jpg (500x375): time 0.052s (resize+conv+proposal: 0.034s nms+regionwise: 0.018s)
	000456.jpg (500x375): time 0.062s (resize+conv+proposal: 0.034s nms+regionwise: 0.028s)
	001150.jpg (500x375): time 0.058s (resize+conv+proposal: 0.034s nms+regionwise: 0.023s)
	mean time: 0.059s
	```
    - The visual results might be different from those in the paper due to numerical variations.	
    - Running time on other GPUs

    |       GPU / mean time        |        VGG-16        |        ZF     |     
    | :--------------------------: |:--------------------:|:------------: |
    |  K40                         |        198ms         |       59ms    |
    |  Titan Black                 |        174ms         |       56ms    |
    | Titan X                      |        151ms         |       59ms    |

### Preparation for Training:
0.	Run `fetch_data/fetch_model_ZF.m` to download an ImageNet-pre-trained ZF net.
0.	Run `fetch_data/fetch_model_VGG16.m` to download an ImageNet-pre-trained VGG-16 net.
0.	Download VOC 2007 and 2012 data to ./datasets


### Training:
0. Run `experiments/script_faster_rcnn_VOC2007_ZF.m` to train a model with ZF net. It runs four steps as follows:
    - Train RPN with conv layers tuned; compute RPN results on the train/test sets.
    - Train Fast R-CNN with conv layers tuned using step-1 RPN proposals; evaluate detection mAP.
    - Train RPN with conv layers fixed; compute RPN results on the train/test sets. 
    - Train Fast R-CNN with conv layers fixed using step-3 RPN proposals; evaluate detection mAP.
    - **Note**: the entire training time is ~12 hours on K40.
0. Run `experiments/script_faster_rcnn_VOC2007_VGG16.m` to train a model with VGG net.
    - **Note**: the entire training time is ~2 days on K40.
0. Check other scripts in `./experiments` for more settings.

### Resources

**Note**: This documentation may contain links to third party websites which are provided for your convenience only. Such third party websites are not under Microsoft’s control. Microsoft does not endorse or make any representation guarantee or assurance regarding any third party website content service or product. Third party websites may be subject to the third party’s terms conditions and privacy statements.

0. Experiment logs: [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!110&amp;authkey=!ACpgYZR2MmfklwI&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/wu841r7zmebjp6r/faster_rcnn_logs.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1ntJ3dLv)
0. Regions proposals of our trained RPN:
    - ZF net trained on VOC 07 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!115&amp;authkey=!AJJMrFJHKLXIg5c&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1pKGBDyz)
    - ZF net trained on VOC 07/12 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!117&amp;authkey=!AJiy5F6Cum1iosI&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1jGAgkZW)
    - VGG net trained on VOC 07 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!116&amp;authkey=!AH4Zi_KAaun7MhQ&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1qWHv4JU)
    - VGG net trained on VOC 07/12 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!118&amp;authkey=!AB_lKk3dbGyr1-I&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1c0fQpqg)
    - **Note**: the proposals are in the format of [left top right bottom confidence]

If the automatic ""fetch_data"" fails you may manually download resouces from:

0. Pre-complied caffe mex:
    - Windows-based mex complied with VS2013 and Cuda6.5: [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!111&amp;authkey=!AFVWFGTbViiX5tg&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/m6sg347tiaqpcwy/caffe_mex.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1i3m0i0H)
0. ImageNet-pretrained networks:
    - Zeiler &amp; Fergus (ZF) net [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!113&amp;authkey=!AIzdm0sD_SmhUQ4&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/sw58b2froihzwyf/model_ZF.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1o6zipPS)
    - VGG-16 net [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!114&amp;authkey=!AE8uV9B07dREbhM&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/z5rrji25uskha73/model_VGG16.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1mgzSnI4)
0. Final RPN+FastRCNN models: [OneDrive](https://onedrive.live.com/download?resid=D7AF52BADBA8A4BC!114&amp;authkey=!AERHoxZ-iAx_j34&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/jswrnkaln47clg2/faster_rcnn_final_model.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1hsFKmeK)"
"Abstract:  Open source software projects evolve thanks to a group of volunteers that
help in their development. Thus the success of these projects depends on their
ability to attract (and keep) developers. We believe the openness of a project
i.e. how easy is for a new user to actively contribute to it can help to make
a project more attractive. To explore the openness of a software project we
propose three metrics focused on: (1) the distribution of the project
community (2) the rate of acceptance of external contributions and (3) the
time it takes to become an official collaborator of the project. We have
adapted and applied these metrics to a subset of GitHub projects thus giving
some practical findings on their openness.
## This repo has been deprecated. Please see [Detectron](https://github.com/facebookresearch/Detectron) which includes an implementation of [Mask R-CNN](https://arxiv.org/abs/1703.06870).

# *Faster* R-CNN: Towards Real-Time Object Detection with Region Proposal Networks

By Shaoqing Ren Kaiming He Ross Girshick Jian Sun at Microsoft Research

### Introduction

**Faster** R-CNN is an object detection framework based on deep convolutional networks which includes a Region Proposal Network (RPN) and an Object Detection Network. Both networks are trained for sharing convolutional layers for fast testing. 

Faster R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1506.01497).

This repo contains a MATLAB re-implementation of Fast R-CNN. Details about Fast R-CNN are in: [rbgirshick/fast-rcnn](https://github.com/rbgirshick/fast-rcnn).

This code has been tested on Windows 7/8 64-bit Windows Server 2012 R2 and Linux and on MATLAB 2014a.

Python version is available at [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn).

### License

Faster R-CNN is released under the MIT License (refer to the LICENSE file for details).

### Citing Faster R-CNN

If you find Faster R-CNN useful in your research please consider citing:

    @article{ren15fasterrcnn
        Author = {Shaoqing Ren Kaiming He Ross Girshick Jian Sun}
        Title = {{Faster R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks}
        Journal = {arXiv preprint arXiv:1506.01497}
        Year = {2015}
    }

### Main Results
|                           | training data                          | test data            | mAP   | time/img |
| ------------------------- |:--------------------------------------:|:--------------------:|:-----:|:-----: |
| Faster RCNN VGG-16       | VOC 2007 trainval                      | VOC 2007 test        | 69.9% | 198ms |
| Faster RCNN VGG-16       | VOC 2007 trainval + 2012 trainval      | VOC 2007 test        | 73.2% | 198ms |
| Faster RCNN VGG-16       | VOC 2012 trainval                      | VOC 2012 test        | 67.0% | 198ms |
| Faster RCNN VGG-16       | VOC 2007 trainval&amp;test + 2012 trainval | VOC 2012 test        | 70.4% | 198ms |

**Note**: The mAP results are subject to random variations. We have run 5 times independently for ZF net and the mAPs are 59.9 (as in the paper) 60.4 59.5 60.1 and 59.5 with a mean of 59.88 and std 0.39.


### Contents
0. [Requirements: software](#requirements-software)
0. [Requirements: hardware](#requirements-hardware)
0. [Preparation for Testing](#preparation-for-testing)
0. [Testing Demo](#testing-demo)
0. [Preparation for Training](#preparation-for-training)
0. [Training](#training)
0. [Resources](#resources)


### Requirements: software

0. `Caffe` build for Faster R-CNN (included in this repository see `external/caffe`)
    - If you are using Windows you may download a compiled mex file by running `fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m`
    - If you are using Linux or you want to compile for Windows please follow the [instructions](https://github.com/ShaoqingRen/caffe/tree/faster-R-CNN) on our Caffe branch.
0.	MATLAB
 
    
### Requirements: hardware

GPU: Titan Titan Black Titan X K20 K40 K80.

0. Region Proposal Network (RPN)
    - 2GB GPU memory for ZF net
    - 5GB GPU memory for VGG-16 net
0. Object Detection Network (Fast R-CNN)
    - 3GB GPU memory for ZF net
    - 8GB GPU memory for VGG-16 net


### Preparation for Testing:
0.	Run `fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m` to download a compiled Caffe mex (for Windows only).
0.	Run `faster_rcnn_build.m`
0.	Run `startup.m`


### Testing Demo:
0.	Run `fetch_data/fetch_faster_rcnn_final_model.m` to download our trained models.
0.	Run `experiments/script_faster_rcnn_demo.m` to test a single demo image.
    - You will see the timing information as below. We get the following running time on K40 @ 875 MHz and Intel Xeon CPU E5-2650 v2 @ 2.60GHz for the demo images with VGG-16:
	```Shell
	001763.jpg (500x375): time 0.201s (resize+conv+proposal: 0.150s nms+regionwise: 0.052s)
	004545.jpg (500x375): time 0.201s (resize+conv+proposal: 0.151s nms+regionwise: 0.050s)
	000542.jpg (500x375): time 0.192s (resize+conv+proposal: 0.151s nms+regionwise: 0.041s)
	000456.jpg (500x375): time 0.202s (resize+conv+proposal: 0.152s nms+regionwise: 0.050s)
	001150.jpg (500x375): time 0.194s (resize+conv+proposal: 0.151s nms+regionwise: 0.043s)
	mean time: 0.198s
	```
	and with ZF net:
	```Shell
	001763.jpg (500x375): time 0.061s (resize+conv+proposal: 0.032s nms+regionwise: 0.029s)
	004545.jpg (500x375): time 0.063s (resize+conv+proposal: 0.034s nms+regionwise: 0.029s)
	000542.jpg (500x375): time 0.052s (resize+conv+proposal: 0.034s nms+regionwise: 0.018s)
	000456.jpg (500x375): time 0.062s (resize+conv+proposal: 0.034s nms+regionwise: 0.028s)
	001150.jpg (500x375): time 0.058s (resize+conv+proposal: 0.034s nms+regionwise: 0.023s)
	mean time: 0.059s
	```
    - The visual results might be different from those in the paper due to numerical variations.	
    - Running time on other GPUs

    |       GPU / mean time        |        VGG-16        |        ZF     |     
    | :--------------------------: |:--------------------:|:------------: |
    |  K40                         |        198ms         |       59ms    |
    |  Titan Black                 |        174ms         |       56ms    |
    | Titan X                      |        151ms         |       59ms    |

### Preparation for Training:
0.	Run `fetch_data/fetch_model_ZF.m` to download an ImageNet-pre-trained ZF net.
0.	Run `fetch_data/fetch_model_VGG16.m` to download an ImageNet-pre-trained VGG-16 net.
0.	Download VOC 2007 and 2012 data to ./datasets


### Training:
0. Run `experiments/script_faster_rcnn_VOC2007_ZF.m` to train a model with ZF net. It runs four steps as follows:
    - Train RPN with conv layers tuned; compute RPN results on the train/test sets.
    - Train Fast R-CNN with conv layers tuned using step-1 RPN proposals; evaluate detection mAP.
    - Train RPN with conv layers fixed; compute RPN results on the train/test sets. 
    - Train Fast R-CNN with conv layers fixed using step-3 RPN proposals; evaluate detection mAP.
    - **Note**: the entire training time is ~12 hours on K40.
0. Run `experiments/script_faster_rcnn_VOC2007_VGG16.m` to train a model with VGG net.
    - **Note**: the entire training time is ~2 days on K40.
0. Check other scripts in `./experiments` for more settings.

### Resources

**Note**: This documentation may contain links to third party websites which are provided for your convenience only. Such third party websites are not under Microsoft’s control. Microsoft does not endorse or make any representation guarantee or assurance regarding any third party website content service or product. Third party websites may be subject to the third party’s terms conditions and privacy statements.

0. Experiment logs: [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!110&amp;authkey=!ACpgYZR2MmfklwI&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/wu841r7zmebjp6r/faster_rcnn_logs.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1ntJ3dLv)
0. Regions proposals of our trained RPN:
    - ZF net trained on VOC 07 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!115&amp;authkey=!AJJMrFJHKLXIg5c&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1pKGBDyz)
    - ZF net trained on VOC 07/12 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!117&amp;authkey=!AJiy5F6Cum1iosI&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1jGAgkZW)
    - VGG net trained on VOC 07 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!116&amp;authkey=!AH4Zi_KAaun7MhQ&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1qWHv4JU)
    - VGG net trained on VOC 07/12 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!118&amp;authkey=!AB_lKk3dbGyr1-I&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1c0fQpqg)
    - **Note**: the proposals are in the format of [left top right bottom confidence]

If the automatic ""fetch_data"" fails you may manually download resouces from:

0. Pre-complied caffe mex:
    - Windows-based mex complied with VS2013 and Cuda6.5: [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!111&amp;authkey=!AFVWFGTbViiX5tg&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/m6sg347tiaqpcwy/caffe_mex.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1i3m0i0H)
0. ImageNet-pretrained networks:
    - Zeiler &amp; Fergus (ZF) net [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!113&amp;authkey=!AIzdm0sD_SmhUQ4&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/sw58b2froihzwyf/model_ZF.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1o6zipPS)
    - VGG-16 net [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!114&amp;authkey=!AE8uV9B07dREbhM&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/z5rrji25uskha73/model_VGG16.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1mgzSnI4)
0. Final RPN+FastRCNN models: [OneDrive](https://onedrive.live.com/download?resid=D7AF52BADBA8A4BC!114&amp;authkey=!AERHoxZ-iAx_j34&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/jswrnkaln47clg2/faster_rcnn_final_model.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1hsFKmeK)"
"Abstract:  GitHub is the most popular repository for open source code. It has more than
3.5 million users as the company declared in April 2013 and more than 10
million repositories as of December 2013. It has a publicly accessible API
and since March 2012 it also publishes a stream of all the events occurring
on public projects. Interactions among GitHub users are of a complex nature and
take place in different forms. Developers create and fork repositories push
code approve code pushed by others bookmark their favorite projects and
follow other developers to keep track of their activities.
In this paper we present a characterization of GitHub as both a social
network and a collaborative platform. To the best of our knowledge this is the
first quantitative study about the interactions happening on GitHub. We analyze
the logs from the service over 18 months (between March 11 2012 and September
11 2013) describing 183.54 million events and we obtain information about
2.19 million users and 5.68 million repositories both growing linearly in
time. We show that the distributions of the number of contributors per project
watchers per project and followers per user show a power-law-like shape. We
analyze social ties and repository-mediated collaboration patterns and we
observe a remarkably low level of reciprocity of the social connections. We
also measure the activity of each user in terms of authored events and we
observe that very active users do not necessarily have a large number of
followers. Finally we provide a geographic characterization of the centers of
activity and we investigate how distance influences collaboration.
## This repo has been deprecated. Please see [Detectron](https://github.com/facebookresearch/Detectron) which includes an implementation of [Mask R-CNN](https://arxiv.org/abs/1703.06870).

# *Faster* R-CNN: Towards Real-Time Object Detection with Region Proposal Networks

By Shaoqing Ren Kaiming He Ross Girshick Jian Sun at Microsoft Research

### Introduction

**Faster** R-CNN is an object detection framework based on deep convolutional networks which includes a Region Proposal Network (RPN) and an Object Detection Network. Both networks are trained for sharing convolutional layers for fast testing. 

Faster R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1506.01497).

This repo contains a MATLAB re-implementation of Fast R-CNN. Details about Fast R-CNN are in: [rbgirshick/fast-rcnn](https://github.com/rbgirshick/fast-rcnn).

This code has been tested on Windows 7/8 64-bit Windows Server 2012 R2 and Linux and on MATLAB 2014a.

Python version is available at [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn).

### License

Faster R-CNN is released under the MIT License (refer to the LICENSE file for details).

### Citing Faster R-CNN

If you find Faster R-CNN useful in your research please consider citing:

    @article{ren15fasterrcnn
        Author = {Shaoqing Ren Kaiming He Ross Girshick Jian Sun}
        Title = {{Faster R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks}
        Journal = {arXiv preprint arXiv:1506.01497}
        Year = {2015}
    }

### Main Results
|                           | training data                          | test data            | mAP   | time/img |
| ------------------------- |:--------------------------------------:|:--------------------:|:-----:|:-----: |
| Faster RCNN VGG-16       | VOC 2007 trainval                      | VOC 2007 test        | 69.9% | 198ms |
| Faster RCNN VGG-16       | VOC 2007 trainval + 2012 trainval      | VOC 2007 test        | 73.2% | 198ms |
| Faster RCNN VGG-16       | VOC 2012 trainval                      | VOC 2012 test        | 67.0% | 198ms |
| Faster RCNN VGG-16       | VOC 2007 trainval&amp;test + 2012 trainval | VOC 2012 test        | 70.4% | 198ms |

**Note**: The mAP results are subject to random variations. We have run 5 times independently for ZF net and the mAPs are 59.9 (as in the paper) 60.4 59.5 60.1 and 59.5 with a mean of 59.88 and std 0.39.


### Contents
0. [Requirements: software](#requirements-software)
0. [Requirements: hardware](#requirements-hardware)
0. [Preparation for Testing](#preparation-for-testing)
0. [Testing Demo](#testing-demo)
0. [Preparation for Training](#preparation-for-training)
0. [Training](#training)
0. [Resources](#resources)


### Requirements: software

0. `Caffe` build for Faster R-CNN (included in this repository see `external/caffe`)
    - If you are using Windows you may download a compiled mex file by running `fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m`
    - If you are using Linux or you want to compile for Windows please follow the [instructions](https://github.com/ShaoqingRen/caffe/tree/faster-R-CNN) on our Caffe branch.
0.	MATLAB
 
    
### Requirements: hardware

GPU: Titan Titan Black Titan X K20 K40 K80.

0. Region Proposal Network (RPN)
    - 2GB GPU memory for ZF net
    - 5GB GPU memory for VGG-16 net
0. Object Detection Network (Fast R-CNN)
    - 3GB GPU memory for ZF net
    - 8GB GPU memory for VGG-16 net


### Preparation for Testing:
0.	Run `fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m` to download a compiled Caffe mex (for Windows only).
0.	Run `faster_rcnn_build.m`
0.	Run `startup.m`


### Testing Demo:
0.	Run `fetch_data/fetch_faster_rcnn_final_model.m` to download our trained models.
0.	Run `experiments/script_faster_rcnn_demo.m` to test a single demo image.
    - You will see the timing information as below. We get the following running time on K40 @ 875 MHz and Intel Xeon CPU E5-2650 v2 @ 2.60GHz for the demo images with VGG-16:
	```Shell
	001763.jpg (500x375): time 0.201s (resize+conv+proposal: 0.150s nms+regionwise: 0.052s)
	004545.jpg (500x375): time 0.201s (resize+conv+proposal: 0.151s nms+regionwise: 0.050s)
	000542.jpg (500x375): time 0.192s (resize+conv+proposal: 0.151s nms+regionwise: 0.041s)
	000456.jpg (500x375): time 0.202s (resize+conv+proposal: 0.152s nms+regionwise: 0.050s)
	001150.jpg (500x375): time 0.194s (resize+conv+proposal: 0.151s nms+regionwise: 0.043s)
	mean time: 0.198s
	```
	and with ZF net:
	```Shell
	001763.jpg (500x375): time 0.061s (resize+conv+proposal: 0.032s nms+regionwise: 0.029s)
	004545.jpg (500x375): time 0.063s (resize+conv+proposal: 0.034s nms+regionwise: 0.029s)
	000542.jpg (500x375): time 0.052s (resize+conv+proposal: 0.034s nms+regionwise: 0.018s)
	000456.jpg (500x375): time 0.062s (resize+conv+proposal: 0.034s nms+regionwise: 0.028s)
	001150.jpg (500x375): time 0.058s (resize+conv+proposal: 0.034s nms+regionwise: 0.023s)
	mean time: 0.059s
	```
    - The visual results might be different from those in the paper due to numerical variations.	
    - Running time on other GPUs

    |       GPU / mean time        |        VGG-16        |        ZF     |     
    | :--------------------------: |:--------------------:|:------------: |
    |  K40                         |        198ms         |       59ms    |
    |  Titan Black                 |        174ms         |       56ms    |
    | Titan X                      |        151ms         |       59ms    |

### Preparation for Training:
0.	Run `fetch_data/fetch_model_ZF.m` to download an ImageNet-pre-trained ZF net.
0.	Run `fetch_data/fetch_model_VGG16.m` to download an ImageNet-pre-trained VGG-16 net.
0.	Download VOC 2007 and 2012 data to ./datasets


### Training:
0. Run `experiments/script_faster_rcnn_VOC2007_ZF.m` to train a model with ZF net. It runs four steps as follows:
    - Train RPN with conv layers tuned; compute RPN results on the train/test sets.
    - Train Fast R-CNN with conv layers tuned using step-1 RPN proposals; evaluate detection mAP.
    - Train RPN with conv layers fixed; compute RPN results on the train/test sets. 
    - Train Fast R-CNN with conv layers fixed using step-3 RPN proposals; evaluate detection mAP.
    - **Note**: the entire training time is ~12 hours on K40.
0. Run `experiments/script_faster_rcnn_VOC2007_VGG16.m` to train a model with VGG net.
    - **Note**: the entire training time is ~2 days on K40.
0. Check other scripts in `./experiments` for more settings.

### Resources

**Note**: This documentation may contain links to third party websites which are provided for your convenience only. Such third party websites are not under Microsoft’s control. Microsoft does not endorse or make any representation guarantee or assurance regarding any third party website content service or product. Third party websites may be subject to the third party’s terms conditions and privacy statements.

0. Experiment logs: [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!110&amp;authkey=!ACpgYZR2MmfklwI&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/wu841r7zmebjp6r/faster_rcnn_logs.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1ntJ3dLv)
0. Regions proposals of our trained RPN:
    - ZF net trained on VOC 07 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!115&amp;authkey=!AJJMrFJHKLXIg5c&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1pKGBDyz)
    - ZF net trained on VOC 07/12 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!117&amp;authkey=!AJiy5F6Cum1iosI&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1jGAgkZW)
    - VGG net trained on VOC 07 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!116&amp;authkey=!AH4Zi_KAaun7MhQ&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1qWHv4JU)
    - VGG net trained on VOC 07/12 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!118&amp;authkey=!AB_lKk3dbGyr1-I&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1c0fQpqg)
    - **Note**: the proposals are in the format of [left top right bottom confidence]

If the automatic ""fetch_data"" fails you may manually download resouces from:

0. Pre-complied caffe mex:
    - Windows-based mex complied with VS2013 and Cuda6.5: [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!111&amp;authkey=!AFVWFGTbViiX5tg&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/m6sg347tiaqpcwy/caffe_mex.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1i3m0i0H)
0. ImageNet-pretrained networks:
    - Zeiler &amp; Fergus (ZF) net [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!113&amp;authkey=!AIzdm0sD_SmhUQ4&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/sw58b2froihzwyf/model_ZF.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1o6zipPS)
    - VGG-16 net [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!114&amp;authkey=!AE8uV9B07dREbhM&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/z5rrji25uskha73/model_VGG16.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1mgzSnI4)
0. Final RPN+FastRCNN models: [OneDrive](https://onedrive.live.com/download?resid=D7AF52BADBA8A4BC!114&amp;authkey=!AERHoxZ-iAx_j34&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/jswrnkaln47clg2/faster_rcnn_final_model.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1hsFKmeK)"
"Abstract:  K-mer abundance analysis is widely used for many purposes in nucleotide
sequence analysis including data preprocessing for de novo assembly repeat
detection and sequencing coverage estimation. We present the khmer software
package for fast and memory efficient online counting of k-mers in sequencing
data sets. Unlike previous methods based on data structures such as hash
tables suffix arrays and trie structures khmer relies entirely on a simple
probabilistic data structure a Count-Min Sketch. The Count-Min Sketch permits
online updating and retrieval of k-mer counts in memory which is necessary to
support online k-mer analysis algorithms. On sparse data sets this data
structure is considerably more memory efficient than any exact data structure.
In exchange the use of a Count-Min Sketch introduces a systematic overcount
for k-mers; moreover only the counts and not the k-mers are stored. Here we
analyze the speed the memory usage and the miscount rate of khmer for
generating k-mer frequency distributions and retrieving k-mer counts for
individual k-mers. We also compare the performance of khmer to several other
k-mer counting packages including Tallymer Jellyfish BFCounter DSK KMC
Turtle and KAnalyze. Finally we examine the effectiveness of profiling
sequencing error k-mer abundance trimming and digital normalization of reads
in the context of high khmer false positive rates. khmer is implemented in C++
wrapped in a Python interface offers a tested and robust API and is freely
available under the BSD license at this http URL.
## This repo has been deprecated. Please see [Detectron](https://github.com/facebookresearch/Detectron) which includes an implementation of [Mask R-CNN](https://arxiv.org/abs/1703.06870).

# *Faster* R-CNN: Towards Real-Time Object Detection with Region Proposal Networks

By Shaoqing Ren Kaiming He Ross Girshick Jian Sun at Microsoft Research

### Introduction

**Faster** R-CNN is an object detection framework based on deep convolutional networks which includes a Region Proposal Network (RPN) and an Object Detection Network. Both networks are trained for sharing convolutional layers for fast testing. 

Faster R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1506.01497).

This repo contains a MATLAB re-implementation of Fast R-CNN. Details about Fast R-CNN are in: [rbgirshick/fast-rcnn](https://github.com/rbgirshick/fast-rcnn).

This code has been tested on Windows 7/8 64-bit Windows Server 2012 R2 and Linux and on MATLAB 2014a.

Python version is available at [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn).

### License

Faster R-CNN is released under the MIT License (refer to the LICENSE file for details).

### Citing Faster R-CNN

If you find Faster R-CNN useful in your research please consider citing:

    @article{ren15fasterrcnn
        Author = {Shaoqing Ren Kaiming He Ross Girshick Jian Sun}
        Title = {{Faster R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks}
        Journal = {arXiv preprint arXiv:1506.01497}
        Year = {2015}
    }

### Main Results
|                           | training data                          | test data            | mAP   | time/img |
| ------------------------- |:--------------------------------------:|:--------------------:|:-----:|:-----: |
| Faster RCNN VGG-16       | VOC 2007 trainval                      | VOC 2007 test        | 69.9% | 198ms |
| Faster RCNN VGG-16       | VOC 2007 trainval + 2012 trainval      | VOC 2007 test        | 73.2% | 198ms |
| Faster RCNN VGG-16       | VOC 2012 trainval                      | VOC 2012 test        | 67.0% | 198ms |
| Faster RCNN VGG-16       | VOC 2007 trainval&amp;test + 2012 trainval | VOC 2012 test        | 70.4% | 198ms |

**Note**: The mAP results are subject to random variations. We have run 5 times independently for ZF net and the mAPs are 59.9 (as in the paper) 60.4 59.5 60.1 and 59.5 with a mean of 59.88 and std 0.39.


### Contents
0. [Requirements: software](#requirements-software)
0. [Requirements: hardware](#requirements-hardware)
0. [Preparation for Testing](#preparation-for-testing)
0. [Testing Demo](#testing-demo)
0. [Preparation for Training](#preparation-for-training)
0. [Training](#training)
0. [Resources](#resources)


### Requirements: software

0. `Caffe` build for Faster R-CNN (included in this repository see `external/caffe`)
    - If you are using Windows you may download a compiled mex file by running `fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m`
    - If you are using Linux or you want to compile for Windows please follow the [instructions](https://github.com/ShaoqingRen/caffe/tree/faster-R-CNN) on our Caffe branch.
0.	MATLAB
 
    
### Requirements: hardware

GPU: Titan Titan Black Titan X K20 K40 K80.

0. Region Proposal Network (RPN)
    - 2GB GPU memory for ZF net
    - 5GB GPU memory for VGG-16 net
0. Object Detection Network (Fast R-CNN)
    - 3GB GPU memory for ZF net
    - 8GB GPU memory for VGG-16 net


### Preparation for Testing:
0.	Run `fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m` to download a compiled Caffe mex (for Windows only).
0.	Run `faster_rcnn_build.m`
0.	Run `startup.m`


### Testing Demo:
0.	Run `fetch_data/fetch_faster_rcnn_final_model.m` to download our trained models.
0.	Run `experiments/script_faster_rcnn_demo.m` to test a single demo image.
    - You will see the timing information as below. We get the following running time on K40 @ 875 MHz and Intel Xeon CPU E5-2650 v2 @ 2.60GHz for the demo images with VGG-16:
	```Shell
	001763.jpg (500x375): time 0.201s (resize+conv+proposal: 0.150s nms+regionwise: 0.052s)
	004545.jpg (500x375): time 0.201s (resize+conv+proposal: 0.151s nms+regionwise: 0.050s)
	000542.jpg (500x375): time 0.192s (resize+conv+proposal: 0.151s nms+regionwise: 0.041s)
	000456.jpg (500x375): time 0.202s (resize+conv+proposal: 0.152s nms+regionwise: 0.050s)
	001150.jpg (500x375): time 0.194s (resize+conv+proposal: 0.151s nms+regionwise: 0.043s)
	mean time: 0.198s
	```
	and with ZF net:
	```Shell
	001763.jpg (500x375): time 0.061s (resize+conv+proposal: 0.032s nms+regionwise: 0.029s)
	004545.jpg (500x375): time 0.063s (resize+conv+proposal: 0.034s nms+regionwise: 0.029s)
	000542.jpg (500x375): time 0.052s (resize+conv+proposal: 0.034s nms+regionwise: 0.018s)
	000456.jpg (500x375): time 0.062s (resize+conv+proposal: 0.034s nms+regionwise: 0.028s)
	001150.jpg (500x375): time 0.058s (resize+conv+proposal: 0.034s nms+regionwise: 0.023s)
	mean time: 0.059s
	```
    - The visual results might be different from those in the paper due to numerical variations.	
    - Running time on other GPUs

    |       GPU / mean time        |        VGG-16        |        ZF     |     
    | :--------------------------: |:--------------------:|:------------: |
    |  K40                         |        198ms         |       59ms    |
    |  Titan Black                 |        174ms         |       56ms    |
    | Titan X                      |        151ms         |       59ms    |

### Preparation for Training:
0.	Run `fetch_data/fetch_model_ZF.m` to download an ImageNet-pre-trained ZF net.
0.	Run `fetch_data/fetch_model_VGG16.m` to download an ImageNet-pre-trained VGG-16 net.
0.	Download VOC 2007 and 2012 data to ./datasets


### Training:
0. Run `experiments/script_faster_rcnn_VOC2007_ZF.m` to train a model with ZF net. It runs four steps as follows:
    - Train RPN with conv layers tuned; compute RPN results on the train/test sets.
    - Train Fast R-CNN with conv layers tuned using step-1 RPN proposals; evaluate detection mAP.
    - Train RPN with conv layers fixed; compute RPN results on the train/test sets. 
    - Train Fast R-CNN with conv layers fixed using step-3 RPN proposals; evaluate detection mAP.
    - **Note**: the entire training time is ~12 hours on K40.
0. Run `experiments/script_faster_rcnn_VOC2007_VGG16.m` to train a model with VGG net.
    - **Note**: the entire training time is ~2 days on K40.
0. Check other scripts in `./experiments` for more settings.

### Resources

**Note**: This documentation may contain links to third party websites which are provided for your convenience only. Such third party websites are not under Microsoft’s control. Microsoft does not endorse or make any representation guarantee or assurance regarding any third party website content service or product. Third party websites may be subject to the third party’s terms conditions and privacy statements.

0. Experiment logs: [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!110&amp;authkey=!ACpgYZR2MmfklwI&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/wu841r7zmebjp6r/faster_rcnn_logs.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1ntJ3dLv)
0. Regions proposals of our trained RPN:
    - ZF net trained on VOC 07 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!115&amp;authkey=!AJJMrFJHKLXIg5c&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1pKGBDyz)
    - ZF net trained on VOC 07/12 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!117&amp;authkey=!AJiy5F6Cum1iosI&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1jGAgkZW)
    - VGG net trained on VOC 07 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!116&amp;authkey=!AH4Zi_KAaun7MhQ&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1qWHv4JU)
    - VGG net trained on VOC 07/12 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!118&amp;authkey=!AB_lKk3dbGyr1-I&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1c0fQpqg)
    - **Note**: the proposals are in the format of [left top right bottom confidence]

If the automatic ""fetch_data"" fails you may manually download resouces from:

0. Pre-complied caffe mex:
    - Windows-based mex complied with VS2013 and Cuda6.5: [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!111&amp;authkey=!AFVWFGTbViiX5tg&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/m6sg347tiaqpcwy/caffe_mex.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1i3m0i0H)
0. ImageNet-pretrained networks:
    - Zeiler &amp; Fergus (ZF) net [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!113&amp;authkey=!AIzdm0sD_SmhUQ4&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/sw58b2froihzwyf/model_ZF.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1o6zipPS)
    - VGG-16 net [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!114&amp;authkey=!AE8uV9B07dREbhM&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/z5rrji25uskha73/model_VGG16.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1mgzSnI4)
0. Final RPN+FastRCNN models: [OneDrive](https://onedrive.live.com/download?resid=D7AF52BADBA8A4BC!114&amp;authkey=!AERHoxZ-iAx_j34&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/jswrnkaln47clg2/faster_rcnn_final_model.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1hsFKmeK)"
"Abstract:  We describe a statistical model to estimate the covariance matrix of matter
tracer two-point correlation functions with cosmological simulations. Assuming
a fixed number of cosmological simulation runs we describe how to build a
`statistical emulator' of the two-point function covariance over a specified
range of input cosmological parameters. Because the simulation runs with
different cosmological models help to constrain the form of the covariance we
predict that the cosmology-dependent covariance may be estimated with a
comparable number of simulations as would be needed to estimate the covariance
for fixed cosmology. Our framework is a necessary first step in planning a
simulations campaign for analyzing the next generation of cosmological surveys.
## This repo has been deprecated. Please see [Detectron](https://github.com/facebookresearch/Detectron) which includes an implementation of [Mask R-CNN](https://arxiv.org/abs/1703.06870).

# *Faster* R-CNN: Towards Real-Time Object Detection with Region Proposal Networks

By Shaoqing Ren Kaiming He Ross Girshick Jian Sun at Microsoft Research

### Introduction

**Faster** R-CNN is an object detection framework based on deep convolutional networks which includes a Region Proposal Network (RPN) and an Object Detection Network. Both networks are trained for sharing convolutional layers for fast testing. 

Faster R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1506.01497).

This repo contains a MATLAB re-implementation of Fast R-CNN. Details about Fast R-CNN are in: [rbgirshick/fast-rcnn](https://github.com/rbgirshick/fast-rcnn).

This code has been tested on Windows 7/8 64-bit Windows Server 2012 R2 and Linux and on MATLAB 2014a.

Python version is available at [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn).

### License

Faster R-CNN is released under the MIT License (refer to the LICENSE file for details).

### Citing Faster R-CNN

If you find Faster R-CNN useful in your research please consider citing:

    @article{ren15fasterrcnn
        Author = {Shaoqing Ren Kaiming He Ross Girshick Jian Sun}
        Title = {{Faster R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks}
        Journal = {arXiv preprint arXiv:1506.01497}
        Year = {2015}
    }

### Main Results
|                           | training data                          | test data            | mAP   | time/img |
| ------------------------- |:--------------------------------------:|:--------------------:|:-----:|:-----: |
| Faster RCNN VGG-16       | VOC 2007 trainval                      | VOC 2007 test        | 69.9% | 198ms |
| Faster RCNN VGG-16       | VOC 2007 trainval + 2012 trainval      | VOC 2007 test        | 73.2% | 198ms |
| Faster RCNN VGG-16       | VOC 2012 trainval                      | VOC 2012 test        | 67.0% | 198ms |
| Faster RCNN VGG-16       | VOC 2007 trainval&amp;test + 2012 trainval | VOC 2012 test        | 70.4% | 198ms |

**Note**: The mAP results are subject to random variations. We have run 5 times independently for ZF net and the mAPs are 59.9 (as in the paper) 60.4 59.5 60.1 and 59.5 with a mean of 59.88 and std 0.39.


### Contents
0. [Requirements: software](#requirements-software)
0. [Requirements: hardware](#requirements-hardware)
0. [Preparation for Testing](#preparation-for-testing)
0. [Testing Demo](#testing-demo)
0. [Preparation for Training](#preparation-for-training)
0. [Training](#training)
0. [Resources](#resources)


### Requirements: software

0. `Caffe` build for Faster R-CNN (included in this repository see `external/caffe`)
    - If you are using Windows you may download a compiled mex file by running `fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m`
    - If you are using Linux or you want to compile for Windows please follow the [instructions](https://github.com/ShaoqingRen/caffe/tree/faster-R-CNN) on our Caffe branch.
0.	MATLAB
 
    
### Requirements: hardware

GPU: Titan Titan Black Titan X K20 K40 K80.

0. Region Proposal Network (RPN)
    - 2GB GPU memory for ZF net
    - 5GB GPU memory for VGG-16 net
0. Object Detection Network (Fast R-CNN)
    - 3GB GPU memory for ZF net
    - 8GB GPU memory for VGG-16 net


### Preparation for Testing:
0.	Run `fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m` to download a compiled Caffe mex (for Windows only).
0.	Run `faster_rcnn_build.m`
0.	Run `startup.m`


### Testing Demo:
0.	Run `fetch_data/fetch_faster_rcnn_final_model.m` to download our trained models.
0.	Run `experiments/script_faster_rcnn_demo.m` to test a single demo image.
    - You will see the timing information as below. We get the following running time on K40 @ 875 MHz and Intel Xeon CPU E5-2650 v2 @ 2.60GHz for the demo images with VGG-16:
	```Shell
	001763.jpg (500x375): time 0.201s (resize+conv+proposal: 0.150s nms+regionwise: 0.052s)
	004545.jpg (500x375): time 0.201s (resize+conv+proposal: 0.151s nms+regionwise: 0.050s)
	000542.jpg (500x375): time 0.192s (resize+conv+proposal: 0.151s nms+regionwise: 0.041s)
	000456.jpg (500x375): time 0.202s (resize+conv+proposal: 0.152s nms+regionwise: 0.050s)
	001150.jpg (500x375): time 0.194s (resize+conv+proposal: 0.151s nms+regionwise: 0.043s)
	mean time: 0.198s
	```
	and with ZF net:
	```Shell
	001763.jpg (500x375): time 0.061s (resize+conv+proposal: 0.032s nms+regionwise: 0.029s)
	004545.jpg (500x375): time 0.063s (resize+conv+proposal: 0.034s nms+regionwise: 0.029s)
	000542.jpg (500x375): time 0.052s (resize+conv+proposal: 0.034s nms+regionwise: 0.018s)
	000456.jpg (500x375): time 0.062s (resize+conv+proposal: 0.034s nms+regionwise: 0.028s)
	001150.jpg (500x375): time 0.058s (resize+conv+proposal: 0.034s nms+regionwise: 0.023s)
	mean time: 0.059s
	```
    - The visual results might be different from those in the paper due to numerical variations.	
    - Running time on other GPUs

    |       GPU / mean time        |        VGG-16        |        ZF     |     
    | :--------------------------: |:--------------------:|:------------: |
    |  K40                         |        198ms         |       59ms    |
    |  Titan Black                 |        174ms         |       56ms    |
    | Titan X                      |        151ms         |       59ms    |

### Preparation for Training:
0.	Run `fetch_data/fetch_model_ZF.m` to download an ImageNet-pre-trained ZF net.
0.	Run `fetch_data/fetch_model_VGG16.m` to download an ImageNet-pre-trained VGG-16 net.
0.	Download VOC 2007 and 2012 data to ./datasets


### Training:
0. Run `experiments/script_faster_rcnn_VOC2007_ZF.m` to train a model with ZF net. It runs four steps as follows:
    - Train RPN with conv layers tuned; compute RPN results on the train/test sets.
    - Train Fast R-CNN with conv layers tuned using step-1 RPN proposals; evaluate detection mAP.
    - Train RPN with conv layers fixed; compute RPN results on the train/test sets. 
    - Train Fast R-CNN with conv layers fixed using step-3 RPN proposals; evaluate detection mAP.
    - **Note**: the entire training time is ~12 hours on K40.
0. Run `experiments/script_faster_rcnn_VOC2007_VGG16.m` to train a model with VGG net.
    - **Note**: the entire training time is ~2 days on K40.
0. Check other scripts in `./experiments` for more settings.

### Resources

**Note**: This documentation may contain links to third party websites which are provided for your convenience only. Such third party websites are not under Microsoft’s control. Microsoft does not endorse or make any representation guarantee or assurance regarding any third party website content service or product. Third party websites may be subject to the third party’s terms conditions and privacy statements.

0. Experiment logs: [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!110&amp;authkey=!ACpgYZR2MmfklwI&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/wu841r7zmebjp6r/faster_rcnn_logs.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1ntJ3dLv)
0. Regions proposals of our trained RPN:
    - ZF net trained on VOC 07 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!115&amp;authkey=!AJJMrFJHKLXIg5c&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1pKGBDyz)
    - ZF net trained on VOC 07/12 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!117&amp;authkey=!AJiy5F6Cum1iosI&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1jGAgkZW)
    - VGG net trained on VOC 07 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!116&amp;authkey=!AH4Zi_KAaun7MhQ&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1qWHv4JU)
    - VGG net trained on VOC 07/12 trainval [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!118&amp;authkey=!AB_lKk3dbGyr1-I&amp;ithint=file%2czip) [BaiduYun](http://pan.baidu.com/s/1c0fQpqg)
    - **Note**: the proposals are in the format of [left top right bottom confidence]

If the automatic ""fetch_data"" fails you may manually download resouces from:

0. Pre-complied caffe mex:
    - Windows-based mex complied with VS2013 and Cuda6.5: [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!111&amp;authkey=!AFVWFGTbViiX5tg&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/m6sg347tiaqpcwy/caffe_mex.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1i3m0i0H)
0. ImageNet-pretrained networks:
    - Zeiler &amp; Fergus (ZF) net [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!113&amp;authkey=!AIzdm0sD_SmhUQ4&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/sw58b2froihzwyf/model_ZF.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1o6zipPS)
    - VGG-16 net [OneDrive](https://onedrive.live.com/download?resid=36FEC490FBC32F1A!114&amp;authkey=!AE8uV9B07dREbhM&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/z5rrji25uskha73/model_VGG16.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1mgzSnI4)
0. Final RPN+FastRCNN models: [OneDrive](https://onedrive.live.com/download?resid=D7AF52BADBA8A4BC!114&amp;authkey=!AERHoxZ-iAx_j34&amp;ithint=file%2czip) [DropBox](https://www.dropbox.com/s/jswrnkaln47clg2/faster_rcnn_final_model.zip?dl=0) [BaiduYun](http://pan.baidu.com/s/1hsFKmeK)"
"Abstract:  We propose a new exact method for shortest-path distance queries on
large-scale networks. Our method precomputes distance labels for vertices by
performing a breadth-first search from every vertex. Seemingly too obvious and
too inefficient at first glance the key ingredient introduced here is pruning
during breadth-first searches. While we can still answer the correct distance
for any pair of vertices from the labels it surprisingly reduces the search
space and sizes of labels. Moreover we show that we can perform 32 or 64
breadth-first searches simultaneously exploiting bitwise operations. We
experimentally demonstrate that the combination of these two techniques is
efficient and robust on various kinds of large-scale real-world networks. In
particular our method can handle social networks and web graphs with hundreds
of millions of edges which are two orders of magnitude larger than the limits
of previous exact methods with comparable query time to those of previous
methods.
Pruned Landmark Labeling
========================

Pruned landmark labeling is a new shortest-path distance querying algorithm for real-world graphs such as social networks web graphs biological networks and computer networks.

## Advantages
The algorithm has the following advantages (for details please see our paper):

* **Fast** --- it answers distance queries in microseconds
* **Scalable** --- it can be applied to networks with hundreds of millions of edges
* **Exact** --- unlike approximate methods it always answers exactly correct distance and
* **Almost Parameter Free** --- unlike other state-of-the-art methods it does not require any parameter tuning.

Moreover this implementation is:

* **Easy to Use** --- by copying only one header file to your project you can start using the index.

## Usage
Given a graph it first constructs an index. Then using the index it can quickly answer distance between two vertices.

### From CUI Interface

    $ make
    $ bin/construct_index samples/graph_example.tsv index_file
    $ bin/query_distance index_file &lt;&lt;&lt; ""1 4""
    2

* Execute `make` to build programs.
* Execute `bin/construct_index` to construct an index from a graph.
* Execute `bin/query_distance` and write pairs of vertices to STDIN to query distance between pairs of vertices.



### From Your Program

    PrunedLandmarkLabeling&lt;&gt; pll;
    pll.ConstructIndex(edge_list);
    cout &lt;&lt; pll.QueryDistance(1 4) &lt;&lt; endl;

* Call `ConstructIndex` to construct an index from a graph (an edge list or a file).
* Call `QueryDistance` to query distance between two vertices.
* Call `LoadIndex` and `StoreIndex` to load and store an index.

For further information please see `pruned_landmark_labeling.h` samples and tests.

### Details

* In a graph file each line should contain two integers describing an edge (see `samples/graph_example.tsv`).
* Vertices should be described by integers starting from zero.
* Program `bin/query_distance` reads pairs of vertices until EOF thus you can use it to process multiple pairs of vertices at once.
* Execute `make test` to run tests (*google-gtest* is required).

## References

* Takuya Akiba Yoichi Iwata and Yuichi Yoshida **[Fast Exact Shortest-Path Distance Queries on Large Networks by Pruned Landmark Labeling](http://www-imai.is.s.u-tokyo.ac.jp/~takiba/papers/sigmod13_pll.pdf)**.
In *SIGMOD 2013* to appear."
"Abstract:  Recent observations constrained the tangential velocity of M31 with respect
to the Milky Way (MW) to be v_tan<34.4 km/s and the radial velocity to be in
the range v_rad=-109+/- 4.4 km/s (van der Marel et al. 2012). In this study we
use a large volume high resolution N-body cosmological simulation (Bolshoi)
together with three constrained simulations to statistically study this
kinematics in the context of the LCDM. The comparison of the ensembles of
simulated pairs with the observed LG at the 1-sigma level in the uncertainties
has been done with respect to the radial and tangential velocities the reduced
orbital energy (e_tot) angular momentum (l_orb) and the dimensionless spin
parameter lambda. Our main results are: (i) the preferred radial and
tangential velocities for pairs in LCDM are v_rad=-80+/-20 km/s v_tan=50+/-10
km/s (ii) pairs around that region are 3 to 13 times more common than pairs
within the observational values (iii) 15%to 24% of LG-like pairs in LCDM have
energy and angular momentum consistent with observations while (iv) 9% to 13%
of pairs in the same sample show similar values in the inferred dimensionless
spin parameter. It follows that within current observational uncertainties the
quasi-conserved quantities that characterize the orbit of the LG i.e. e_tot
r_orb and lambda do not challenge the standard LCDM model but the model is in
tension with regard to the actual values of the radial and tangential
velocities. This might hint to a problem of the LCDM model to reproduce the
observed LG.
LG_Kinematics
=============

Quantify how special is the observed Local Group (Andromeda &amp; Milky Way) kinematics in the LCDM framework

Replicating the results
=======================

* Clone the repository
* Enter code/ and open the IPython notebook main_analysis.ipynb
* Execute all the cells in the notebook.

Paper 
======
[ The kinematics of the Local Group in a cosmological context - ArXiv link](http://arxiv.org/abs/1303.2690)"
"Abstract:  I present a new kind of astronomical database based on small text files and a
distributed version control system. This encourages the community to work
collaboratively. It creates a decentralized completely open and democratic way
of managing small to medium sized heterogeneous astronomical databases and
catalogues. The use of the XML file format allows an easy to parse and read
yet dynamic and extendable database structure.
The Open Exoplanet Catalogue is based on these principles and presented as an
example. It is a catalogue of all discovered extra-solar planets. It is the
only catalogue that can correctly represent the orbital structure of planets in
arbitrary binary triple and quadruple star systems as well as orphan planets.
LG_Kinematics
=============

Quantify how special is the observed Local Group (Andromeda &amp; Milky Way) kinematics in the LCDM framework

Replicating the results
=======================

* Clone the repository
* Enter code/ and open the IPython notebook main_analysis.ipynb
* Execute all the cells in the notebook.

Paper 
======
[ The kinematics of the Local Group in a cosmological context - ArXiv link](http://arxiv.org/abs/1303.2690)"
"Abstract:  The last 30 years have seen the creation of a variety of electronic
collaboration tools for science and business. Some of the best-known
collaboration tools support text editing (e.g. wikis). Wikipedia's success
shows that large-scale collaboration can produce highly valuable content.
Meanwhile much structured data is being collected and made publicly available.
We have never had access to more powerful databases and statistical packages.
Is large-scale collaborative data analysis now possible? Using a quantitative
analysis of Web 2.0 data visualization sites we find evidence that at least
moderate open collaboration occurs. We then explore some of the limiting
factors of collaboration over data.
LG_Kinematics
=============

Quantify how special is the observed Local Group (Andromeda &amp; Milky Way) kinematics in the LCDM framework

Replicating the results
=======================

* Clone the repository
* Enter code/ and open the IPython notebook main_analysis.ipynb
* Execute all the cells in the notebook.

Paper 
======
[ The kinematics of the Local Group in a cosmological context - ArXiv link](http://arxiv.org/abs/1303.2690)"
